
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
        <link rel="prev" href="../transformer/">
      
      
        <link rel="next" href="../%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%80%83%E8%AF%95/">
      
      
      <link rel="icon" href="../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.2">
    
    
      
        <title>频率学派和贝叶斯学派、概率模型和非概率模型、生成式模型和判别式模型 - My Docs</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.d7758b05.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    <body dir="ltr">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#_1" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../.." title="My Docs" class="md-header__button md-logo" aria-label="My Docs" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            My Docs
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              频率学派和贝叶斯学派、概率模型和非概率模型、生成式模型和判别式模型
            
          </span>
        </div>
      </div>
    </div>
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="My Docs" class="md-nav__button md-logo" aria-label="My Docs" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    My Docs
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        
  
  <span class="md-ellipsis">
    这是一个欢迎页面
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../algorithm/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    algorithm
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../git/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    git
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../%E4%BB%8E%E5%95%A5%E4%B9%9F%E4%B8%8D%E4%BC%9A%E5%88%B0%20GPT-3%20%E5%92%8C%20InstructGPT/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    从啥也不会到 GPT 3 和 InstructGPT
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../%E7%AE%97%E6%B3%95%E6%80%9D%E6%83%B3/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    算法思想
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_6" checked>
        
          
          <label class="md-nav__link" for="__nav_6" id="__nav_6_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    LLM&dl
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_6_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_6">
            <span class="md-nav__icon md-icon"></span>
            LLM&dl
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../ReinforcementLearing/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    强化学习
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../ai_agent/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    RAG 和使用工具在智能体（AI agents）解决大模型幻觉（hallucination）方面的异同
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../transformer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    transformer相关扣细节
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  <span class="md-ellipsis">
    频率学派和贝叶斯学派、概率模型和非概率模型、生成式模型和判别式模型
    
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  <span class="md-ellipsis">
    频率学派和贝叶斯学派、概率模型和非概率模型、生成式模型和判别式模型
    
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#_2" class="md-nav__link">
    <span class="md-ellipsis">
      生成式模型回答“数据从哪里来”，判别式模型回答“数据属于哪一类”
    </span>
  </a>
  
    <nav class="md-nav" aria-label="生成式模型回答“数据从哪里来”，判别式模型回答“数据属于哪一类”">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mlemap" class="md-nav__link">
    <span class="md-ellipsis">
      贝叶斯学派和频率学派（MLE、MAP、贝叶斯估计）
    </span>
  </a>
  
    <nav class="md-nav" aria-label="贝叶斯学派和频率学派（MLE、MAP、贝叶斯估计）">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_3" class="md-nav__link">
    <span class="md-ellipsis">
      两大学派和正则的关系
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_4" class="md-nav__link">
    <span class="md-ellipsis">
      监督学习中的生成式模型和判别式模型
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mlemap_1" class="md-nav__link">
    <span class="md-ellipsis">
      概念辨析：MLE、MAP、生成式模型、判别式模型
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#1-mle-vs-map" class="md-nav__link">
    <span class="md-ellipsis">
      1. 参数估计方法：MLE vs. MAP
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-vs" class="md-nav__link">
    <span class="md-ellipsis">
      2. 模型类型：生成式模型 vs. 判别式模型
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3" class="md-nav__link">
    <span class="md-ellipsis">
      3. 参数估计方法与模型类型的交叉关系
    </span>
  </a>
  
    <nav class="md-nav" aria-label="3. 参数估计方法与模型类型的交叉关系">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1-map" class="md-nav__link">
    <span class="md-ellipsis">
      (1) 生成式模型 + MAP
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-mle" class="md-nav__link">
    <span class="md-ellipsis">
      (2) 生成式模型 + MLE
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3-mle" class="md-nav__link">
    <span class="md-ellipsis">
      (3) 判别式模型 + MLE
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#4-map" class="md-nav__link">
    <span class="md-ellipsis">
      (4) 判别式模型 + MAP
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#4" class="md-nav__link">
    <span class="md-ellipsis">
      4. 总结：一张表理清所有关系
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#5" class="md-nav__link">
    <span class="md-ellipsis">
      5. 常见问题解答
    </span>
  </a>
  
    <nav class="md-nav" aria-label="5. 常见问题解答">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#q1" class="md-nav__link">
    <span class="md-ellipsis">
      Q1：生成式模型一定比判别式模型强吗？
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#q2map" class="md-nav__link">
    <span class="md-ellipsis">
      Q2：MAP和贝叶斯估计有什么区别？
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#q3" class="md-nav__link">
    <span class="md-ellipsis">
      Q3：逻辑回归是生成式还是判别式模型？
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_5" class="md-nav__link">
    <span class="md-ellipsis">
      最终结论
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_6" class="md-nav__link">
    <span class="md-ellipsis">
      概率模型和非概率模型
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_7" class="md-nav__link">
    <span class="md-ellipsis">
      那么损失函数是怎么来的呢？
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#1" class="md-nav__link">
    <span class="md-ellipsis">
      1. 线性回归：最小二乘与最大似然的统一
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-pca" class="md-nav__link">
    <span class="md-ellipsis">
      2. 主成分分析（PCA）：方差最大化与概率生成模型
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3-k-means" class="md-nav__link">
    <span class="md-ellipsis">
      3. K-means聚类：硬聚类与高斯混合模型的特例
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#4_1" class="md-nav__link">
    <span class="md-ellipsis">
      4. 逻辑回归：概率分类与几何间隔最大化
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#5-autoencoder" class="md-nav__link">
    <span class="md-ellipsis">
      5. 自编码器（Autoencoder）：重构误差与生成模型
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#6" class="md-nav__link">
    <span class="md-ellipsis">
      6. 决策树：规则学习与概率估计
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_8" class="md-nav__link">
    <span class="md-ellipsis">
      总结：模型的多面性与应用选择
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gptdecoder" class="md-nav__link">
    <span class="md-ellipsis">
      一、GPT的Decoder结构与训练目标
    </span>
  </a>
  
    <nav class="md-nav" aria-label="一、GPT的Decoder结构与训练目标">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1-decoder" class="md-nav__link">
    <span class="md-ellipsis">
      1. Decoder架构的核心机制
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2" class="md-nav__link">
    <span class="md-ellipsis">
      2. 预训练目标：语言建模
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3_1" class="md-nav__link">
    <span class="md-ellipsis">
      3. 微调目标：任务导向的联合分布建模
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#px-y" class="md-nav__link">
    <span class="md-ellipsis">
      二、训练过程：如何建模联合分布 ( P(X, Y) )？
    </span>
  </a>
  
    <nav class="md-nav" aria-label="二、训练过程：如何建模联合分布 ( P(X, Y) )？">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_9" class="md-nav__link">
    <span class="md-ellipsis">
      案例：情感分析任务
    </span>
  </a>
  
    <nav class="md-nav" aria-label="案例：情感分析任务">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1_1" class="md-nav__link">
    <span class="md-ellipsis">
      步骤1：数据格式化
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2_1" class="md-nav__link">
    <span class="md-ellipsis">
      步骤2：联合分布的分解
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3_2" class="md-nav__link">
    <span class="md-ellipsis">
      步骤3：参数更新
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#x-y" class="md-nav__link">
    <span class="md-ellipsis">
      三、推理过程：如何生成 ( X ) 或 ( Y )？
    </span>
  </a>
  
    <nav class="md-nav" aria-label="三、推理过程：如何生成 ( X ) 或 ( Y )？">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1-y" class="md-nav__link">
    <span class="md-ellipsis">
      案例1：生成标签 ( Y )（判别式任务）
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-x" class="md-nav__link">
    <span class="md-ellipsis">
      案例2：生成文本 ( X )（生成式任务）
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_10" class="md-nav__link">
    <span class="md-ellipsis">
      四、联合分布建模的底层实现
    </span>
  </a>
  
    <nav class="md-nav" aria-label="四、联合分布建模的底层实现">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1_2" class="md-nav__link">
    <span class="md-ellipsis">
      1. 自注意力机制的作用
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2_2" class="md-nav__link">
    <span class="md-ellipsis">
      2. 概率链式法则的具体化
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3_3" class="md-nav__link">
    <span class="md-ellipsis">
      3. 生成策略（推理阶段）
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_11" class="md-nav__link">
    <span class="md-ellipsis">
      五、训练与推理的对比
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gpt" class="md-nav__link">
    <span class="md-ellipsis">
      六、总结：GPT如何实现联合分布建模
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_12" class="md-nav__link">
    <span class="md-ellipsis">
      例子：生成句子 "我喜欢学习"
    </span>
  </a>
  
    <nav class="md-nav" aria-label="例子：生成句子 "我喜欢学习"">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1_3" class="md-nav__link">
    <span class="md-ellipsis">
      1. 联合概率的链式分解
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-gpt" class="md-nav__link">
    <span class="md-ellipsis">
      2. 条件概率的具体计算（以GPT为例）
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3-gpt" class="md-nav__link">
    <span class="md-ellipsis">
      3. GPT的训练过程：最大化似然估计
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#1_4" class="md-nav__link">
    <span class="md-ellipsis">
      1. 单层感知器的局限性
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2_3" class="md-nav__link">
    <span class="md-ellipsis">
      2. 多层感知器的核心思想
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3_4" class="md-nav__link">
    <span class="md-ellipsis">
      3. 非线性激活函数的作用
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#4_2" class="md-nav__link">
    <span class="md-ellipsis">
      4. 多层组合的直观理解
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#5_1" class="md-nav__link">
    <span class="md-ellipsis">
      5. 反向传播与参数优化
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#6_1" class="md-nav__link">
    <span class="md-ellipsis">
      6. 神经网络与非线性分类的实例
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_13" class="md-nav__link">
    <span class="md-ellipsis">
      总结
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_14" class="md-nav__link">
    <span class="md-ellipsis">
      【机器学习我到底在学什么】哲学角度聊聊贝叶斯派和频率派，数学角度看看极大似然估计和最大后验估计
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%80%83%E8%AF%95/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    人工智能考试
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    动手学深度学习
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    损失函数
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    机器学习基础
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%80%83%E8%AF%95/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    机器学习考试
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_7" >
        
          
          <label class="md-nav__link" for="__nav_7" id="__nav_7_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    fedGraphRAG mess
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_7_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_7">
            <span class="md-nav__icon md-icon"></span>
            fedGraphRAG mess
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../fedGraphRAG_mess/FedGraph/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    FedGraph: Federated Graph Learning With Intelligent Sampling
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../fedGraphRAG_mess/FederatedScope-GNN/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    FederatedScope-GNN: Towards a Unified, Comprehensive and Efficient Package for Federated Graph Learning
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../fedGraphRAG_mess/%E8%81%94%E9%82%A6KGs/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    联邦KGs
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../fedGraphRAG_mess/%E8%81%94%E9%82%A6%E5%AD%A6%E4%B9%A0%E5%92%8C%E8%81%94%E9%82%A6%E5%9B%BE%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    联邦学习和联邦图学习相关
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_8" >
        
          
          <label class="md-nav__link" for="__nav_8" id="__nav_8_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Infra
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_8_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_8">
            <span class="md-nav__icon md-icon"></span>
            Infra
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../infra/6.824/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    6.824
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../infra/CSAPP/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    CSAPP
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../infra/SICP/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    SICP
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../infra/linux/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    linux
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_9" >
        
          
          <label class="md-nav__link" for="__nav_9" id="__nav_9_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Java
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_9_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_9">
            <span class="md-nav__icon md-icon"></span>
            Java
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../java/javaWeb/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Java学习路线
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_10" >
        
          
          <label class="md-nav__link" for="__nav_10" id="__nav_10_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Mess
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_10_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_10">
            <span class="md-nav__icon md-icon"></span>
            Mess
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../mess/jinnseikann/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    iroiro
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../mess/maajyann/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    maahyann
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../mess/mess/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    mess
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../mess/simpread-%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1%E5%92%8C%E5%87%BD%E6%95%B0%E5%BC%8F%E7%BC%96%E7%A8%8B%E7%9A%84%E6%9C%AC%E8%B4%A8%E5%8C%BA%E5%88%AB/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Simpread 面向对象和函数式编程的本质区别
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../mess/%E7%9F%A9%E9%98%B5%E8%AE%BA/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    矩阵论
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_11" >
        
          
          <label class="md-nav__link" for="__nav_11" id="__nav_11_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    搜广推
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_11_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_11">
            <span class="md-nav__icon md-icon"></span>
            搜广推
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../%E6%90%9C%E5%B9%BF%E6%8E%A8/%E6%90%9C%E5%B9%BF%E6%8E%A8/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    搜广推相关知识
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#_2" class="md-nav__link">
    <span class="md-ellipsis">
      生成式模型回答“数据从哪里来”，判别式模型回答“数据属于哪一类”
    </span>
  </a>
  
    <nav class="md-nav" aria-label="生成式模型回答“数据从哪里来”，判别式模型回答“数据属于哪一类”">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mlemap" class="md-nav__link">
    <span class="md-ellipsis">
      贝叶斯学派和频率学派（MLE、MAP、贝叶斯估计）
    </span>
  </a>
  
    <nav class="md-nav" aria-label="贝叶斯学派和频率学派（MLE、MAP、贝叶斯估计）">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_3" class="md-nav__link">
    <span class="md-ellipsis">
      两大学派和正则的关系
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_4" class="md-nav__link">
    <span class="md-ellipsis">
      监督学习中的生成式模型和判别式模型
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mlemap_1" class="md-nav__link">
    <span class="md-ellipsis">
      概念辨析：MLE、MAP、生成式模型、判别式模型
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#1-mle-vs-map" class="md-nav__link">
    <span class="md-ellipsis">
      1. 参数估计方法：MLE vs. MAP
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-vs" class="md-nav__link">
    <span class="md-ellipsis">
      2. 模型类型：生成式模型 vs. 判别式模型
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3" class="md-nav__link">
    <span class="md-ellipsis">
      3. 参数估计方法与模型类型的交叉关系
    </span>
  </a>
  
    <nav class="md-nav" aria-label="3. 参数估计方法与模型类型的交叉关系">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1-map" class="md-nav__link">
    <span class="md-ellipsis">
      (1) 生成式模型 + MAP
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-mle" class="md-nav__link">
    <span class="md-ellipsis">
      (2) 生成式模型 + MLE
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3-mle" class="md-nav__link">
    <span class="md-ellipsis">
      (3) 判别式模型 + MLE
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#4-map" class="md-nav__link">
    <span class="md-ellipsis">
      (4) 判别式模型 + MAP
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#4" class="md-nav__link">
    <span class="md-ellipsis">
      4. 总结：一张表理清所有关系
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#5" class="md-nav__link">
    <span class="md-ellipsis">
      5. 常见问题解答
    </span>
  </a>
  
    <nav class="md-nav" aria-label="5. 常见问题解答">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#q1" class="md-nav__link">
    <span class="md-ellipsis">
      Q1：生成式模型一定比判别式模型强吗？
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#q2map" class="md-nav__link">
    <span class="md-ellipsis">
      Q2：MAP和贝叶斯估计有什么区别？
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#q3" class="md-nav__link">
    <span class="md-ellipsis">
      Q3：逻辑回归是生成式还是判别式模型？
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_5" class="md-nav__link">
    <span class="md-ellipsis">
      最终结论
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_6" class="md-nav__link">
    <span class="md-ellipsis">
      概率模型和非概率模型
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_7" class="md-nav__link">
    <span class="md-ellipsis">
      那么损失函数是怎么来的呢？
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#1" class="md-nav__link">
    <span class="md-ellipsis">
      1. 线性回归：最小二乘与最大似然的统一
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-pca" class="md-nav__link">
    <span class="md-ellipsis">
      2. 主成分分析（PCA）：方差最大化与概率生成模型
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3-k-means" class="md-nav__link">
    <span class="md-ellipsis">
      3. K-means聚类：硬聚类与高斯混合模型的特例
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#4_1" class="md-nav__link">
    <span class="md-ellipsis">
      4. 逻辑回归：概率分类与几何间隔最大化
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#5-autoencoder" class="md-nav__link">
    <span class="md-ellipsis">
      5. 自编码器（Autoencoder）：重构误差与生成模型
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#6" class="md-nav__link">
    <span class="md-ellipsis">
      6. 决策树：规则学习与概率估计
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_8" class="md-nav__link">
    <span class="md-ellipsis">
      总结：模型的多面性与应用选择
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gptdecoder" class="md-nav__link">
    <span class="md-ellipsis">
      一、GPT的Decoder结构与训练目标
    </span>
  </a>
  
    <nav class="md-nav" aria-label="一、GPT的Decoder结构与训练目标">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1-decoder" class="md-nav__link">
    <span class="md-ellipsis">
      1. Decoder架构的核心机制
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2" class="md-nav__link">
    <span class="md-ellipsis">
      2. 预训练目标：语言建模
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3_1" class="md-nav__link">
    <span class="md-ellipsis">
      3. 微调目标：任务导向的联合分布建模
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#px-y" class="md-nav__link">
    <span class="md-ellipsis">
      二、训练过程：如何建模联合分布 ( P(X, Y) )？
    </span>
  </a>
  
    <nav class="md-nav" aria-label="二、训练过程：如何建模联合分布 ( P(X, Y) )？">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_9" class="md-nav__link">
    <span class="md-ellipsis">
      案例：情感分析任务
    </span>
  </a>
  
    <nav class="md-nav" aria-label="案例：情感分析任务">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1_1" class="md-nav__link">
    <span class="md-ellipsis">
      步骤1：数据格式化
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2_1" class="md-nav__link">
    <span class="md-ellipsis">
      步骤2：联合分布的分解
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3_2" class="md-nav__link">
    <span class="md-ellipsis">
      步骤3：参数更新
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#x-y" class="md-nav__link">
    <span class="md-ellipsis">
      三、推理过程：如何生成 ( X ) 或 ( Y )？
    </span>
  </a>
  
    <nav class="md-nav" aria-label="三、推理过程：如何生成 ( X ) 或 ( Y )？">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1-y" class="md-nav__link">
    <span class="md-ellipsis">
      案例1：生成标签 ( Y )（判别式任务）
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-x" class="md-nav__link">
    <span class="md-ellipsis">
      案例2：生成文本 ( X )（生成式任务）
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_10" class="md-nav__link">
    <span class="md-ellipsis">
      四、联合分布建模的底层实现
    </span>
  </a>
  
    <nav class="md-nav" aria-label="四、联合分布建模的底层实现">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1_2" class="md-nav__link">
    <span class="md-ellipsis">
      1. 自注意力机制的作用
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2_2" class="md-nav__link">
    <span class="md-ellipsis">
      2. 概率链式法则的具体化
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3_3" class="md-nav__link">
    <span class="md-ellipsis">
      3. 生成策略（推理阶段）
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_11" class="md-nav__link">
    <span class="md-ellipsis">
      五、训练与推理的对比
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gpt" class="md-nav__link">
    <span class="md-ellipsis">
      六、总结：GPT如何实现联合分布建模
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_12" class="md-nav__link">
    <span class="md-ellipsis">
      例子：生成句子 "我喜欢学习"
    </span>
  </a>
  
    <nav class="md-nav" aria-label="例子：生成句子 "我喜欢学习"">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1_3" class="md-nav__link">
    <span class="md-ellipsis">
      1. 联合概率的链式分解
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-gpt" class="md-nav__link">
    <span class="md-ellipsis">
      2. 条件概率的具体计算（以GPT为例）
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3-gpt" class="md-nav__link">
    <span class="md-ellipsis">
      3. GPT的训练过程：最大化似然估计
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#1_4" class="md-nav__link">
    <span class="md-ellipsis">
      1. 单层感知器的局限性
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2_3" class="md-nav__link">
    <span class="md-ellipsis">
      2. 多层感知器的核心思想
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3_4" class="md-nav__link">
    <span class="md-ellipsis">
      3. 非线性激活函数的作用
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#4_2" class="md-nav__link">
    <span class="md-ellipsis">
      4. 多层组合的直观理解
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#5_1" class="md-nav__link">
    <span class="md-ellipsis">
      5. 反向传播与参数优化
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#6_1" class="md-nav__link">
    <span class="md-ellipsis">
      6. 神经网络与非线性分类的实例
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_13" class="md-nav__link">
    <span class="md-ellipsis">
      总结
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_14" class="md-nav__link">
    <span class="md-ellipsis">
      【机器学习我到底在学什么】哲学角度聊聊贝叶斯派和频率派，数学角度看看极大似然估计和最大后验估计
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  



<h1 id="_1">频率学派和贝叶斯学派、概率模型和非概率模型、生成式模型和判别式模型</h1>
<h2 id="_2">生成式模型回答“数据从哪里来”，判别式模型回答“数据属于哪一类”</h2>
<h3 id="mlemap">贝叶斯学派和频率学派（MLE、MAP、贝叶斯估计）</h3>
<p><strong>概率是样本空间的公理，频率派和贝叶斯派的区别只是对概率的理解和对模型参数的理解。</strong>   频率派认为概率就是频率的极限，贝叶斯派认为概率是信心的度量。实验数量不是无限的情况下频率派偶发事件影响严重，贝叶斯派可以引入先验。   <br />
<strong>两大学派的求解问题思路主要为：训练（MLE、MAP、贝叶斯估计求解模型参数）、预测（判别或者生成）</strong>。</p>
<p>MAP是取参数theta后验分布的众数，贝叶斯估计是求后验分布的期望，朴素贝叶斯的theta是预测时的先验和条件，后验分布的期望很难求。  </p>
<h4 id="_3">两大学派和正则的关系</h4>
<p>正则化本质上是人类主观添加的规则，用于弥补数据的不完美。<br />
频率派认为数据应能客观反映现实，因此只关注极大似然估计，而贝叶斯派则认为数据有限且不完美，因此引入先验概率，并通过贝叶斯定理修正为后验概率。这种额外的假设被称为正则。<br />
频率学派的MLE损失函数加上正则化项后就是MAP的损失函数，殊途同归。<strong>经验风险最小化和结构风险最小化</strong>。期望损失和经验风险/结构风险的区别：一个是损失函数的数学期望一个是损失函数的样本平均数，但P（X，Y）不可知（也就是）</p>
<h3 id="_4">监督学习中的生成式模型和判别式模型</h3>
<p>直接看b站情书的朴素贝叶斯就是生成模型的典型。朴素贝叶斯训练过程就是在数据集中统计计算概率，朴素在于各个特征互相独立。对p（xy）建模（因为都求出来了）然后自然地由贝叶斯公式得到要求的条件概率。最后取分类类别的时候单纯的概率高的是。 <br />
<img alt="" src="https://cdn.jsdelivr.net/gh/EuphratesG/myPic@main/202501260929882.png" /><br />
<strong>生成式模型到底是怎么生成的找到病因了</strong>，输出不是单个概率值，而是一个概率分布！！！通过这个概率分布去做采样。开始时生成第一个单词，输出第一个单词的概率分布（如 "The": 0.5, "A": 0.3, "It": 0.2）。采样第一个单词：从分布中采样得到 "The"。而判别式模型只能输出一个概率值。 MLM也只是bert训练时做的一个带训练目标的句子分类而已</p>
<h3 id="mlemap_1"><strong>概念辨析：MLE、MAP、生成式模型、判别式模型</strong></h3>
<p>这几个概念分属不同维度，但它们在实际模型训练和预测中相互关联。以下是分层次的解析：</p>
<hr />
<h3 id="1-mle-vs-map"><strong>1. 参数估计方法：MLE vs. MAP</strong></h3>
<p>这两个概念是<strong>参数估计方法</strong>，用于从数据中学习模型参数。</p>
<table>
<thead>
<tr>
<th><strong>方法</strong></th>
<th><strong>核心思想</strong></th>
<th><strong>公式</strong></th>
<th><strong>特点</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>MLE</strong></td>
<td>最大化<strong>似然函数</strong>，找到最可能生成数据的参数（仅依赖数据本身）。</td>
<td>( \theta_{\text{MLE}} = \arg\max_{\theta} P(D \mid \theta) )</td>
<td>完全依赖数据，可能过拟合（尤其是小数据场景）。</td>
</tr>
<tr>
<td><strong>MAP</strong></td>
<td>在MLE基础上引入<strong>先验分布</strong>，找到后验概率最大的参数（结合数据与先验知识）。</td>
<td>( \theta_{\text{MAP}} = \arg\max_{\theta} P(D \mid \theta)P(\theta) )</td>
<td>通过先验修正参数估计，更鲁棒（如拉普拉斯平滑）。</td>
</tr>
</tbody>
</table>
<p><strong>关系与区别</strong>：<br />
- MLE是MAP的特例（当先验分布是均匀分布时，MAP退化为MLE）。<br />
- <strong>实际应用</strong>：<br />
  - 朴素贝叶斯中的条件概率估计使用MAP（拉普拉斯平滑）。<br />
  - 逻辑回归的参数估计通常使用MLE（交叉熵损失），但加入L2正则化等价于MAP（高斯先验）。</p>
<hr />
<h3 id="2-vs"><strong>2. 模型类型：生成式模型 vs. 判别式模型</strong></h3>
<p>这两个概念是<strong>模型的设计范式</strong>，决定了模型如何建模输入 ( X ) 和输出 ( Y ) 的关系。</p>
<table>
<thead>
<tr>
<th><strong>模型类型</strong></th>
<th><strong>建模目标</strong></th>
<th><strong>核心能力</strong></th>
<th><strong>典型例子</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>生成式模型</strong></td>
<td>联合分布 ( P(X, Y) )</td>
<td>生成新样本、分类、数据补全</td>
<td>朴素贝叶斯、隐马尔可夫模型（HMM）、GAN</td>
</tr>
<tr>
<td><strong>判别式模型</strong></td>
<td>条件分布 ( P(Y \mid X) )</td>
<td>分类、回归、预测</td>
<td>逻辑回归、支持向量机（SVM）、神经网络、线性回归</td>
</tr>
</tbody>
</table>
<p><strong>关系与区别</strong>：<br />
- <strong>生成式模型</strong>需要显式建模数据的生成过程（如 ( P(X \mid Y) ) 和 ( P(Y) )），因此计算复杂度更高。<br />
- <strong>判别式模型</strong>直接关注输入到输出的映射，通常更高效且在小样本任务中表现更好。  </p>
<hr />
<h3 id="3"><strong>3. 参数估计方法与模型类型的交叉关系</strong></h3>
<p>生成式模型和判别式模型在训练时，均可使用MLE或MAP进行参数估计。以下是典型组合：</p>
<h4 id="1-map"><strong>(1) 生成式模型 + MAP</strong></h4>
<ul>
<li><strong>例子</strong>：朴素贝叶斯（带拉普拉斯平滑）。  </li>
<li><strong>过程</strong>：  </li>
<li>假设先验分布（如Dirichlet先验）。  </li>
<li>通过最大化后验概率 ( P(\theta \mid D) ) 估计参数（如条件概率 ( P(X_i \mid Y) )）。  </li>
</ul>
<h4 id="2-mle"><strong>(2) 生成式模型 + MLE</strong></h4>
<ul>
<li><strong>例子</strong>：高斯混合模型（GMM）。  </li>
<li><strong>过程</strong>：<br />
  直接最大化似然函数 ( P(D \mid \theta) )，估计高斯分布的均值和方差。  </li>
</ul>
<h4 id="3-mle"><strong>(3) 判别式模型 + MLE</strong></h4>
<ul>
<li><strong>例子</strong>：逻辑回归（无正则化）。  </li>
<li><strong>过程</strong>：<br />
  最大化条件似然 ( P(Y \mid X, \theta) )，使用交叉熵损失函数。  </li>
</ul>
<h4 id="4-map"><strong>(4) 判别式模型 + MAP</strong></h4>
<ul>
<li><strong>例子</strong>：带L2正则化的逻辑回归。  </li>
<li><strong>过程</strong>：<br />
  最大化后验概率 ( P(\theta \mid D) )，等价于在损失函数中加入L2正则项（假设参数服从高斯先验）。  </li>
</ul>
<hr />
<h3 id="4"><strong>4. 总结：一张表理清所有关系</strong></h3>
<table>
<thead>
<tr>
<th><strong>维度</strong></th>
<th><strong>生成式模型</strong></th>
<th><strong>判别式模型</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>核心建模目标</strong></td>
<td>( P(X, Y) )</td>
<td>( P(Y \mid X) )</td>
</tr>
<tr>
<td><strong>能否生成数据</strong></td>
<td>能（如生成图片、文本）</td>
<td>不能</td>
</tr>
<tr>
<td><strong>参数估计方法</strong></td>
<td>MLE（如GMM）、MAP（如朴素贝叶斯）</td>
<td>MLE（如逻辑回归）、MAP（如带L2正则化的SVM）</td>
</tr>
<tr>
<td><strong>典型任务</strong></td>
<td>生成、分类、聚类</td>
<td>分类、回归、预测</td>
</tr>
</tbody>
</table>
<hr />
<h3 id="5"><strong>5. 常见问题解答</strong></h3>
<h4 id="q1"><strong>Q1：生成式模型一定比判别式模型强吗？</strong></h4>
<ul>
<li><strong>不一定</strong>！生成式模型需要更强的假设（如朴素贝叶斯的特征独立性假设），如果假设不成立，性能可能较差。判别式模型通常在小样本或复杂任务中表现更好。</li>
</ul>
<h4 id="q2map"><strong>Q2：MAP和贝叶斯估计有什么区别？</strong></h4>
<ul>
<li><strong>MAP</strong>是点估计（取后验概率最大的参数值），<strong>贝叶斯估计</strong>是积分所有参数可能性（计算后验分布的期望）。贝叶斯估计更全面但计算复杂，MAP是实用折中。</li>
</ul>
<h4 id="q3"><strong>Q3：逻辑回归是生成式还是判别式模型？</strong></h4>
<ul>
<li><strong>判别式模型</strong>，因为它直接建模 ( P(Y \mid X) )，不涉及 ( P(X) ) 或 ( P(X \mid Y) )。</li>
</ul>
<hr />
<h3 id="_5"><strong>最终结论</strong></h3>
<ul>
<li><strong>MLE/MAP</strong>是参数估计方法，<strong>生成式/判别式</strong>是模型设计范式，两者是正交的维度。  </li>
<li>实际中选择哪种组合，取决于任务需求（是否需要生成数据）、数据量（小数据适合MAP）和模型假设的合理性。</li>
</ul>
<p>房价预测的线性回归模型属于<strong>判别式模型</strong>。以下是关键分析步骤：</p>
<ol>
<li>
<p><strong>模型目标</strong>：<br />
   线性回归旨在直接学习输入变量（如房屋面积、位置等）与输出变量（房价）之间的映射关系，即建模条件概率 ( P(Y|X) )。它关注的是在给定特征 ( X ) 时如何预测目标值 ( Y )，而非联合分布 ( P(X,Y) )。</p>
</li>
<li>
<p><strong>生成式 vs 判别式的核心区别</strong>：  </p>
</li>
<li><strong>生成式模型</strong>（如朴素贝叶斯）学习联合分布 ( P(X,Y) )，可生成新的数据样本（如模拟房屋特征和对应价格）。  </li>
<li>
<p><strong>判别式模型</strong>（如逻辑回归、支持向量机）直接建模 ( P(Y|X) ) 或决策边界，仅关注预测输出，不涉及输入数据的生成。</p>
</li>
<li>
<p><strong>线性回归的数学本质</strong>：<br />
   假设目标变量 ( Y ) 服从以 ( X ) 的线性组合为均值的高斯分布，通过最大似然估计参数。这种条件概率建模方式（( Y ) 依赖于 ( X )）明确属于判别式框架。</p>
</li>
<li>
<p><strong>误区澄清</strong>：<br />
   即使模型能预测 ( Y )，若未对 ( X ) 的分布进行建模（如生成新房屋特征），仍为判别式模型。生成式模型需同时描述 ( X ) 和 ( Y ) 的关系。</p>
</li>
</ol>
<p><strong>结论</strong>：线性回归通过直接建模条件概率 ( P(Y|X) ) 完成预测任务，属于典型的判别式模型。</p>
<h3 id="_6">概率模型和非概率模型</h3>
<p><strong>极大似然估计、最大后验估计、贝叶斯估计，这些都是基于概率似然的，所以一定是概率模型</strong>。生成式模型一定是概率模型，判别式模型可以是概率模型（线性回归的噪声假设）也可以是非概率模型（线性回归直接设fx求）</p>
<h3 id="_7">那么损失函数是怎么来的呢？</h3>
<p>机器学习领域，注意是机器学习领域，我们要找到输入X和输出Y（标签Y）的关系。而大前提就是XY满足一个联合分布（代表X和Y存在某种机器学习要拟合的目标关系，所以最终结果的XY关系当然不会相互独立） <br />
频率学派根据最大似然估计，做多次实验，把在该参数下每次实验结果的概率乘起来，因为这是确定的已知结果所以让该概率值最大的参数值就是模型的参数值。<br />
贝叶斯学派则根据最大后验估计引入先验。随着样本量的增加，后验分布的先验信息影响会减弱，最终后验分布会接近似然函数，即MLE的结果。  </p>
<p><strong>那么损失函数和这些有什么关系</strong>？<strong>损失函数是标签Y和预测值FX的函数</strong>。损失函数可以是根据朴素逻辑得到的（例如感知机模型，针对误分类点到超平面的距离总和进行建模，该值最小自然模型拟合完毕），也可以是根据最大似然估计的式子截取的和模型参数有关的部分（例如平方误差损失就是回归问题，噪声满足高斯分布的前提下最大似然估计的式子里取和模型参数有关的部分得到的）总而言之损失函数都必须是模型参数的函数，同时优化它可以达到拟合理想模型参数的目的。  </p>
<hr />
<p><strong>同一个模型的多视角解释：概率与非概率的融合</strong></p>
<p>在机器学习和统计学中，确实存在许多模型可以通过概率和非概率视角进行解释。这种双重解释性不仅丰富了模型的理论基础，还扩展了其应用场景。以下是几个典型示例及其详细分析：</p>
<hr />
<h3 id="1"><strong>1. 线性回归：最小二乘与最大似然的统一</strong></h3>
<p><strong>模型形式</strong>：<br />
( y = \mathbf{w}^T \mathbf{x} + \epsilon )</p>
<ul>
<li>
<p><strong>非概率视角（最小二乘法）</strong>：<br />
  通过最小化预测值与真实值的平方误差（(\sum (y_i - \mathbf{w}^T \mathbf{x}_i)^2)）求解参数 (\mathbf{w})。其核心是几何意义上的最优拟合，不涉及概率假设。</p>
</li>
<li>
<p><strong>概率视角（最大似然估计）</strong>：<br />
  假设误差项 (\epsilon \sim \mathcal{N}(0, \sigma^2))，则观测值 (y) 服从正态分布 (y \sim \mathcal{N}(\mathbf{w}^T \mathbf{x}, \sigma^2))。此时，最小化平方误差等价于最大化数据的似然函数。<br />
<strong>结论</strong>：同一数学形式下，线性回归既是优化问题，也是概率模型的参数估计。</p>
</li>
</ul>
<hr />
<h3 id="2-pca"><strong>2. 主成分分析（PCA）：方差最大化与概率生成模型</strong></h3>
<p><strong>模型目标</strong>：寻找数据的主成分方向。</p>
<ul>
<li>
<p><strong>非概率视角（方差最大化）</strong>：<br />
  PCA通过最大化投影数据的方差（或最小化重构误差）找到正交基，属于纯代数方法，无概率假设。</p>
</li>
<li>
<p><strong>概率视角（概率PCA, PPCA）</strong>：<br />
  假设数据由隐变量 (\mathbf{z}) 生成，且 (\mathbf{x} = \mathbf{W}\mathbf{z} + \mu + \epsilon)，其中 (\epsilon \sim \mathcal{N}(0, \sigma^2\mathbf{I}))。通过最大似然估计，PPCA的隐变量空间与经典PCA的主成分一致。<br />
<strong>结论</strong>：PPCA为PCA提供了概率生成框架，数学结果相同但解释路径不同。</p>
</li>
</ul>
<hr />
<h3 id="3-k-means"><strong>3. K-means聚类：硬聚类与高斯混合模型的特例</strong></h3>
<p><strong>算法目标</strong>：将数据划分为 (K) 个簇。</p>
<ul>
<li>
<p><strong>非概率视角（优化损失函数）</strong>：<br />
  K-means最小化样本到簇中心的平方距离，是一种迭代优化算法，不涉及概率分布。</p>
</li>
<li>
<p><strong>概率视角（高斯混合模型, GMM）</strong>：<br />
  当GMM的每个组分协方差矩阵趋近于零（即退化为点质量分布）时，其最大似然估计等价于K-means的硬分配。<br />
<strong>结论</strong>：K-means可视为GMM在特定条件下的概率模型简化。</p>
</li>
</ul>
<hr />
<h3 id="4_1"><strong>4. 逻辑回归：概率分类与几何间隔最大化</strong></h3>
<p><strong>模型形式</strong>：<br />
( P(y=1|\mathbf{x}) = \frac{1}{1 + e^{-\mathbf{w}^T \mathbf{x}}} )</p>
<ul>
<li>
<p><strong>概率视角（最大似然估计）</strong>：<br />
  假设标签 (y) 服从伯努利分布，通过最大化对数似然（即最小化交叉熵）训练模型，直接输出类别的概率。</p>
</li>
<li>
<p><strong>非概率视角（几何间隔）</strong>：<br />
  逻辑回归的决策边界可视为寻找一个超平面 (\mathbf{w}^T \mathbf{x} = 0)，使得正负样本的“置信度”（(\mathbf{w}^T \mathbf{x}) 的绝对值）尽可能大。尽管损失函数仍基于概率，但其几何解释与支持向量机（SVM）的间隔最大化思想有相似之处。<br />
<strong>结论</strong>：逻辑回归虽以概率形式表达，但可通过几何视角理解其分类行为。</p>
</li>
</ul>
<hr />
<h3 id="5-autoencoder"><strong>5. 自编码器（Autoencoder）：重构误差与生成模型</strong></h3>
<p><strong>模型结构</strong>：编码器-解码器网络，最小化输入与重构输出的差异。</p>
<ul>
<li>
<p><strong>非概率视角（降维与重构）</strong>：<br />
  自编码器通过压缩与解压缩数据学习低维表示，目标是最小化重构误差（如均方误差），不涉及概率建模。</p>
</li>
<li>
<p><strong>概率视角（变分自编码器, VAE）</strong>：<br />
  VAE引入隐变量 (\mathbf{z}) 的概率分布（如高斯分布），并最大化数据的对数似然下界。其重构过程可视为从概率分布中采样生成数据。<br />
<strong>结论</strong>：VAE为自编码器添加了概率生成框架，扩展了其解释性和应用范围。</p>
</li>
</ul>
<hr />
<h3 id="6"><strong>6. 决策树：规则学习与概率估计</strong></h3>
<p><strong>模型机制</strong>：基于特征阈值递归分割数据。</p>
<ul>
<li>
<p><strong>非概率视角（规则生成）</strong>：<br />
  决策树通过信息增益或基尼系数选择分裂点，生成明确的分类规则，输出为确定性类别。</p>
</li>
<li>
<p><strong>概率视角（概率估计）</strong>：<br />
  在树的叶节点统计样本类别分布，可输出属于各类别的概率（如随机森林通过投票比例估计概率）。<br />
<strong>结论</strong>：决策树的原生设计是非概率的，但可通过叶节点的统计信息实现概率解释。</p>
</li>
</ul>
<hr />
<h3 id="_8"><strong>总结：模型的多面性与应用选择</strong></h3>
<p>许多经典模型兼具概率和非概率解释，这种双重性源于数学形式与假设的灵活性。选择解释方式时需考虑：</p>
<ol>
<li><strong>问题需求</strong>：是否需要不确定性量化（如医疗诊断）或生成能力（如VAE）？  </li>
<li><strong>数据特性</strong>：小样本数据可能更适合贝叶斯概率框架，而大数据场景偏向非概率的高效优化。  </li>
<li><strong>解释性要求</strong>：业务场景中，决策树或线性模型的非概率规则更易被接受。  </li>
</ol>
<p>最终，模型的选择和解释取决于实际问题的核心需求，而非非此即彼的理论偏好。这种多视角的融合正是机器学习丰富性和实用性的体现。</p>
<p>我们可以将GPT的Decoder结构、训练目标（预训练与微调）以及推理生成过程，与联合分布建模的具体例子结合起来，分步骤解释生成式模型如何实现 ( P(X, Y) ) 的学习与生成。以下是详细说明：</p>
<hr />
<h3 id="gptdecoder"><strong>一、GPT的Decoder结构与训练目标</strong></h3>
<h4 id="1-decoder"><strong>1. Decoder架构的核心机制</strong></h4>
<ul>
<li><strong>自回归（Autoregressive）</strong>：GPT的Decoder逐词生成文本，每一步只能看到当前词左侧的上下文（通过<strong>Masked Self-Attention</strong>实现）。</li>
<li><strong>输入表示</strong>：文本序列被转换为Token Embeddings + Positional Embeddings，输入Decoder层。</li>
<li><strong>输出预测</strong>：每个位置输出下一个Token的概率分布（通过Softmax）。</li>
</ul>
<h4 id="2"><strong>2. 预训练目标：语言建模</strong></h4>
<ul>
<li><strong>任务</strong>：给定前文 ( w_1, w_2, \ldots, w_{t-1} )，预测下一个词 ( w_t )。</li>
<li><strong>数学形式</strong>：最大化似然函数：
  [
  \mathcal{L} = \sum_{t=1}^T \log P(w_t | w_1, \ldots, w_{t-1})
  ]</li>
<li><strong>本质</strong>：通过逐词预测，模型隐式学习文本序列的联合分布 ( P(X) )（无监督场景）。</li>
</ul>
<h4 id="3_1"><strong>3. 微调目标：任务导向的联合分布建模</strong></h4>
<ul>
<li><strong>任务示例</strong>：情感分析（( X )=文本，( Y )=标签）。</li>
<li><strong>输入格式</strong>：将任务转化为文本生成。例如：</li>
<li>输入序列："这部电影很无聊。情感标签："</li>
<li>目标输出："负面"</li>
<li><strong>训练目标</strong>：模型学习联合分布 ( P(X, Y) )，即同时建模文本 ( X ) 和标签 ( Y ) 的关系。</li>
</ul>
<hr />
<h3 id="px-y"><strong>二、训练过程：如何建模联合分布 ( P(X, Y) )？</strong></h3>
<h4 id="_9"><strong>案例：情感分析任务</strong></h4>
<p>假设训练数据包含电影评论 ( X ) 和标签 ( Y )（正面/负面）。</p>
<h5 id="1_1"><strong>步骤1：数据格式化</strong></h5>
<ul>
<li>将标签 ( Y ) 作为文本的一部分输入模型。例如：
  <code>输入序列："情感分析：这部电影太棒了！标签："
  目标输出："正面"</code>
  （通过这种方式，模型将 ( X ) 和 ( Y ) 视为一个整体序列）</li>
</ul>
<h5 id="2_1"><strong>步骤2：联合分布的分解</strong></h5>
<ul>
<li>模型的训练目标变为预测标签 ( Y )（即下一个Token），基于输入文本 ( X )：
  [
  P(X, Y) = P(Y | X) \cdot P(X)
  ]
  其中 ( P(X) ) 已在预训练阶段通过语言建模学习。</li>
</ul>
<h5 id="3_2"><strong>步骤3：参数更新</strong></h5>
<ul>
<li>模型通过交叉熵损失，学习从 ( X ) 到 ( Y ) 的映射，同时保持对 ( X ) 生成能力（预训练知识的迁移）。</li>
</ul>
<hr />
<h3 id="x-y"><strong>三、推理过程：如何生成 ( X ) 或 ( Y )？</strong></h3>
<h4 id="1-y"><strong>案例1：生成标签 ( Y )（判别式任务）</strong></h4>
<ul>
<li><strong>输入</strong>："情感分析：这部电影节奏拖沓，毫无新意。标签："</li>
<li><strong>生成过程</strong>：</li>
<li>Decoder逐步生成Token，基于上文预测下一个词。</li>
<li>模型根据预训练和微调学到的联合分布 ( P(X, Y) )，计算候选词（如“负面”“正面”）的概率。</li>
<li>选择概率最高的词作为输出（如“负面”）。</li>
</ul>
<h4 id="2-x"><strong>案例2：生成文本 ( X )（生成式任务）</strong></h4>
<ul>
<li><strong>输入</strong>："生成一段负面影评："</li>
<li><strong>生成过程</strong>：</li>
<li>Decoder从起始Token开始，逐词生成文本。</li>
<li>每一步基于已生成的内容（如“服务差，价格高”），预测下一个词（如“，再也不会光顾了。”）。</li>
<li>生成的文本需符合“负面”标签的语义（隐式调用 ( P(X | Y=\text{负面}) )）。</li>
</ul>
<hr />
<h3 id="_10"><strong>四、联合分布建模的底层实现</strong></h3>
<h4 id="1_2"><strong>1. 自注意力机制的作用</strong></h4>
<ul>
<li><strong>捕捉长程依赖</strong>：模型通过Self-Attention，识别文本中与标签相关的关键片段。<br />
  （例如，在情感分析中，关注“无聊”“糟糕”等词）</li>
<li><strong>标签与文本的关联</strong>：在微调阶段，标签 ( Y ) 作为输入的一部分，通过注意力权重与文本 ( X ) 建立联系。</li>
</ul>
<h4 id="2_2"><strong>2. 概率链式法则的具体化</strong></h4>
<p>假设生成序列为 ( S = [X; Y] = [w_1, w_2, \ldots, w_T] )，模型通过链式法则建模联合分布：
[
P(S) = P(w_1) \cdot P(w_2 | w_1) \cdot \ldots \cdot P(w_T | w_1, \ldots, w_{T-1})
]
- <strong>标签生成</strong>：若 ( Y ) 是序列的最后几个Token（如“标签：负面”），模型需基于前文 ( X ) 计算 ( P(Y | X) )。
- <strong>文本生成</strong>：若 ( X ) 是序列的后半部分（如“生成负面影评：…”），模型需基于标签 ( Y ) 计算 ( P(X | Y) )。</p>
<h4 id="3_3"><strong>3. 生成策略（推理阶段）</strong></h4>
<ul>
<li><strong>贪心搜索（Greedy Search）</strong>：每一步选择概率最高的词，但可能陷入局部最优。</li>
<li><strong>束搜索（Beam Search）</strong>：保留多个候选序列，平衡生成质量与多样性。</li>
<li><strong>采样（Sampling）</strong>：根据概率分布随机采样，增加生成多样性。</li>
</ul>
<hr />
<h3 id="_11"><strong>五、训练与推理的对比</strong></h3>
<table>
<thead>
<tr>
<th><strong>阶段</strong></th>
<th><strong>训练（预训练+微调）</strong></th>
<th><strong>推理（生成）</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>目标</strong></td>
<td>最大化文本序列的似然概率，建模 ( P(X) ) 或 ( P(X, Y) )</td>
<td>从学习到的分布中采样，生成符合任务需求的序列</td>
</tr>
<tr>
<td><strong>输入</strong></td>
<td>完整文本序列（可能包含标签）</td>
<td>部分序列（如Prompt），由模型补全</td>
</tr>
<tr>
<td><strong>输出</strong></td>
<td>计算损失并更新参数</td>
<td>生成Token序列（如标签或文本）</td>
</tr>
<tr>
<td><strong>关键</strong></td>
<td>通过Masked Attention限制仅看到左侧上下文</td>
<td>自回归生成，每一步依赖已生成的Token</td>
</tr>
</tbody>
</table>
<hr />
<h3 id="gpt"><strong>六、总结：GPT如何实现联合分布建模</strong></h3>
<ol>
<li><strong>架构设计</strong>：Decoder的自回归结构天然适合序列生成，逐词预测本质上是建模联合分布 ( P(X) ) 或 ( P(X, Y) )。</li>
<li><strong>训练目标</strong>：通过语言模型预训练学习 ( P(X) )，微调阶段将任务转化为序列生成（如 ( X \rightarrow Y ) 或 ( Y \rightarrow X )），扩展为 ( P(X, Y) )。</li>
<li><strong>推理灵活性</strong>：  </li>
<li>给定 ( X ) 生成 ( Y )（如分类任务），本质是计算 ( P(Y | X) )；  </li>
<li>给定 ( Y ) 生成 ( X )（如文本生成），本质是计算 ( P(X | Y) )；  </li>
<li>自由生成 ( (X, Y) ) 对（如数据增强），本质是从 ( P(X, Y) ) 中采样。</li>
</ol>
<p><strong>最终答案</strong>：GPT通过自回归Decoder结构、语言模型预训练和任务微调，隐式学习数据与标签的联合分布 ( P(X, Y) )。在推理时，它通过调整输入提示（Prompt）灵活切换生成方向，既可生成文本 ( X )（如评论），也可生成标签 ( Y )（如情感），体现了对联合分布的统一建模能力。</p>
<p>好的！我们通过一个具体的文本生成例子，结合数学公式，说明语言模型如何将联合概率分解为条件概率链，并以GPT为例解释其训练和生成过程。</p>
<hr />
<h3 id="_12"><strong>例子：生成句子 "我喜欢学习"</strong></h3>
<p>假设模型要生成句子 ( S = ) "我喜欢学习"，对应的词序列为 ( w_1=我, w_2=喜欢, w_3=学习 )。<br />
语言模型的目标是建模整个序列的联合概率 ( P(w_1, w_2, w_3) )，并通过链式法则将其分解为条件概率的乘积。</p>
<hr />
<h4 id="1_3"><strong>1. 联合概率的链式分解</strong></h4>
<p>根据概率链式法则，联合概率可以分解为：
[
P(w_1, w_2, w_3) = P(w_1) \cdot P(w_2 | w_1) \cdot P(w_3 | w_1, w_2)
]
- <strong>第一步</strong>：生成第一个词 ( w_1=我 ) 的概率 ( P(w_1) )。
- <strong>第二步</strong>：在已生成 ( w_1=我 ) 的条件下，生成第二个词 ( w_2=喜欢 ) 的概率 ( P(w_2 | w_1) )。
- <strong>第三步</strong>：在已生成 ( w_1=我, w_2=喜欢 ) 的条件下，生成第三个词 ( w_3=学习 ) 的概率 ( P(w_3 | w_1, w_2) )。</p>
<hr />
<h4 id="2-gpt"><strong>2. 条件概率的具体计算（以GPT为例）</strong></h4>
<p>GPT通过自回归机制逐词生成，每一步基于上文预测下一个词。假设模型参数已通过训练学习到以下条件概率（简化示例）：</p>
<ul>
<li>
<p><strong>生成第一个词 ( w_1=我 )</strong>：<br />
  初始状态下，模型从所有可能的首词中选择，例如：
  [
  P(w_1=我) = 0.3, \quad P(w_1=你) = 0.2, \quad P(w_1=今天) = 0.1, \quad \ldots
  ]
  最终选择概率最高的 ( w_1=我 )。</p>
</li>
<li>
<p><strong>生成第二个词 ( w_2=喜欢 )</strong>：<br />
  基于上文 ( w_1=我 )，模型计算：
  [
  P(w_2=喜欢 | w_1=我) = 0.6, \quad P(w_2=讨厌 | w_1=我) = 0.1, \quad \ldots
  ]
  选择 ( w_2=喜欢 )。</p>
</li>
<li>
<p><strong>生成第三个词 ( w_3=学习 )</strong>：<br />
  基于上文 ( w_1=我, w_2=喜欢 )，模型计算：
  [
  P(w_3=学习 | w_1=我, w_2=喜欢) = 0.5, \quad P(w_3=睡觉 | w_1=我, w_2=喜欢) = 0.2, \quad \ldots
  ]
  选择 ( w_3=学习 )。</p>
</li>
</ul>
<p>最终联合概率为：
[
P(我, 喜欢, 学习) = 0.3 \times 0.6 \times 0.5 = 0.09
]</p>
<hr />
<h3 id="3-gpt"><strong>3. GPT的训练过程：最大化似然估计</strong></h3>
<p>在预训练阶段，GPT通过大量文本学习这些条件概率。例如，给定句子片段 "我 喜欢 _"，模型的目标是最大化下一个词为 "学习" 的概率：
[
\mathcal{L} = \log P(w_3=学习 | w_1=我, w_2=喜欢)
]
通过反向传播调整模型参数，使得模型输出的条件概率 ( P(w_3=学习 | w_1=我, w_2=喜欢) ) 尽可能接近实际数据中的分布。</p>
<hr />
<ul>
<li><strong>联合概率分解</strong>：语言模型通过链式法则 ( P(w_1, \ldots, w_T) = \prod_{t=1}^T P(w_t | w_1, \ldots, w_{t-1}) ) 将联合分布转化为条件概率的乘积。</li>
</ul>
<p>神经网络通过多层感知器的组合叠加解决非线性分类问题的理解可以分为以下几个关键点：</p>
<hr />
<h3 id="1_4"><strong>1. 单层感知器的局限性</strong></h3>
<p><strong>感知器（Perceptron）</strong>是最简单的神经网络单元，本质是一个线性分类器：
- <strong>数学形式</strong>：( y = \sigma(w \cdot x + b) )，其中 ( \sigma ) 是激活函数（如阶跃函数）。
- <strong>能力</strong>：只能解决<strong>线性可分</strong>问题（如AND、OR逻辑门），但无法处理<strong>非线性可分</strong>问题（如XOR异或逻辑）。</p>
<p><strong>示例</strong>：XOR问题的线性不可分性<br />
<img src="https://miro.medium.com/v2/resize:fit:1400/1*Tc8UgR_fjI_h0p3y4HvMwA.png" width="300"/></p>
<p>单层感知器无法找到一条直线将XOR的两类数据分开。</p>
<hr />
<h3 id="2_3"><strong>2. 多层感知器的核心思想</strong></h3>
<p>通过<strong>叠加多个感知器层</strong>（隐藏层），并引入<strong>非线性激活函数</strong>，神经网络可以学习复杂的非线性决策边界。<br />
- <strong>关键操作</strong>：<br />
  1. <strong>线性变换</strong>：每层通过权重 ( W ) 和偏置 ( b ) 对输入进行线性组合。<br />
  2. <strong>非线性激活</strong>：对线性变换的结果应用激活函数（如ReLU、Sigmoid），打破线性限制。<br />
  3. <strong>逐层传递</strong>：将前一层的输出作为下一层的输入，逐步抽象特征。</p>
<p><strong>示例</strong>：用两层感知器解决XOR问题<br />
<img src="https://miro.medium.com/v2/resize:fit:1400/1*3QSFbW4UZv4jq4v6Vyj8TA.png" width="400"/></p>
<ul>
<li><strong>第一层</strong>：构建两个线性边界，将输入空间划分为多个区域。  </li>
<li><strong>第二层</strong>：组合第一层的输出，形成最终的XOR决策边界。</li>
</ul>
<hr />
<h3 id="3_4"><strong>3. 非线性激活函数的作用</strong></h3>
<p>若没有非线性激活函数，多层感知器等价于单层线性模型。<br />
<strong>常见激活函数</strong>：<br />
- <strong>Sigmoid</strong>：( \sigma(z) = \frac{1}{1+e^{-z}} )，输出范围(0,1)，适合概率映射。<br />
- <strong>ReLU</strong>：( \text{ReLU}(z) = \max(0, z) )，解决梯度消失问题，加速训练。<br />
- <strong>Tanh</strong>：( \tanh(z) = \frac{e^z - e^{-z}}{e^z + e^{-z}} )，输出范围(-1,1)，中心对称。</p>
<p><strong>数学解释</strong>：<br />
假设一个两层网络：
[
y = W_2 \cdot \sigma(W_1 \cdot x + b_1) + b_2
]
- 若 ( \sigma ) 是线性函数（如恒等函数），则整体仍是线性变换：<br />
  [
  y = W_2 \cdot (W_1 \cdot x + b_1) + b_2 = (W_2 W_1) \cdot x + (W_2 b_1 + b_2)
  ]
- <strong>非线性激活函数</strong>使得网络的表达能力指数级增强，能够逼近任意复杂函数（Universal Approximation Theorem）。</p>
<hr />
<h3 id="4_2"><strong>4. 多层组合的直观理解</strong></h3>
<p>神经网络通过逐层抽象特征，将低层特征组合为高层语义：
- <strong>输入层</strong>：原始数据（如像素值）。<br />
- <strong>隐藏层1</strong>：检测简单模式（如边缘、颜色）。<br />
- <strong>隐藏层2</strong>：组合简单模式为复杂结构（如纹理、形状）。<br />
- <strong>输出层</strong>：综合高层特征进行分类（如物体类别）。</p>
<p><strong>示例</strong>：图像分类<br />
<img src="https://developers.google.com/static/machine-learning/practical/images/mlcc/feature_hierarchy.png" width="500"/></p>
<ul>
<li><strong>浅层</strong>：边缘、角点 → <strong>中层</strong>：纹理、部件 → <strong>深层</strong>：物体整体。</li>
</ul>
<hr />
<h3 id="5_1"><strong>5. 反向传播与参数优化</strong></h3>
<p>多层感知器的训练依赖于<strong>反向传播算法</strong>和<strong>梯度下降</strong>：
1. <strong>前向传播</strong>：计算预测值 ( \hat{y} ) 和损失函数 ( \mathcal{L} )。<br />
2. <strong>反向传播</strong>：从输出层到输入层逐层计算梯度 ( \frac{\partial \mathcal{L}}{\partial W} )。<br />
3. <strong>参数更新</strong>：通过梯度下降调整权重和偏置，最小化损失。</p>
<p><strong>数学形式</strong>：<br />
对于权重 ( W_{ij}^{(l)} )（第 ( l ) 层第 ( i ) 个神经元到第 ( j ) 个神经元的连接权重）：
[
W_{ij}^{(l)} \leftarrow W_{ij}^{(l)} - \eta \cdot \frac{\partial \mathcal{L}}{\partial W_{ij}^{(l)}}
]
其中 ( \eta ) 是学习率。</p>
<hr />
<h3 id="6_1"><strong>6. 神经网络与非线性分类的实例</strong></h3>
<p><strong>问题</strong>：用神经网络对螺旋形数据进行分类。<br />
<strong>网络结构</strong>：3层（输入层→隐藏层→输出层），激活函数为ReLU。<br />
<strong>训练结果</strong>：<br />
<img src="https://cs231n.github.io/assets/nn1/spiral_net.png" width="400"/></p>
<ul>
<li><strong>单层网络</strong>：只能拟合线性边界，分类失败。  </li>
<li><strong>多层网络</strong>：通过非线性激活和组合，形成复杂决策边界。</li>
</ul>
<hr />
<h3 id="_13"><strong>总结</strong></h3>
<ul>
<li><strong>单层感知器的限制</strong>：仅能解决线性可分问题。  </li>
<li><strong>多层感知器的突破</strong>：通过叠加隐藏层和非线性激活函数，学习复杂非线性关系。  </li>
<li><strong>核心机制</strong>：  </li>
<li><strong>线性变换</strong>：组合输入特征。  </li>
<li><strong>非线性激活</strong>：引入表达能力。  </li>
<li><strong>反向传播</strong>：优化模型参数。  </li>
<li><strong>直观理解</strong>：逐层抽象特征，从简单模式到复杂语义，最终解决非线性分类问题。</li>
</ul>
<h3 id="_14">【机器学习我到底在学什么】哲学角度聊聊贝叶斯派和频率派，数学角度看看极大似然估计和最大后验估计</h3>
<p>https://www.bilibili.com/video/BV1Ea4y1J7Jq/?spm_id_from=333.337.search-card.all.click&amp;vd_source=1e9369bdf50b21325055a7d2089c90b7<br />
- <strong>这里体现了：</strong>
  - 线性回归（判别式模型）也可以是概率模型，极大似然估计推到MSE上
  - 极大似然估计plus正则化和最大后验估计殊途同归</p>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "../..", "features": [], "search": "../../assets/javascripts/workers/search.f8cc74c7.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../../assets/javascripts/bundle.f13b1293.min.js"></script>
      
    
  </body>
</html>