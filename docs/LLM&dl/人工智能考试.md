# 人工智能考试
符号主义（三段论推理）、连结主义（机器学习）、行为学派（强化学习）  
## 第二章 知识表示
人工智能各个方向可抽象为解决问题，又下分为问题表示和寻找方法。  
1. 状态空间法 （State,S0,Fuction,StateGoal）四元组  
例子：八数码（华容道）3*3棋盘8个棋子，状态空间大小是9!  
状态图(搜索图)是运用状态空间进行问题求解的基础，相当于从操作图中找出一条解路径。  
看猴子那个例子就行了，先定义状态参数（例如猴子那个就有四个参数加以限制组成五种状态）及初始状态目标状态，再定义操作符（操作），解就是能够从初始状态转换至目标状态操作符的序列  
2. 问题规约表示 汉诺伊塔 就是递归思想
3. 与/或图表示法（就是状态空间+问题规约的形式化表示） 对于汉诺伊塔的表示没有异议，但是对于猴子例子的表示其实可以把摘香蕉那一步也算在第三个或子问题里面就合理一点（应该是根节点少画了一个与节点）。是不是终叶节点是由认为规定的。求解思想：从终叶节点本源问题倒推→判别节点“可解/不可解”→判断初始节点是否可解  
4. 产生式规则or系统 一种描述形式语言的语法，是AI中应用最多的知识表示方法之一。 动物识别
5. 知识的逻辑表示方法 符号主义人工智能就来自于此。①命题逻辑。逻辑等价四条（逆否命题、蕴含消除成非和并、两个德摩根） 消解和归结（w消解和w归结非并）②命题逻辑->谓词逻辑 为什么要从命题逻辑到谓词逻辑。命题逻辑无法表达局部与整体、一般与个别的关系。这里的谓词指sentential function是一个函数值只有TF的函数，而where条件中的谓词指的是predicate。③谓词演算 **例子还是看猴子那个例子理解透彻了。相当于用谓词逻辑的符号体系描述了状态空间法**
6. 语义网络模型 例子就看最后那个  
7. 知识图谱本质就是由知识点互相连接构成的语义网络。
知识图谱推理。**学的是蕴含式前面半部分推理规则。**三元组的一阶逻辑形式，为基于“知识图谱”的推理创造了条件。可利用一阶逻辑进行知识图谱的推理。
所谓一阶归纳学习（FOIL），推理思路：从一般到特殊，逐步给目标谓词添加前提约束谓词，直到构成的推理规则不覆盖任何反例。  
正例：满足当前规则的三元组  
反例：由图谱其他关系推出的不可能满足当前规则的三元组（如夫妻不可能是父子关系，注意是已知不可能满足的，未知的不算）  
注意这里正例和反例是当前规则覆盖的正例和反例数量，数量会随着加入前提约束谓词而改变。  
带入的时候变元可以取任何值，因此z取james不代表y不能取james  
算法流程：先列出初始正反例，逐个加入前提谓词列出新的正反例去计算该谓词的gain值取最大的加入，然后删除初始正反例中不符合规则的正反例后迭代  

8. 概率图谱推理  
不同事件的独立或非独立关系的图，看贝叶斯网络那个就好  
概率图模型分为：贝叶斯网络(Bayesian Network)和马尔可夫网络(Markov Network)两大类。
贝叶斯网络就看个概率计算就好了。计算联合概率分布（同时发生）。独立同时发生的话是概率的乘积，存在条件概率的情况同时发生的话就先算父节点再算子节点的条件概率。  
马尔科夫逻辑网络，估计考不了pass  
因果推理 前面概率图的延伸，固定条件对照然后计算差值，估计不考例子都没得  


## 第三章 搜索与问题求解
**盲目搜索** 
p24之前，就说了一个①搜索的范式，看一下p12、p16就行了  
盲目搜索指不重新排序open表的搜索  
②BFS 就是把n节点扩展的节点都放在open表的尾部（DFS是头部）  例子八数码华容道（不是八皇后） sb了，广度优先搜索前面的是把左上右下都找了一遍找到答案后就可以停止了  
DFS的问题是如果问题存在无穷分支则会在分支里出不来，因此引入搜索深度，但是你这样就必须知道问题的解的深度，也很僵硬。总结盲目搜索产生的无用节点较多，效率不高（算法题方向针对此优化剪枝等）  
③**启发式搜索**  
若能找到一种方法，用于提示open列表节点的排列顺序，选择最有希望的节点进行扩展，搜索效率将大幅提高。很多情况下，可通过检测来确定节点的合理顺序。优先考虑检测的搜索方法，称启发式搜索\有信息搜索。  
有序搜索，引入一个f代价值（已有代价+启发信息）对open表进行排序。p61例子。正确选择估价函数，对搜索结果具有决定作用，选择不当可能失去解。  

④ A\*算法 f估价函数的选择是选择最优解  
**启发式搜索的h估价函数（启发信息）是需要操作者根据问题来定的**  
首先介绍A算法，A算法就是前面说的使用f的算法，又分为全局择优（所有open表中节点排序）和局部择优（n节点扩展的节点排序后再加入open表）两种  
A\*算法中所谓的h\*这里指的离目标状态的距离是当前8个数字位与目标结点对应数字位距离和（不考虑中间路径） 例子p78  
⑤**与或树搜索**（是前述盲目和启发式搜索的底层替换，还是可以用二者）  
盲目搜索 就是加入判断可解不可解，p99例子 这样不可解的节点最后会留在open列表里移不出去，到最后自然就可以删除掉这些节点。  
与或树的启发式搜索，要计算所有解树的代价（代价计算方式p110）例子p112、p116与求和或取小找希望树（最小代价解树）这里似乎我扩展一层之后叶子节点的h值都直接给我了，这咋出题sai。    
⑥具体引申——博弈树（就是一种与或树）的启发式搜索（必考仔细理解）  
为什么说状态空间的大小是9！，这里指的是某一个走到终叶节点的子树，每次固定一个棋子，那么就是9！  
框框是A做出的选择，因此有多个框的情况就是A在选择


蒙特卡洛搜索 一大坨，待定  


## 第四章 不确定性推理 
人工智能主要就体现在求解不确定问题的能力上（核心问题之一）
p12不确定性推理的类型  
那个骰子看贝叶斯公式的例子其实是把1/2给约了，上下其实是有1/2的。那就把贝叶斯公式看成**新出现的事实对于先验的影响**，所谓知识的内化就好了。所谓的蒙提霍尔问题也可以看做动作主题的行为对于先验（每个盒子中有奖品的概率，先验是一个直观概率）的影响。    

经典概率方法（只考虑证据为真/假两种情况）和逆概率方法  
①主观贝叶斯方法  
联合概率公式：  
P(W1,W2,W3,…)=P(W1)×P(W2|W1)×P(W3|W2,W1)×…  
公式解释：句子中某个词出现的概率，只依赖于它前面有限K个词。  
朴素贝叶斯就是假设所有特征之间相互独立（新观察到的特征信息，例子中是周末、晴天、温度高等）实际情况下独立性假设并非总是成立。  
两个例子 武汉长江大桥和女孩去图书馆  
②可信度方法
理解之后套公式即可，想看个复杂例子就看例子2  
③证据理论（广义概率论）  
p68 就是概率论的推广，所有样本空间所有子集的概率分配函数之和为1。  
信任函数bel 某个子集的信任函数就该集合所有子集的概率分配函数值之和。  
似然函数pl 这里和之前说的似然函数还有点区别，下界  
大概明白怎么算就行了p84例子  
④模糊推理
引入所谓模糊集合的概念，元素不再属于或者不属于集合，而是有多大可能（隶属度）属于该集合。  
p117例子理解一下

## 第六章 智能计算
分布式的算法
1. 遗传算法
算函数最值的一个例子，p52给出了遗传算法的作用  
染色体交换和变异代际趋于无穷时一定能找到最优解
2. 蚁群算法
没有集中控制or不提供全局模型的情况下的分布式问题求解  
例子ppt最后的旅行商问题，感觉这个应该考不了  

## 第七章and第八章 机器学习
离开具体场景和问题，讨论采用何种机器学习算法毫无意义，应该在机器学习中合理引入已有先验假设对模型进行约束，以提升模型效果。  
其实神经网络的话层数多就包含了复杂度，包括CNN卷积核变多等等。  
前面的基础知识讨论ppt选看  
频率学派和贝叶斯学派的区别就在于是否考虑先验，先验是均匀分布还是随机变量。  

1. 贝叶斯学习/网络 贝叶斯最优分类器 贝叶斯朴素分类器  
贝叶斯网络表示一组变量的联合概率分布  
P(A,B,C,D)=P(A│B,C,D)P(B│C,D)P(C│D)P(D)  
如果有关系A到bc合到d的话就是下面这样  
P(A,B,C,D)=P(A)P(B│A)P(C│A)P(D|B,C)  
就是前面那个女孩去图书馆的例子，已观测到的数据D的每个样本都应该是具有很多个特征的。  
2. 线性回归又臭又长  最后公式都给你了估计直接带就行了  
3. 决策树必考  
例子p74  
注意gain是属性A对全集D的信息增益，选择最小的一个将根节点分类。
4. k-means KNN
5. SVM、ada boosting




12312
