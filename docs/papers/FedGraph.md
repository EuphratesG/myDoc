# FedGraph: Federated Graph Learning With Intelligent Sampling


已有的FL研究多集中在CNN，然而很多任务会产生图数据。由此进一步，传统GCN在联邦学习方面又很少被探索。本文提出的FedGraph建立在联邦学习多个client的基础上，每个client hold一个子图。用于解决GCN的两个已有问题：
1. 传统GCN需要在客户之间传递模型参数信息可能会导致信息泄露。cross-client convolution operation
2. 大规模图数据导致的高昂训练成本overhead。使用深度强化学习网络去做图采样（sampling）。

**这篇重点在2，强化学习的samplin上**


简述之前综述里提到的techniques，然而他们没有提到inter-graph的连接。  
本文的client不允许直接的数据共享。
本文工作：提出1，提出2，实验跑通它们获得性能优化。    

图采样 GCN训练时图数据量过大导致的问题，业界主要有两个策略：
1. 下一层随机采样当前层节点的邻居的子集，很多节点是公共邻居被重复计算
2. 按参数每层全随机采样节点，可能会选到和上层节点没边的节点

## 针对用户间传递模型参数导致信息泄露的问题  
保证安全通常使用加密传输或基于硬件设计，但这同样开销很大，不符合FedGraph高训练速度的要求。**cross-client convolution operation，embeds them into低维表示。**  
就是各个用户本地训练的第一层只使用本地数据，第二层开始才使用其他客户传的节点数据，这样就保护了元数据。因为即便拿到别人数据的用户可以通过本地参数矩阵去近似远程参数矩阵，但是这样最多做到预测出第二层的节点embedding，第一层的计算里还包含了j的邻接矩阵信息，没法推算。    

## 针对大规模图数据的问题（涉及强化学习）
暂时略

也就是说这篇所谓涉及边间信息重建的部分就是直接传跨用户节点的embedding，只不过不给元数据而已。  