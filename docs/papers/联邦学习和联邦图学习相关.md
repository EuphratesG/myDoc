# 联邦学习和联邦图学习相关
联邦学习，主要用于解决当无法直接获取到有效数据的时候，如何进行机器学习。换句话说是在数据不能直接使用的情况下，远端进行学习。关注重点，是在数据的安全和隐私得到保证下进行远端学习。分布式学习，是多机器并行模式。这里可以是：数据并行学习，模型合成（此时和联邦学习模式差不多，但是没有隐私限制），模型并行学习。关注重点，是如何能让机器学习**并行化**来提高学习速度。




把数据集split到用户之后一定会产生非独立同分布问题。  
生物医学、社交网络（都存在隐私数据问题）  
GNN常用的公开数据集大致有分子、蛋白质、论文引用网络、社交网络等这几类  
COLLAB 一共5000张图，3个领域  

分子数据集：MUTAG, BZR, COX2, DHFR, PTC_MR, AIDS, NCI1
蛋白质数据集：ENZYMES, DD, PROTEINS
社交网络数据集：COLLAB, IMDB-BINARY, IMDBMULTI  



MUTAG、PTC-MR、NCI109数据集为分子化合物数据集，每个graph代表一个硝基化合物分子，有两个类别，代表这个分子是诱变芳香族或杂芳香族。n表示结点数，m表示边的个数，N表示图的个数
DS_A.txt (m lines)：图的邻接矩阵，每一行的结构为(row, col)，即一条边。
DS_graph_indicator.txt (n lines)：表明结点属于哪一个图的文件。
DS_graph_labels.txt (N lines)：图的labels。
DS_node_labels.txt (n lines)：结点的labels。
DS_edge_labels.txt (m lines)：边labels。
DS_edge_attributes.txt (m lines)：边特征。
DS_node_attributes.txt (n lines)：结点的特征。
DS_graph_attributes.txt (N lines)：图的特征，可以理解为全局变量  
分子数据集  
AIDS 艾滋病筛选化合物数据集 node代表一种元素，一张图是一个化合物（共2000张图），图label代表原子或者离子化合物，边label代表化学价、node属性（元素、电荷、x、y）
蛋白质数据集：  
Protein数据集：每个节点是一个secondary structure elements，如果两个节点在氨基酸序列3D空间中是相邻节点就会存在一条边。  
社交网络数据集：  
IMDB数据集：每个节点代表一个演员，每条边代表两个演员是否出现在同一个电影里。IMDB-BINARY类别有两个：爱情片和动作片如果同时是爱情片和动作片的话，就会归类为动作片。IMDB-MULTI则在爱情片和动作片的基础上，加了一个科幻片类别。
COLLAB数据集：每个节点代表一个researcher，每个graph我猜是代表了一个科研团体？每一个Graph有一个类别，共有高能物理、凝聚态物理和天体物理3个类别。


FGML两个setting：
FL with structured data 训练时数据全部locally  
structured FL 用户之间存在client级别的图（用户之间存在relation）,但数据集不一定是图。    

相较于传统FL，FGML需要额外关注以下四个问题：

1. 跨用户端的遗失信息，各用户端只有本地子图，无法在节点表征阶段考虑到原本全局图中的邻居节点信息；  
2. 结构信息泄露，分享邻接矩阵或节点嵌入信息都有可能增加图结构信息泄露的风险；  
3. 跨用户端的数据异构性，不同用户端间的图结构/节点特征信息分布可能大相径庭，加剧了非独立同分布特性；  
4. 参数处理策略，尤其针对结构联邦，可以充分利用用户端间的结构信息设计更有效的参数聚合/更新策略；  

## 图学习、联邦学习简介  
图可以分为同质图和异质图（顶点或边有不同的type）  
图学习主要是去学图顶点的embedding/representation，专门搞出一个矩阵V*n的矩阵。图级别的representation可以由顶点级别的embedding池化而来。图又可以分为同构图和异构图（典型的像KGs和user-item图）  
传统FL：最后的loss就是对每个用户的本地数据集训练的loss做一个加权平均（根据各个用户那里的采样个数进行加权）。FL传统算法FedAvg：服务器和用户之间传输的数据仅仅只是模型参数model的θ矩阵。某次循环开始时服务器给selected的用户本轮的模型参数，用户在本地数据集训练模型，本地训练完毕后各个用户上传更新后的参数给server聚合，聚合完毕后开始下一轮。   
有趣的是GNN和FL都涉及了aggregate，GNN里面aggregate了邻居节点的信息更新本节点的信息，FL里面aggregate了selected用户对model参数的更新信息然后由服务器聚合新的model参数。  

FL with structured data 又分为每个用户本地数据集有很多图（一般是图层面的学习）或single的图/大图的子图（一般是顶点层面的学习）  
structured FL 用户之间存在client级别的关系图。值得注意的是structured FL用户本地的数据不一定非要是非欧式数据。  
![](https://cdn.jsdelivr.net/gh/EuphratesG/myPic@main/202310131118236.png)
## FL WITH STRUCTURED DATA
客户拥有私有的非欧式数据集，由服务器协调共同训练一个图学习模型。  
### Cross-Client Information Reconstruction
为了解决上面所述问题1 The existing techniques can be categorized as 重建跨用户的节点之间的边信息
intermediate result transmission and missing neighbor generation.  
1. intermediate result transmission：transmit node embeddings directly。（FedGraph）。为了避免用户子图元数据泄露，计算时第一层卷积不用其他用户的邻居节点数据。所谓第l层应该指的是参数矩阵.  问题：实际上很难保证原图的结构还有，传输开销也大
2. Missing Neighbor Generation  如果server不知道原图信息上面的就没法用了。每个用户都训练一个predictor去预测impaird 子图每个节点隐藏邻居节点的个数，和一个encoder去预测隐藏节点特征  

### Overlapping Instance Alignment
所谓重叠实例对齐，不同用户可见的子图可能会包含某些相同的节点，这样在子图中embedding计算完毕后会有多个结果。  解决这个问题的方法就是基于这些用户层面计算的结果去计算一个global的结果。全局图中的一个实例（节点或知识图谱中的实体）可能分散到了多个客户端，在计算其全局嵌入时需要进行对齐。  
1. Homogeneous Graph-Based Alignment  同构图重叠实例对齐目前的工作基本基于vertical FL。情况1所有客户端上都有一组节点V，但在不同客户端上节点属性和节点间关系不同，需要在中央服务器进行表征向量聚合：VFGNN对同一个顶点各个用户得到的H做combine（concat、mean、regression）
情况2客户端中有一个存有图数据结构信息，其它则只有节点属性，因此不能直接使用现有的图模型方法，可以通过计算近似矩阵进行结构信息共享：SGNN提出相似度矩阵，先在只包含feature的客户端用oh编码嵌入特性，然后根据矩阵计算节点embedding。 
2.  KG-Based Alignment 略
3.   User-Item Graph-Based Alignment 略

### Non-IID Data Adaptation
非独立同分布数据适配  
解决第三个问题，跨用户端的数据异构性（也是针对垂直FL而言的）  The intuition of mitigating the problem is
either to train an effective global model or to train specialized models for each client.
1.  Single Global Model-Based Methods 略
2.  Personalized Model-Based Methods 略

## STRUCTURED FL


数据隐私性（用户对于数据有完全的自主权）、传统ml通常数据比较集中在一个server  
用户自己数据不够，传统要将各个用户整合到服务器去train。联邦学习各个用户可以一起合作分别在自己设备上train不需要把数据发到放服务器。  
因为随着硬件发展，个人设备的算力也可以train了，并不是说完全依赖server去train  
fl主要通过交换加密的参数来保护用户的隐私  
水平fl（有相同的特征空间但没有重叠节点，样本的特征和标签是具备的，训练样本的维度也是一样的，只是训练样本分布在很多的边缘设备上）、垂直fl（有重叠节点但特征空间不同即节点的特征不一样）、联邦迁移学习  
简单一看  





