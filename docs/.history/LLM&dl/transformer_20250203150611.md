# transformer相关扣细节

1、[为什么 Transformer 需要进行 Multi-head Attention？](https://www.zhihu.com/question/341222779/answer/814111138)
-------------------------------------------------------------------------------------------------------------

可以类比 CNN 中同时使用**多个卷积核**的作用，直观上讲，不同随机初始化的多头的注意力**有助于网络捕捉到更丰富的特征 / 信息**。Multi-Head 其实不是必须的，去掉一些头效果依然有不错的效果（而且效果下降可能是因为参数量下降），这是因为在头足够的情况下，这些头已经能够有关注位置信息、关注语法信息、关注罕见词的能力了，再多一些头，无非是一种 enhance 或 noise 而已。  
**至于为什么每个head相比原dmodel要降维，参数少的同时特征空间多一点**。


2、transformer 是如何处理可变长度数据的？
-----------------------------------------------------------------------------------------

对不等长的数据，按照最长或者固定长度进行补齐，利用 padding mask 机制，补齐的数据并不参与训练，和 RNN 的 mask 机制一样。RNN 是通过 time step 方式处理的，transformer 在**计算 attention 得分矩阵时处理**。

3、Transformer 为什么 Q 和 K 使用不同的权重矩阵生成，为何不能使用同一个值进行自身的点乘？
------------------------------------------------------------------------------------------------------------------------

使用 Q/K/V 不相同可以保证在不同空间进行投影，增强了表达能力，提高了泛化能力。
- 如果 K 和 Q 使用不同的值的话, A 对 B 的重要程度是 Key(A) * Query(B), 而 B 对 A 的重要程度是 Key(B) * Query(A). 可以看出, A 对 B 的重要程度与 B 对 A 的重要程度是不同的. 然而, 如果 K 和 Q 是一样的, 那么 A 对 B 的重要程度和 B 对 A 的重要程度是一样的
- 对角线元素（词与自身的点积值）通常会较大，因为它们是自相关点积的结果。
然而，对角线值并不一定总是每一行的最大值，因为随机初始化可能导致某些非对角线元素意外偏高。导致对其他词的关注变少，对本身的关注变多。  





5、Transformer 计算 attention 的时候为何选择点乘而不是加法？两者计算复杂度和效果上有什么区别？
-----------------------------------------------------------

为了计算更快。矩阵加法在加法这一块的计算量确实简单，但是作为一个整体计算 attention 的时候相当于一个隐层，整体计算量和点积相似。在效果上来说，从实验分析，两者的效果和 dk 相关，dk 越大，加法的效果越显著

6、[为什么在进行 softmax 之前需要对 attention 进行 scaled（为什么除以 dk 的平方根），并使用公式推导进行讲解](https://www.zhihu.com/question/339723385/answer/782509914)
----------------------------------------------------------------------------------------------------------------------------------



在 Transformer 中，Query (\( Q \)) 和 Key (\( K \)) 的内积的方差与向量的维度 \( d_k \) 正相关，主要是由于随机变量的累积效应。**内积方差大了会导致softmax之后趋向于one hot，因为指数函数本身也在拉大差距的原因**。具体解释如下：


根据随机变量的加法方差公式：
\[
\text{Var}(a + b) = \text{Var}(a) + \text{Var}(b) \quad \text{（独立随机变量）}
\]

对于 \( Q \cdot K = \sum_{i=1}^{d_k} q_i k_i \)，其方差为：
\[
\text{Var}(Q \cdot K) = \sum_{i=1}^{d_k} \text{Var}(q_i k_i)
\]

因为 \( q_i \) 和 \( k_i \) 是独立随机变量，且每个 \( q_i k_i \) 的期望值为 0，方差为 1：

- \( \mathbb{E}[X^2] \) 和 \( \mathbb{E}[Y^2] \) 分别是 \( X \) 和 \( Y \) 的二次矩（即它们的方差加上它们的均值的平方）。

\[
\text{Var}(q_i k_i) = \mathbb{E}[q_i^2] \cdot \mathbb{E}[k_i^2] - (\mathbb{E}[q_i] \cdot \mathbb{E}[k_i])^2
\]
\[
\text{Var}(q_i k_i) = \text{Var}(q_i) \cdot \text{Var}(k_i) = 1 \cdot 1 = 1
\]

因此：
\[
\text{Var}(Q \cdot K) = d_k \cdot 1 = d_k
\]

- 方差与 \( d_k \) **线性正相关**。
- \( d_k \) 越大，内积结果的波动范围（方差）就越大。



### **. 示例直观解释**
假设 \( Q = [q_1, q_2, \dots, q_{d_k}] \), \( K = [k_1, k_2, \dots, k_{d_k}] \)：
1. 若 \( d_k = 2 \)：
   \[
   Q \cdot K = q_1 k_1 + q_2 k_2
   \]
   结果主要取决于两个元素的积，数值波动较小。

2. 若 \( d_k = 100 \)：
   \[
   Q \cdot K = \sum_{i=1}^{100} q_i k_i
   \]
   此时累积了 100 个独立随机变量的贡献，总方差变为 \( \text{Var}(Q \cdot K) = 100 \)，数值波动显著增大。



**在d较大时，softmax 将几乎全部的[概率分布](https://zhida.zhihu.com/search?content_id=168161579&content_type=Article&match_order=1&q=%E6%A6%82%E7%8E%87%E5%88%86%E5%B8%83&zhida_source=entity)都分配给了最大值对应的标签**。也就是说**极大的[点积值](https://zhida.zhihu.com/search?content_id=168161579&content_type=Article&match_order=1&q=%E7%82%B9%E7%A7%AF%E5%80%BC&zhida_source=entity)将整个 softmax 推向梯度平缓区，使得收敛困难**，**梯度消失为 0，造成参数更新困难**。

7、在计算 attention score 的时候如何对 padding 做 mask 操作？
-----------------------------------------------

这里是因为 padding 都是 0，e0=1, 但是 softmax 的函数，也会导致为 padding 的值占全局一定概率，mask 就是让这部分值取无穷小，让他再 softmax 之后基本也为 0，不去影响非 attention socore 的分布 mask为True的位置会被设为-1e9

8、[transformer 为什么使用 layer normalization，而不是其他的归一化方法？](https://zhuanlan.zhihu.com/p/360144789/https%3C/i%3E://www.zhihu.com/question/395811291/answer/1257223002)
-----------------------------------------------------------------------------------------------------------------------------------------------------------------

BatchNorm是对一个batch-size样本内的每个特征[分别]做归一化（例如32张图片的一个通道做归一化），LayerNorm是[分别]对每个样本的所有特征做归一化（例如一个句子的所有词内部归一化，句子是样本词是特征）。这样就好理解了。

9、[在测试或者预测时，Transformer 里 decoder 为什么还需要 seq mask？](https://zhuanlan.zhihu.com/p/360144789/%3Ci%3Ehttps://www%3C/i%3E.zhihu.c%3Ci%3Eom/%3C/i%3Equestion/369075515/answer/994819222)
-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
因为输出下一个词时不能让模型知道之后的信息，要一个一个输出。训练的时候teacher-forcing知道所有信息。  

10、[Transformer 不同 batch 的长度可以不一样吗？还有同一 batch 内为什么需要长度一样？](https://zhuanlan.zhihu.com/p/360144789/ht%3C/i%3Etps://www.zhihu.com/questio%3Ci%3En/4%3C/i%3E39438113/answer/1714391336)
------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
- 不同 batch 的句子 token 数量可以不同，pytorch在并行时把seq_len填充到一致再padding
- 同一 batch 内，所有句子的 token 数量 必须一致。 
- 向量维度（token 嵌入维度） 在所有句子中是相同的，不受句子长度影响。

11、[Transformer 的 Positional embedding 为什么有用？](https://www.zhihu.com/question/385895601/answer/1146997944)
----------------------------------------------------------------------------------------------------------
可固定位置编码也可用可训练的位置编码

12、[Transformer 在哪里做了权重共享，为什么可以做权重共享？好处是什么？](https://www.zhihu.com/question/333419099/answer/743341017)
-------------------------------------------------------------------------------------------------------

Transformer 在两个地方进行了权重共享：

**（1）**Encoder 和 Decoder 间的 Embedding 层权重共享；

**（2）**Decoder 中 Embedding 层和 FC 层权重共享。

**对于（1）**，《[Attention is all you need](https://zhida.zhihu.com/search?content_id=168161579&content_type=Article&match_order=1&q=Attention+is+all+you+need&zhida_source=entity)》中 Transformer 被应用在机器翻译任务中，源语言和目标语言是不一样的，但它们可以共用一张大词表，对于两种语言中共同出现的词（比如：数字，标点等等）可以得到更好的表示，而且对于 Encoder 和 Decoder，**嵌入时都只有对应语言的 embedding 会被激活**，因此是可以共用一张词表做权重共享的。

论文中，Transformer 词表用了 bpe 来处理，所以最小的单元是 subword。英语和德语同属[日耳曼语族](https://zhida.zhihu.com/search?content_id=168161579&content_type=Article&match_order=1&q=%E6%97%A5%E8%80%B3%E6%9B%BC%E8%AF%AD%E6%97%8F&zhida_source=entity)，有很多相同的 subword，可以共享类似的语义。而像中英这样相差较大的语系，语义共享作用可能不会很大。

但是，共用词表会使得词表数量增大，增加 softmax 的计算时间，因此实际使用中是否共享可能要根据情况权衡。


**对于（2）**，Embedding 层可以说是通过 onehot 去取到对应的 embedding 向量，FC 层可以说是相反的，通过向量（定义为 x）去得到它可能是某个词的 softmax 概率，取概率最大（贪婪情况下）的作为预测值。

那哪一个会是概率最大的呢？在 FC 层的每一行量级相同的前提下，理论上和 x 相同的那一行对应的点积和 softmax 概率会是最大的（可类比本文问题 1）。

因此，Embedding 层和 FC 层权重共享，[Embedding 层](https://zhida.zhihu.com/search?content_id=168161579&content_type=Article&match_order=5&q=Embedding%E5%B1%82&zhida_source=entity)中和向量 x 最接近的那一行对应的词，会获得更大的预测概率。实际上，Decoder 中的 **Embedding 层和 FC 层有点像互为逆过程,不一定哪个词，这样参数共享相当于加速训练了**。

通过这样的权重共享可以减少参数的数量，加快收敛。

但开始我有一个困惑是：Embedding 层参数维度是：(v,d)，FC 层参数维度是：(d,v)，可以直接共享嘛，还是要转置？其中 v 是词表大小，d 是 [embedding 维度](https://zhida.zhihu.com/search?content_id=168161579&content_type=Article&match_order=1&q=embedding%E7%BB%B4%E5%BA%A6&zhida_source=entity)。

查看 pytorch 源码发现真的可以直接共享：

```
fc = nn.Linear(d, v, bias=False)    # Decoder FC层定义
​
weight = Parameter(torch.Tensor(out_features, in_features))   # Linear层权重定义

```

Linear 层的权重定义中，是按照 (out_features, in_features) 顺序来的，实际计算会先将 weight 转置在乘以输入矩阵。所以 FC 层 对应的 Linear 权重维度也是 (v,d)，可以直接共享。

13、transformer 一个 block 中最耗时的部分？
--------------------------------
在 **Transformer** 中，一个 **block** 通常由两部分组成：**多头自注意力机制（Multi-Head Self-Attention）** 和 **前馈神经网络（Feedforward Neural Network, FFN）**。其中，最耗时的部分通常是 **多头自注意力机制**，尤其是 **自注意力计算** 的步骤。

### 1. **多头自注意力机制（Multi-Head Self-Attention）**

自注意力机制（Self-Attention）是 Transformer 中的核心计算部分，尤其是在 **计算注意力权重（Attention Weights）** 时，消耗了大量计算资源。

#### 为什么多头自注意力最耗时？

- **点积计算：** 自注意力机制的核心计算是 **Query（Q）**、**Key（K）** 和 **Value（V）** 的点积操作。对于每个位置的 token，模型需要计算其与其他所有 token 的关系（即点积），这需要计算 **N x N** 的矩阵（其中 N 是序列长度）。计算这些点积操作的复杂度是 \( O(N^2 \cdot d) \)，其中 \( d \) 是每个 token 的维度。
  
- **矩阵操作：** 自注意力需要计算一个 **注意力矩阵**，然后根据注意力权重加权输入的 **Value** 向量。这个操作本质上是一个矩阵乘法，计算量较大。即使对于一个相对较短的序列，随着序列长度的增加，这个计算量是 **二次增长** 的。

- **多头机制：** Transformer 中的多头自注意力将注意力机制拆成多个头，每个头处理不同的子空间，这会增加计算量。假设有 \( h \) 个头，每个头的维度是 \( d_k \)，那么每个头的计算复杂度是 \( O(N^2 \cdot d_k) \)，总的计算量是 \( O(N^2 \cdot d_k \cdot h) \)。总的计算复杂度通常是 \( O(N^2 \cdot d) \)，其中 \( d \) 是每个头的维度和头数的总和。

- **并行计算的挑战：** 虽然 Transformer 可以并行计算，但每个 token 仍然需要和其他 token 进行 **全局交互**，这使得自注意力计算的 **并行化效率**受到一定限制，尤其是在长序列的情况下，计算开销较大。

### 2. **前馈神经网络（Feedforward Neural Network, FFN）**

- 在 Transformer 中，FFN 由两个线性变换和一个激活函数（通常是 ReLU）组成，计算量比多头自注意力小。
- 假设 FFN 的隐藏层大小是 \( d_{\text{ff}} \)，那么 FFN 的复杂度是 \( O(N \cdot d_{\text{model}} \cdot d_{\text{ff}}) \)，比自注意力的复杂度 \( O(N^2 \cdot d_{\text{model}}) \) 要小很多，尤其是当 \( N \) 较大时。

### 3. **总结**

- **最耗时的部分：** 在一个 Transformer block 中，最耗时的部分通常是 **多头自注意力机制**，尤其是 **自注意力计算**。计算 **注意力权重** 的复杂度是 **二次增长的（\( O(N^2) \)）**，这对于长序列来说是一个计算瓶颈。
- **前馈神经网络：** 相对而言，前馈神经网络的计算量要小很多，主要是由两个线性变换组成，复杂度为 **\( O(N \cdot d_{\text{model}} \cdot d_{\text{ff}}) \)**。

因此，在 Transformer 的每个 block 中，最耗时的部分通常是 **自注意力计算**，尤其是在序列长度较长时。  

在 Transformer 中，比较 **前馈神经网络（Feedforward Neural Network, FFN）** 和 **多头自注意力机制（Multi-Head Self-Attention）** 的 **参数量**，通常情况下， **FFN** 的参数量会 **比多头自注意力机制** 更大。以下是两者的详细比较：

### 1. **多头自注意力（Multi-Head Self-Attention）**

对于一个 Transformer block 中的多头自注意力机制，假设：
- 输入的嵌入维度为 \( d_{\text{model}} \)。
- 使用 \( h \) 个注意力头。
- 每个注意力头的维度为 \( d_k \)（通常，\( d_k = d_{\text{model}} / h \)）。

#### 参数量
多头自注意力的核心计算涉及 **Query（Q）**、**Key（K）** 和 **Value（V）** 的线性变换，它们是通过权重矩阵与输入的嵌入向量进行矩阵乘法得到的。
- 对于每个注意力头，生成 Q、K、V 所需的参数量是：
  \[
  \text{Parameters for each head} = 3 \times (d_{\text{model}} \times d_k)
  \]
  其中，Q、K、V 权重矩阵的大小都是 \( d_{\text{model}} \times d_k \)。
- 总的多头自注意力参数量：
  \[
  \text{Total Parameters for multi-head attention} = 3 \times d_{\text{model}} \times d_k \times h
  \]
  其中 \( h \) 是注意力头的数量。

最后，注意力计算后的结果还需要进行一个 **线性变换**（用于合并多头注意力的输出），因此还需要一个额外的权重矩阵：
\[
\text{Final Linear Transformation Parameters} = d_{\text{model}} \times d_{\text{model}}
\]

因此，多头自注意力机制的总参数量是：
\[
\text{Total Parameters for multi-head attention} = 3 \times d_{\text{model}} \times d_k \times h + d_{\text{model}}^2
\]
其中 \( d_k = d_{\text{model}} / h \)，所以可以简化为：
\[
\text{Total Parameters for multi-head attention} = 3 \times d_{\text{model}}^2 + d_{\text{model}}^2 = 4 \times d_{\text{model}}^2
\]
（假设 \( d_{\text{model}} \) 是一个能被 \( h \) 整除的数）

### 2. **前馈神经网络（Feedforward Neural Network, FFN）**

每个 Transformer block 中的 **前馈神经网络** 通常包括两个线性变换（一个升维和一个降维），并且通常会加入 **激活函数（如 ReLU）** 在中间。假设 FFN 的隐藏层大小为 \( d_{\text{ff}} \)，通常 \( d_{\text{ff}} \) 要比 \( d_{\text{model}} \) 大很多（例如，2048 与 512 或 1024）。

#### 参数量
- 输入到 FFN 的嵌入大小是 \( d_{\text{model}} \)。
- 第一个线性变换（升维）将输入从 \( d_{\text{model}} \) 转换到 \( d_{\text{ff}} \)，所需参数量是：
  \[
  \text{Parameters for first linear layer} = d_{\text{model}} \times d_{\text{ff}}
  \]
- 第二个线性变换（降维）将输出从 \( d_{\text{ff}} \) 转换回 \( d_{\text{model}} \)，所需参数量是：
  \[
  \text{Parameters for second linear layer} = d_{\text{ff}} \times d_{\text{model}}
  \]
- 因此，前馈神经网络的总参数量是：
  \[
  \text{Total Parameters for FFN} = d_{\text{model}} \times d_{\text{ff}} + d_{\text{ff}} \times d_{\text{model}}
  \]
  由于两个部分是对称的，因此可以简化为：
  \[
  \text{Total Parameters for FFN} = 2 \times d_{\text{model}} \times d_{\text{ff}}
  \]

### 3. **比较**

- **多头自注意力（Multi-Head Attention）** 的参数量是 \( 4 \times d_{\text{model}}^2 \)。
- **前馈神经网络（Feedforward Network, FFN）** 的参数量是 \( 2 \times d_{\text{model}} \times d_{\text{ff}} \)。

如果假设 \( d_{\text{ff}} \) 比 \( d_{\text{model}} \) 大很多（例如，2048 vs 512），那么 **FFN 的参数量会更大**。通常， \( d_{\text{ff}} \) 可能是 \( d_{\text{model}} \) 的 4 到 8 倍，甚至更大，这样 **FFN 的参数量** 将比 **多头自注意力的参数量** 更大。

### 举个例子：

假设 \( d_{\text{model}} = 512 \) 和 \( d_{\text{ff}} = 2048 \)：
- 多头自注意力的参数量：\( 4 \times 512^2 = 1,048,576 \)
- 前馈神经网络的参数量：\( 2 \times 512 \times 2048 = 2,097,152 \)

在这种情况下，**前馈神经网络的参数量会是多头自注意力的两倍**。

### 总结

- 如果 **\( d_{\text{ff}} \)（前馈网络的隐藏层维度）远大于 \( d_{\text{model}} \)**，那么 **FFN 的参数量通常会比多头自注意力的参数量多**。
- 在大多数标准配置中，前馈神经网络的参数量往往较大，尤其是在 \( d_{\text{ff}} \) 被设计为远大于 \( d_{\text{model}} \) 时。


14、[Transformer 使用 positionencoding 会影响输入 embedding 的原特征吗？](https://zhuanlan.zhihu.com/p/360144789/http%3Ci%3Es://www.zhihu.co%3C/i%3Em/question/350116316/answer/864616018)
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------

不会，只是一个加和过程


16、**Self-Attention 的时间复杂度是怎么计算的？**
-----------------------------------

Self-Attention 时间复杂度： $O(n^2 \cdot d)$O(n^2 \cdot d) ，这里，n 是序列的长度，d 是 embedding 的维度。

Self-Attention 包括**三个步骤：相似度计算，softmax 和加权平均**，它们分别的时间复杂度是：

**相似度计算可以看作大小为 (n,d) 和(d,n)的两个矩阵相乘**： $(n,d)*(d,n)=O(n^2 \cdot d)$(n,d)*(d,n)=O(n^2 \cdot d) ，得到一个 (n,n) 的矩阵

softmax 就是直接计算了，时间复杂度为 $O(n^2)$O(n^2)

加权平均可以看作大小为 (n,n) 和(n,d)的两个矩阵相乘： $(n,n)*(n,d)=O(n^2 \cdot d)$(n,n)*(n,d)=O(n^2 \cdot d) ，得到一个 (n,d) 的矩阵

因此，Self-Attention 的时间复杂度是 $O(n^2 \cdot d)$O(n^2 \cdot d) 。

这里再分析一下 Multi-Head Attention，它的作用类似于 CNN 中的多核。

多头的实现不是循环的计算每个头，而是通过 transposes and reshapes，用[矩阵乘法](https://zhida.zhihu.com/search?content_id=168161579&content_type=Article&match_order=1&q=%E7%9F%A9%E9%98%B5%E4%B9%98%E6%B3%95&zhida_source=entity)来完成的。

> In practice, the multi-headed attention are done with transposes and reshapes rather than actual separate tensors. —— 来自 google BERT 源码

Transformer/BERT 中把 **d ，**也就是 hidden_size/embedding_size 这个维度做了 reshape 拆分，可以去看 Google 的 TF [源码](https://link.zhihu.com/?target=https%3A//github.com/google-research/bert)或者上面的 pytorch 源码：

> hidden_size (d) = num_attention_heads (m) * attention_head_size (a)，也即 d=m*a

并将 num_attention_heads 维度 transpose 到前面，使得 Q 和 K 的维度都是 (m,n,a)，这里不考虑 batch 维度。

这样点积可以看作大小为 (m,n,a) 和(m,a,n)的两个张量相乘，得到一个 (m,n,n) 的矩阵，其实就相当于 (n,a) 和(a,n)的两个矩阵相乘，做了 m 次，时间复杂度（感谢评论区指出）是 $O(n^2 \cdot m \cdot a)=O(n^2 \cdot d)$O(n^2 \cdot m \cdot a)=O(n^2 \cdot d) 。

张量乘法时间复杂度分析参见：[矩阵、张量乘法的时间复杂度分析](https://link.zhihu.com/?target=https%3A//liwt31.github.io/2018/10/12/mul-complexity/)

因此 Multi-Head Attention 时间复杂度也是 $O(n^2 \cdot d)$O(n^2 \cdot d) ，复杂度相较单头并没有变化，主要还是 transposes and reshapes 的操作，相当于把一个大矩阵相乘变成了多个小矩阵的相乘。




20、简单介绍一下 Transformer 的位置编码？有什么意义和优缺点？
--------------------------------------

参见：ref="[https://z](https://link.zhihu.com/?target=https%3A//z)_huan_[http://lan.zhihu.com/p/106644634](http://lan.zhihu.com/p/106644634)"> 一文读懂 Transformer 模型的位置编码

21、你还了解哪些关于位置编码的技术，各自的优缺点是什么？
-----------------------------

参见：[如何优雅地编码文本中的位置信息？三种 positionalencoding 方法简述](https://link.zhihu.com/?target=https%3A//zhuanlan.zh%253Ci%253Eihu.com/%253C/i%253Ep/121126531)

[让研究人员绞尽脑汁的 Transformer 位置编码](https://zhuanlan.zhihu.com/p/360144789/h%3Ci%3Ettps://zhuanlan%3C/i%3E.zhihu.com/p/352898810)

22、简单讲一下 Transformer 中的残差结构以及意义。
--------------------------------

防止梯度消失，帮助深层网络训练


24、简答讲一下 BatchNorm 技术，以及它的优缺点。
------------------------------

参见：[https://zhuanlan.zhihu.com/p/153183322](https://zhuanlan.zhihu.com/p/153183322)  
BN 的理解重点在于它是针对整个 Batch 中的样本在同一维度特征在做处理。  
在 MLP 中，比如我们有 10 行 5 列数据。5 列代表特征，10 行代表 10 个样本。是对第一个特征这一列（对应 10 个样本）做一次处理，第二个特征（同样是一列）做一次处理，依次类推。  
在 CNN 中扩展，我们的数据是 N·C·H·W。其中 N 为样本数量也就是 batch_size，C 为通道数，H 为高，W 为宽，BN 保留 C 通道数，在 N,H,W 上做操作。比如说把第一个样本的第一个通道的数据，第二个样本第一个通道的数据..... 第 N 个样本第一个通道的数据作为原始数据，处理得到相应的均值和方差。  
**BN 有两个优点。**  
第一个就是可以解决内部[协变量](https://zhida.zhihu.com/search?content_id=168161579&content_type=Article&match_order=1&q=%E5%8D%8F%E5%8F%98%E9%87%8F&zhida_source=entity)偏移，简单来说训练过程中，各层分布不同，增大了学习难度，BN 缓解了这个问题。当然后来也有论文证明 BN 有作用和这个没关系，而是可以使损失平面更加的平滑，从而加快的[收敛速度](https://zhida.zhihu.com/search?content_id=168161579&content_type=Article&match_order=1&q=%E6%94%B6%E6%95%9B%E9%80%9F%E5%BA%A6&zhida_source=entity)。  
第二个优点就是缓解了梯度饱和问题（如果使用 sigmoid [激活函数](https://zhida.zhihu.com/search?content_id=168161579&content_type=Article&match_order=1&q=%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0&zhida_source=entity)的话），加快收敛。  
**BN 的缺点：**  
第一个，batch_size 较小的时候，效果差。这一点很容易理解。BN 的过程，使用 整个 batch 中样本的均值和方差来模拟全部数据的均值和方差，在 batch_size 较小的时候，效果肯定不好。  
第二个缺点就是 BN 在 RNN 中效果比较差。这一点和第一点原因很类似，不过我单挑出来说。  
首先我们要意识到一点，就是 RNN 的输入是长度是动态的，就是说每个样本的长度是不一样的。  
举个最简单的例子，比如 batch_size 为 10，也就是我有 10 个样本，其中 9 个样本长度为 5，第 10 个样本长度为 20。  
那么问题来了，前五个单词的均值和方差都可以在这个 batch 中求出来从而模型真实均值和方差。但是第 6 个单词到底 20 个单词怎么办？  
只用这一个样本进行模型的话，不就是回到了第一点，batch 太小，导致效果很差。  
第三个缺点就是在测试阶段的问题，分三部分说。  
首先测试的时候，我们可以在队列里拉一个 batch 进去进行计算，但是也有情况是来一个必须尽快出来一个，也就是 batch 为 1，这个时候均值和方差怎么办？  
这个一般是在训练的时候就把均值和方差保存下来，测试的时候直接用就可以。那么选取效果好的均值和方差就是个问题。  
其次在测试的时候，遇到一个样本长度为 1000 的样本，在训练的时候最大长度为 600，那么后面 400 个单词的均值和方差在训练数据没碰到过，这个时候怎么办？  
这个问题我们一般是在数据处理的时候就会做截断。  
还有一个问题就是就是[训练集](https://zhida.zhihu.com/search?content_id=168161579&content_type=Article&match_order=1&q=%E8%AE%AD%E7%BB%83%E9%9B%86&zhida_source=entity)和测试集的均值和方差相差比较大，那么训练集的均值和方差就不能很好的反应你测试数据特性，效果就回差。这个时候就和你的数据处理有关系了。

25、简单描述一下 Transformer 中的[前馈神经网络](https://zhida.zhihu.com/search?content_id=168161579&content_type=Article&match_order=1&q=%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&zhida_source=entity)？使用了什么激活函数？相关优缺点？
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

前馈神经网络采用了两个[线性变换](https://zhida.zhihu.com/search?content_id=168161579&content_type=Article&match_order=1&q=%E7%BA%BF%E6%80%A7%E5%8F%98%E6%8D%A2&zhida_source=entity)，激活函数为 Relu，公式如下：

$FFN(x) = max(0, xW_1 + b_1) W_2 + b_2$ FFN(x) = max(0, xW_1 + b_1) W_2 + b_2

优点：

SGD 算法的收敛速度比 sigmoid 和 tanh 快；（梯度不会饱和，解决了[梯度消失问题](https://zhida.zhihu.com/search?content_id=168161579&content_type=Article&match_order=1&q=%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E9%97%AE%E9%A2%98&zhida_source=entity)）

计算复杂度低，不需要进行指数运算；

适合用于后向传播。

缺点：

ReLU 的输出不是 zero-centered；

ReLU 在训练的时候很” 脆弱”，一不小心有可能导致神经元” 坏死”。举个例子：由于 ReLU 在 x<0 时梯度为 0，这样就导致负的梯度在这个 ReLU 被置零，而且这个神经元有可能再也不会被任何数据激活。如果这个情况发生了，那么这个神经元之后的梯度就永远是 0 了，也就是 ReLU 神经元坏死了，不再对任何数据有所响应。实际操作中，如果你的 learning rate 很大，那么很有可能你网络中的 40% 的神经元都坏死了。 当然，如果你设置了一个合适的较小的 learning rate，这个问题发生的情况其实也不会太频繁。，Dead ReLU Problem（神经元坏死现象）：某些神经元可能永远不会被激活，导致相应参数永远不会被更新（在负数部分，梯度为 0）。产生这种现象的两个原因：参数初始化问题；learning rate 太高导致在训练过程中参数更新太大。 解决方法：采用 Xavier 初始化方法，以及避免将 learning rate 设置太大或使用 adagrad 等自动调节 learning rate 的算法。

ReLU 不会对数据做幅度压缩，所以数据的幅度会随着模型层数的增加不断扩张。





28、Transformer的并行体现在哪个地方？Decoder端可以做并行化吗？
------------------------------------------------------

RNN的每一个样本（一个句子）内部必须一步一步地计算完成，而CNN和Self-Attention则可以完全并行计算。这里说的不能并行指的是每一个样本内部（模型并行），不同样本彼此之间是可以进行并行计算的（数据并行）。  

---

### **1. 训练阶段的并行性**
- **Transformer的并行性**：
  - **自注意力机制**：可同时计算序列中所有位置之间的关联，通过矩阵运算（如Q、K、V的并行计算）一次性处理整个输入序列，无需像RNN那样逐步处理。
  - **前馈网络（FFN）**：可并行处理所有位置的输出。
  - **批处理（数据并行）**：同时处理多个样本（批量输入），利用GPU的并行计算能力加速训练。
  
- **GPT的并行性**：
  - **掩码自注意力**：训练时通过掩码隐藏未来位置信息，但所有位置的注意力计算仍可并行完成（通过矩阵掩码实现，teacher forcing）。
  - **大规模批处理**：与Transformer类似，通过同时处理多个文本序列提升训练效率。

---

### **2. 推理阶段的并行性**
- **Transformer（仅编码器）**：
  - 输入序列已知，可一次性并行计算所有位置的输出（如机器翻译中的编码器）。

- **GPT（自回归模型）**：
  - **单序列推理**：生成文本时需逐个预测下一个Token（自回归），序列内部无法并行。
  - **批处理优化**：同时处理多个独立序列（如同时响应多个用户请求），利用GPU并行计算加速。
  - **KV缓存**：缓存历史Token的Key-Value向量，减少重复计算，间接提升效率。



29、简单描述一下 wordpiece model 和 byte pair encoding，有实际应用过吗？
-------------------------------------------------------

参见：[深入理解 NLP Subword 算法：BPE、WordPiece、ULM](https://link.zhihu.com/?target=https%3A//zhuan%253Ci%253Elan.zhihu%253C/i%253E.com/p/86965595)

30、Transformer 训练的时候学习率是如何设定的？Dropout 是如何设定的，位置在哪里？Dropout 在测试的需要有什么需要注意的吗？
---------------------------------------------------------------------------

31、[有哪些令你印象深刻的魔改 transformer？](https://zhuanlan.zhihu.com/p/360144789/htt%3Ci%3Eps://www.zh%3C/i%3Eihu.com/question/349958732/answer/945349902)
-----------------------------------------------------------------------------------------------------------------------------------------------
32、transformer是否可以引入预训练的词向量？
--------------
可以但不一定有效果，不一定有直接随机初始化而训练的好  


