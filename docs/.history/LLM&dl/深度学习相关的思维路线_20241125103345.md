# 深度学习相关的延伸


## 损失函数和极大似然估计

回归问题的平方损失函数是在假设存在高斯噪声的


## attention和MLP有何不同
所以，注意力机制在尽可能减少算力压力的情况下，达到近似三阶张量全连接层参数量的效果，并且序列内部互相关联的机制使得它更能把握序列的特征。与二阶张量（矩阵）形式的 MLP 是不同的。
qkt和w相比肯定sai
## 样本比例问题
一看就没经受过毒打在样本比例不均衡的场景下例如正负样本1:9你不用正负样本1:1来训模型马上就收敛到全预测为负样本这样loss很低，准确率90%然后你就会发现训练出来的模型就是个粑粑然后你想救 发现救不回来 因为负样本的梯度强度是正样本的九倍模型不可避免地被负样本拐跑了你说那就给正样本加九倍的权重相当于一个正样本复制了九次那不还是正负样本1:1么而且我的实践经验是9倍正样本权重带9个负样本来训练的效果还不如正负样本1:1那不最后成了脱裤子放屁，多此一举？炼丹对前辈还是要有敬畏之心否则回旋镖打到的是你自己这个问题，原本只是回答入门初学者的疑问，但是已经被讨论到了，它本不该有深度。要把样本比例，在各种场景，各种任务下，都讨论清楚，那够写好几篇万字长文了。

我们处理不平衡数据的出发点是，训练集的数据相对于真实数据是有偏的，所以我们试图用一些技巧尽量去学到一个偏差尽量小的模型。但是也也有很多情况，训练集和真实样本空间中是一致的，尽管正负样本的比例并不是1:1，但是他仍然与真实样本空间相符合，这时候似乎就没有使用这些技巧的必要性了


## 什么是vllm？
vLLM 是一种旨在优化大语言模型（LLM）推理性能的高效推理引擎，专注于解决传统 LLM 推理过程中内存管理低效的问题，从而实现更快的推理速度和更低的硬件资源消耗。
PagedAttention 是 vLLM 中的核心技术，专门设计用于优化大语言模型（LLM）推理时的内存管理。它通过引入“分页”的动态内存管理机制，有效解决了传统方法中 Key-Value (KV) 缓存分配的低效问题。

注意gpt单decoder的结构是把cross-attention的模块去掉了的，只有masked-attention和FFN。为什么要kv cache？训练的时候没有，因为训练的时候基于teacher-forcing，每次都是给所有的上下文。推理的时候自回归迭代，才需要kvcache。注意大模型推理分两个阶段，第一个是解析prompt并保存kv cache和产生第一个token，第二个阶段每个循环只会输入一个新生成的token提供q，而把新token的kv计算出来之后再和kv cache并在一起做attention。因此为什么没有q cache就知道了，因为没必要啊只有当前一个q。  

PagedAttention 是一种高效管理 Key-Value Cache 的机制，它将缓存分块存储在内存和磁盘之间，适配长序列生成场景。以下通过一个具体实例，说明 PagedAttention 如何在推理中高效利用缓存生成 Token。

---

### **场景描述**
#### **任务：生成故事文本**  
Prompt:  
`"Once upon a time in a faraway land, a young prince named Arin"`

目标：基于这个 Prompt，生成后续 Token。假设序列长度非常长，超出 GPU 内存的直接缓存能力。

#### **挑战**  
- 当前模型序列长度 \(L = 50,000\)（每个 Token 的 Key 和 Value 是 \(d=16,384\) 维）。  
- 如果全缓存，KV Cache 大小为 \(L \times d \times 2 = 50,000 \times 16,384 \times 2 \approx 1.6\ \text{GB}\)。
- 单 GPU 显存不够，PagedAttention 引入分块管理缓存。


### **PagedAttention 的优势**
1. **动态内存管理**：通过分页机制，将长序列的 Key 和 Value 缓存分为活跃页（存储于 GPU）和非活跃页（存储于 CPU 或磁盘），显著降低显存需求。
2. **上下文扩展能力强**：即使序列长度达到百万级，也可以通过分块管理确保模型可以高效访问全部上下文。
3. **按需调度**：仅在需要时加载非活跃页，减少不必要的数据传输和计算。

PagedAttention 的这种优化对于大模型长序列推理尤为重要，既保证了内存效率，又提供了生成时的上下文完整性。



## gbdt是什么
Gradient Boosting Decision Tree，梯度提升决策树  
属于提升算法（Boosting）的一种。它通过逐步构建多个决策树，并结合每个树的预测结果来提高模型的准确性。GBDT的核心思想是通过优化目标函数（通常是损失函数）来减少误差，并在每一步迭代中使用梯度下降来修正前一轮预测的残差。

## moe
研究低算力slm 产生的灵感，我或许不需要那么多领域的覆盖，所以研究混合专家，做有限领域垂直应用。就好比人类社会，各有分工，简洁明了。  
同时直观想法横向拓展参数量，理论上就应该有提升。  
如何提升MOE并行的效率，专家之间的网络通信会成为计算的瓶颈。而且GPU擅长做矩阵运算，不擅长做分支；每个专家小模型分配的样本数较少，无法得到充分的训练；（是否可以先训练出若干个完整的大模型，例如数学大模型、代码大模型、问答大模型等，然后作为专家的初始化？）需要确保专家模型上的负载均衡

其实就是把attention的ffn换成多个网络（可以是ffn，也可以是别的所有）  
训练和推理的时候在前面加一个gate（router，也是ffn），同时设定超参数topk决定有几个专家加入推理。  
LLM用“MoE”这个词很误导人，和之前推荐系统中的MoE根本不是一回事。LLM用的这个技术准确点应该叫做SMoE。

.推荐系统中的专家是真的专家，每个专家都有特定的功能（一般是每个专家负责输入一个子业务，或者预测一个业务目标）。而LLM各“专家”都是自动学出来的，并没有什么实质的区别。实验发现，从模型中删掉一两个“专家”，也不会使模型发生质的变化，只是效果稍差一点；
推荐系统中的MoE，一般是各个专家都会计算，最后加权求和，并不能节省计算量。而LLM中一般只激活部分“专家”。  

感觉计算中的中间向量结果其实都可以理解成概率，只是需要softmax去归一化罢了（例如上面所说的router的输出就是token数、选专家的概率）  


## 基于梯度下降法的模型训练的源头
深度学习框架帮我们解决了哪些问题？其实用numpy也能实现前向反向传播，但是速度和效率都不行，且不能使用gpu加速。
```python
np.random.seed(0)

N, D = 3, 4

x = np.random.randn(N, D) 
```  
深度学习框架，帮助我们解决的核心问题就是反向传播时的梯度计算和更新，当然，它们的功能远不止这些，像各种方便的loss函数:交叉熵，MSE均方损失...；各种优化器：sgd,adam...;GPU并行计算加速等；模型的保存恢复可视化等等。(没说前向传播的原因是前向传播逻辑实际上写在了网络类里面了)  



## CoT_chain_of_thought

我先放个在这里，预训练可能会用到。
* 人工Few-shot CoT 人工直接给样例回答
* auto zero-shot CoT 只说step by step
  * auto auto-CoT 把上面的结果当做样例





## 面试题
用语言介绍一下Transformer的整体流程 
深度学习的三种并行方式：数据并行，模型并行，流水线并行 
Deepspeed分布式训练的了解，zero 0-3的了解。 
对于CLIP的了解 
说几种对比学习的损失函数，以及它们的特点和优缺点 
说说大模型生成采样的几种方式，它们的特点和优缺点比较 
损失函数中温度的作用 
BLIP的细节。（面试中提的问题是BLIP为什么将训练分成两个阶段） 
Visual Encoder有哪些常见的类型？ 
深度学习中常用的优化器有哪些？ 
SimCSE的了解 
prenorm和postnorm 
LLaMA 2的创新/ChatGLM的创新点/Qwen的创新点/Baichuan的创新点 
LLM的评估方式有哪些？特点是什么？（中文的呢？） 
文本生成模型中生成参数的作用（temperature，top p, top k，num beams） 
LoRA的作用和原理 
CoT的作用 
神经网络经典的激活函数以及它们的优缺点 
softmax函数求导的推导 
BERT的参数量如何计算？ 
AUC和ROC 
batch norm和layer norm 
大模型训练的超参数设置 
经典的词向量模型有哪些？ 
InstructGPT三个阶段的训练过程，用语言描述出来（过程，损失函数） 
大模型推理加速的方法 
Transformer中注意力的作用是什么 
RNN、CNN和Transformer的比较（复杂度，特点，适用范围etc） 
AC自动机 
产生梯度消失问题的原因有哪些？ 
大模型的幻觉问题 
大模型训练数据处理 
RLHF的计算细节 
构建CoT样本的时候，怎么保证覆盖不同的场景？ 
召回的三个指标：Recall、NDCG、RMSE 
RoPE和ALiBi 
交叉熵、NCE和InfoNCE的区别和联系 
贝叶斯学派和概率学派的区别 
一个文件的大小超过了主存容量，如何对这个文件进行排序？应该使用什么算法？ 
Python中的线程、进程和协程 
python中的生成器和迭代器
1.简历中论文课题
2.adam优化器原理
3.LLama框架和transformer框架的主要区别，以及其中的归一化操作是什么，怎样实现的
4.lora的实现原理，其他微调方法.大语言模型量化、蒸馏、剪枝操作如何实现
5.旋转位置编码的原理
6.手撕多头注意力机制
7.反问

为什么位置编码里就要drop out、dropout 的神经元是哪些
layer_norm起到一个什么效果？为什么FFN里也有dropout但是没有实际使用
内积其实是一个矩形的面积，只是几何意义表征了投影
cross的地方选词维度问题
transformer应该是多个句子一个batch（多个token向量），一个句子一个矩阵输入
文中qk的规定实际上是q中向量对于k中向量的注意
mask是在qk相乘完毕之后做的，因此mask矩阵维度和qk相乘的结果一致
代码实现有点问题？通常，d_k 和 d_v 会设置为 d_model // n_heads。  相当于写死
layernorm就是把向量的均值为0方差为1，其实就变成了模为1的向量
在使用预训练的词嵌入（如 Word2Vec 或 GloVe）时，embedding 层的权重也可以被设置为不可训练，或仅在特定情况下更新，具体取决于你的任务需求。
source embedding和target embedding还不太一样 因为词表不一样
全文两处用到注意力，一处是self attention，另一处是co attention，前者不必说，后者的k和v都是encoder的输出，所以k和v的形状（token数而非dk_dv）总是相同的

上来先自我介绍，博主简单介绍了background，然后说了下实习原因，就开始提问了。讲一下teaching forcing的原理；如果生成时全部使用真实token作为输入会有什么后果（项目相关）讲一下beam search算法原理（项目相关）训练模型时有没有遇到过loss为nan的现象，怎么解决的训练时loss一直降不下来可能有哪些原因有没有了解过并行训练，说一下原理dropout的作用训练模型时遇到out of memory怎么解决，没有额外显卡的情况下self attention的计算过程transformer中计算复杂度最高的是哪个模块，时间复杂度与什么有关，复杂度是多少打开IDE现场写算法题（leetcode easy难度）IDE现场手写self attention代码有没有调研过已有的大模型一周能实习几天

解释PPL指标这段实习持续多久，为什么不继续实习除了llama以外，还有在其他中文大模型如chatglm上做推理吗介绍一下对比了哪些baseline方法，简单介绍baseline方法你们使用的方法在不考虑推理长度超过KV Cache时，推理结果的准确性相比正常推理有变化吗有没有做过大模型微调相关工作，不考虑BERT这类预训练模型

问实习经历，介绍大模型长文本推理，主要做长文本哪方面的优化介绍下什么是长度外推你们一共测评了大模型推理过程中哪些指标你们用了什么样的测评数据集，用户输入是怎么样的你提到的baseline方法有自己的测试数据集吗你对long context这方面还有其他了解的吗场景问题：现在有个大模型正常只能处理8k的上下文，现在我们有个200w长度的prompt输入要处理，你觉得有哪些点是可以考虑优化的；我：可以用RAG解决这个问题。面试官：RAG主要处理问答，那要是做文本摘要呢；我：尝试使用prompt压缩

说一下prompt tuning和ICL（In-Context Learning）的区别、共性和联系

BN和LN的区别，NLP领域主要用哪个，为什么不用BN
Llama模型结构相对于传统的transformer架构有什么改进，这个师姐在论文讨论班还专门讲了，我又忘了，只记得RoPE，和MQA、GQA，忘了RMSNorm归一化以及SwiGLU激活函数

llama2中使用的注意力机制是什么?手写实现下分组注意力。了解langchain吗?讲讲其结构。对位置编码熟悉吗?讲讲几种位置编码的异同RLHF的具体工程是什么?包含了哪几个模型?分别讲讲 encoder-only、decoder-only、encoder-decoder 几种大模型的代表作。具体讲讲 p-tuning、lora 等微调方法，并指出它们与传统fine-tuning微调有何不同。显存不够一般怎么解决的?几种主流大模型的 loss 了解过吗? 有哪些异同?了解半精度训练吗?展开讲讲。deepspeed 用过吗? 展开讲讲。

