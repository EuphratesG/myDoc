# 深度学习相关的延伸


## 概率和频率学派和贝叶斯学派、损失函数的由来

**概率是样本空间的公理，频率派和贝叶斯派的区别只是对概率的理解和对模型参数的理解。**  
频率派认为概率就是频率的极限，贝叶斯派认为概率是信心的度量  
先验分布变了贝叶斯公式的分母也会变，但是最大后验的话对每个类别分母都一样，所以还是只需要考虑分子  
实验数量不是无限的情况下频率派偶发事件影响严重，贝叶斯派可以引入先验   
### 例如日麻
日麻主要在做两件事：
- 判断牌山和对手手牌的分布
- 根据分布做出局部最优解
- 我们不考虑奖惩策略建模的问题，按直觉判断
  
样本空间中的样本点（实验）会有若干个选择，然后会有结果。单次实验的所有结果构成一个样本空间。

- 打一张铳不铳是做对环境无改变的实验  
- 摸一张或者别人打一张糊不糊是对环境有改变的实验（摸小球）  
- 而打不打某一张牌则是策略强相关的

### 那么损失函数是怎么来的呢？
机器学习领域，注意是机器学习领域，我们要找到输入X和输出Y（标签Y）的关系。而大前提就是XY满足一个联合分布（代表X和Y存在某种机器学习要拟合的目标关系，所以最终结果的XY关系当然不会相互独立）   
频率学派根据最大似然估计，做多次实验，把在该参数下每次实验结果的概率乘起来，因为这是确定的已知结果所以让该概率值最大的参数值就是模型的参数值。  
贝叶斯学派则根据最大后验估计引入先验。随着样本量的增加，后验分布的先验信息影响会减弱，最终后验分布会接近似然函数，即MLE的结果。  

**那么损失函数和这些有什么关系**？**损失函数是标签Y和预测值FX的函数**。损失函数可以是根据朴素逻辑得到的（例如感知机模型，针对误分类点到超平面的距离总和进行建模，该值最小自然模型拟合完毕），也可以是根据最大似然估计的式子截取的和模型参数有关的部分（例如平方误差损失就是回归问题，噪声满足高斯分布的前提下最大似然估计的式子里取和模型参数有关的部分得到的）总而言之损失函数都必须是模型参数的函数，同时优化它可以达到拟合理想模型参数的目的。  


## 统计学习方法（机器学习）三要素

模型（模型假设空间）、策略（经验风险最小化、结构风险最小化，如何在假设空间中选取最优模型）、算法（实际选取模型，如梯度下降）

损失函数的期望叫做期望损失（expected loss），简单的理解就是对每个样本的损失进行平均（损失*输入样本分布的概率），学习的目标就是优化这个值。但是因为我们不知道联合概率分布的参数所以不能直接计算。所以引入经验损失（empirical loss，所有样本带入损失函数然后按样本数平均），最大似然估计就是经验损失最小化的典型例子。但是经验损失最小化有个问题训练集数据少的时候会**过拟合**。

过拟合（Overfitting）指的是模型在训练数据上表现得非常好，但在未见过的测试数据或新数据上表现较差的现象。

### 如何避免过拟合？主要是前两个原因

**1.** **增加训练数据量**：更多的数据可以帮助模型捕捉到数据的普遍规律，从而提高其泛化能力。

**2.** **简化模型**：使用更简单的模型或减少模型参数的数量，避免模型过度复杂化。比如，减少神经网络的层数、节点数，或使用更简单的回归模型。

3. **使用正则化**：引入正则化方法（如L1/L2正则化、Dropout）来防止模型过度拟合。

4. **交叉验证**：使用交叉验证方法来检查模型在不同数据集上的表现，确保它具有良好的泛化能力。

5. **数据预处理**：去除噪声和冗余特征，确保训练数据尽可能清晰和可靠。

6. **提前停止（Early Stopping）**：在训练过程中，监控验证集的性能，当验证集的性能不再提升时，提前停止训练，避免过拟合。

### 因此我们引入结构风险和结构风险最小化
在经验风险后面加入λJ（f），非负数λ是权衡经验风险和模型复杂度的系数，相当于在模型复杂度角度切入来改善过拟合问题（损失函数要多优化一些值）。贝叶斯估计的最大后验估计就是结构经验最小化的例子，模型复杂度由模型参数的先验分布表示。  


## 生成式模型和判别式模型
直接看b站情书的朴素贝叶斯就是生成模型的典型。训练过程就是在数据集中统计计算概率。  
生成式模型到底是怎么生成的找到病因了，输出不是单个概率值，而是一个概率分布！！！通过这个概率分布去做采样。开始时生成第一个单词，输出第一个单词的概率分布（如 "The": 0.5, "A": 0.3, "It": 0.2）。采样第一个单词：从分布中采样得到 "The"。而判别式模型只能输出一个概率值





## 感知机、逻辑回归、softmax
最初的感知机模型其实就是线性函数外面套一个sign（值>0+1，<0-1）  
频率学派的观点的话那感知机的损失函数直接根据预测错的点到超平面的距离也没啥不行，因为最优w是固定的，因此和极大似然估计的区别只是找到w方法的区别，而01损失函数（对了+1错了+0）不好求导数







概率模型和非概率模型可以相互转化其实 只需要最大似然估计和函数归一化就行




分类问题（包括推广的标注问题）概率模型和fx模型都行 回归问题只能fx模型 这里也能看出概率模型其实就是用来最后分类的  


## attention和MLP有何不同
所以，注意力机制在尽可能减少算力压力的情况下，达到近似三阶张量全连接层参数量的效果，并且序列内部互相关联的机制使得它更能把握序列的特征。与二阶张量（矩阵）形式的 MLP 是不同的。
qkt和w相比肯定sai
## 样本比例问题
一看就没经受过毒打在样本比例不均衡的场景下例如正负样本1:9你不用正负样本1:1来训模型马上就收敛到全预测为负样本这样loss很低，准确率90%然后你就会发现训练出来的模型就是个粑粑然后你想救 发现救不回来 因为负样本的梯度强度是正样本的九倍模型不可避免地被负样本拐跑了你说那就给正样本加九倍的权重相当于一个正样本复制了九次那不还是正负样本1:1么而且我的实践经验是9倍正样本权重带9个负样本来训练的效果还不如正负样本1:1那不最后成了脱裤子放屁，多此一举？炼丹对前辈还是要有敬畏之心否则回旋镖打到的是你自己这个问题，原本只是回答入门初学者的疑问，但是已经被讨论到了，它本不该有深度。要把样本比例，在各种场景，各种任务下，都讨论清楚，那够写好几篇万字长文了。

我们处理不平衡数据的出发点是，训练集的数据相对于真实数据是有偏的，所以我们试图用一些技巧尽量去学到一个偏差尽量小的模型。但是也也有很多情况，训练集和真实样本空间中是一致的，尽管正负样本的比例并不是1:1，但是他仍然与真实样本空间相符合，这时候似乎就没有使用这些技巧的必要性了


## 什么是vllm？
vLLM 是一种旨在优化大语言模型（LLM）推理性能的高效推理引擎，专注于解决传统 LLM 推理过程中内存管理低效的问题，从而实现更快的推理速度和更低的硬件资源消耗。
PagedAttention 是 vLLM 中的核心技术，专门设计用于优化大语言模型（LLM）推理时的内存管理。它通过引入“分页”的动态内存管理机制，有效解决了传统方法中 Key-Value (KV) 缓存分配的低效问题。

注意gpt单decoder的结构是把cross-attention的模块去掉了的，只有masked-attention和FFN。为什么要kv cache？训练的时候没有，因为训练的时候基于teacher-forcing，每次都是给所有的上下文。推理的时候自回归迭代，才需要kvcache。注意大模型推理分两个阶段，第一个是解析prompt并保存kv cache和产生第一个token，第二个阶段每个循环只会输入一个新生成的token提供q，而把新token的kv计算出来之后再和kv cache并在一起做attention。因此为什么没有q cache就知道了，因为没必要啊只有当前一个q。可以缓存Q，但没用。

因为当前token的Q要跟之前所有token的K和V算注意力，所以缓存之前的K和V是有用的，避免重复计算K和V。
Q的使命就是参与注意力值的计算，而之前所有token的注意力值已经算过了，无需再算，所以就不用缓存之前的Q了。  

PagedAttention 是一种高效管理 Key-Value Cache 的机制，它将缓存分块存储在内存和磁盘之间，适配长序列生成场景。以下通过一个具体实例，说明 PagedAttention 如何在推理中高效利用缓存生成 Token。

---

### **场景描述**
#### **任务：生成故事文本**  
Prompt:  
`"Once upon a time in a faraway land, a young prince named Arin"`

目标：基于这个 Prompt，生成后续 Token。假设序列长度非常长，超出 GPU 内存的直接缓存能力。

#### **挑战**  
- 当前模型序列长度 \(L = 50,000\)（每个 Token 的 Key 和 Value 是 \(d=16,384\) 维）。  
- 如果全缓存，KV Cache 大小为 \(L \times d \times 2 = 50,000 \times 16,384 \times 2 \approx 1.6\ \text{GB}\)。
- 单 GPU 显存不够，PagedAttention 引入分块管理缓存。


### **PagedAttention 的优势**
1. **动态内存管理**：通过分页机制，将长序列的 Key 和 Value 缓存分为活跃页（存储于 GPU）和非活跃页（存储于 CPU 或磁盘），显著降低显存需求。
2. **上下文扩展能力强**：即使序列长度达到百万级，也可以通过分块管理确保模型可以高效访问全部上下文。
3. **按需调度**：仅在需要时加载非活跃页，减少不必要的数据传输和计算。

PagedAttention 的这种优化对于大模型长序列推理尤为重要，既保证了内存效率，又提供了生成时的上下文完整性。



## gbdt是什么
GBDT，全称为Gradient Boosting Decision Tree，即梯度提升决策树，是一种集成学习算法。  

**Ensemble Learning（集成学习）** 是一种将多个学习模型结合起来，以提高整体预测性能的机器学习方法。它的基本思想是通过组合多个弱学习器（通常是简单模型）来产生一个强学习器，从而克服单一模型的缺点。集成学习通常能够显著提高模型的准确性和鲁棒性。

### 集成学习与其他机器学习算法的关系

1. **集成学习与基本学习器**：
   - 集成学习依赖于多个基本学习器（或弱学习器）。这些基本学习器可以是任何机器学习算法，如决策树、支持向量机（SVM）、线性回归、k近邻（KNN）等。集成学习的核心思想是通过组合多个弱学习器的结果，形成一个强学习器。
   
   - 例如，**随机森林（Random Forest）** 是一种基于决策树的集成学习方法。它通过多棵决策树的组合来提高预测的准确性。每棵决策树本身可能是一个“弱学习器”，但是多个决策树结合后可以显著提高性能。

2. **集成学习与单一模型的区别**：
   - **单一模型**：通常是一个单独的机器学习模型（如一棵决策树或一个支持向量机）训练出来的模型，适用于某个特定问题。
   - **集成学习**：则是将多个模型（通常是同类或不同类的多个学习器）组合起来，以提高总体的泛化能力，降低模型的偏差（bias）和方差（variance）。
     - **偏差（Bias）**：指模型对数据的简化程度，过高的偏差通常意味着模型太简单，可能无法捕捉到数据的复杂性。
     - **方差（Variance）**：指模型对训练数据的敏感程度，过高的方差可能意味着模型过拟合，不能有效地推广到新的数据。

   - 例如，单一决策树模型可能过拟合（方差高），而通过集成多棵决策树（如在随机森林中）可以减少过拟合，并提高模型的稳定性和准确性。

3. **集成学习与分类/回归算法的关系**：
   集成学习可以应用于任何分类或回归算法中。例如：
   - **分类任务**：如果你使用的基本学习器是分类器（如决策树、逻辑回归、K近邻等），那么集成学习的最终目标就是提高分类准确率。
   - **回归任务**：同理，集成学习可以应用于回归问题，其中常见的集成学习方法包括集成回归树（如梯度提升回归树和随机森林回归）。

4. **集成学习与 Bagging、Boosting、Stacking 的关系**：
   集成学习方法有多种，其中最常见的包括：
   - **Bagging（Bootstrap Aggregating）**：
     - Bagging 的目标是通过减少模型的方差来提高性能。它通过训练多个独立的模型，并对这些模型的结果进行平均（回归）或投票（分类）。
     - **随机森林** 就是 Bagging 的一个典型例子，它通过多次随机采样数据和特征的子集来训练多棵决策树，然后将这些树的预测结果进行集成。
   
   - **Boosting**：
     - Boosting 的目标是通过减少模型的偏差来提高性能。它通过逐步训练一系列模型，每个模型都试图修正前一个模型的错误预测。
     - 常见的 Boosting 算法包括 **AdaBoost** 和 **Gradient Boosting**。这些算法会给训练数据中的错分样本加大权重，迫使后续的模型更好地处理这些难以预测的样本。
   
   - **Stacking**：
     - Stacking（堆叠）是一种更加灵活的集成方法，它通过将不同类型的模型组合在一起，训练一个新的模型来整合这些模型的预测。堆叠通常会使用一个“元学习器”来根据基本学习器的输出进行最终预测。
     - 比如，可以用逻辑回归、决策树和SVM作为基本学习器，然后用一个线性回归模型作为元学习器来综合这些模型的预测结果。

5. **集成学习与深度学习的关系**：
   - 集成学习方法可以与深度学习相结合。例如，通过集成多个深度神经网络（DNN）的结果，可能会比单一的神经网络获得更好的结果。这类集成方法在某些应用中（如图像识别、NLP任务）可能具有较大的优势。
   - 例如，在 **Kaggle** 等数据科学竞赛中，往往通过集成不同类型的深度学习模型来提高准确率。


### Gradient Boosting Decision Tree，梯度提升决策树  
属于提升算法（Boosting）的一种。它通过逐步构建多个决策树，并结合每个树的预测结果来提高模型的准确性。GBDT的核心思想是通过优化目标函数（通常是损失函数）来减少误差，并在每一步迭代中使用梯度下降来修正前一轮预测的残差。

## moe
研究低算力slm 产生的灵感，我或许不需要那么多领域的覆盖，所以研究混合专家，做有限领域垂直应用。就好比人类社会，各有分工，简洁明了。  
同时直观想法横向拓展参数量，理论上就应该有提升。  
如何提升MOE并行的效率，专家之间的网络通信会成为计算的瓶颈。而且GPU擅长做矩阵运算，不擅长做分支；每个专家小模型分配的样本数较少，无法得到充分的训练；（是否可以先训练出若干个完整的大模型，例如数学大模型、代码大模型、问答大模型等，然后作为专家的初始化？）需要确保专家模型上的负载均衡

其实就是把attention的ffn换成多个网络（可以是ffn，也可以是别的所有）  
训练和推理的时候在前面加一个gate（router，也是ffn），同时设定超参数topk决定有几个专家加入推理。  
LLM用“MoE”这个词很误导人，和之前推荐系统中的MoE根本不是一回事。LLM用的这个技术准确点应该叫做SMoE。

.推荐系统中的专家是真的专家，每个专家都有特定的功能（一般是每个专家负责输入一个子业务，或者预测一个业务目标）。而LLM各“专家”都是自动学出来的，并没有什么实质的区别。实验发现，从模型中删掉一两个“专家”，也不会使模型发生质的变化，只是效果稍差一点；
推荐系统中的MoE，一般是各个专家都会计算，最后加权求和，并不能节省计算量。而LLM中一般只激活部分“专家”。  

感觉计算中的中间向量结果其实都可以理解成概率，只是需要softmax去归一化罢了（例如上面所说的router的输出就是token数、选专家的概率）  


## 基于梯度下降法的模型训练的源头
深度学习框架帮我们解决了哪些问题？其实用numpy也能实现前向反向传播，但是速度和效率都不行，且不能使用gpu加速。
```python
np.random.seed(0)

N, D = 3, 4

x = np.random.randn(N, D) 
```  
深度学习框架，帮助我们解决的核心问题就是反向传播时的梯度计算和更新，当然，它们的功能远不止这些，像各种方便的loss函数:交叉熵，MSE均方损失...；各种优化器：sgd,adam...;GPU并行计算加速等；模型的保存恢复可视化等等。(没说前向传播的原因是前向传播逻辑实际上写在了网络类里面了)  



## CoT_chain_of_thought

我先放个在这里，预训练可能会用到。
* 人工Few-shot CoT 人工直接给样例回答
* auto zero-shot CoT 只说step by step
  * auto auto-CoT 把上面的结果当做样例





## 面试题
用语言介绍一下Transformer的整体流程 
深度学习的三种并行方式：数据并行，模型并行，流水线并行 
Deepspeed分布式训练的了解，zero 0-3的了解。 
对于CLIP的了解 
说几种对比学习的损失函数，以及它们的特点和优缺点 
说说大模型生成采样的几种方式，它们的特点和优缺点比较 
损失函数中温度的作用 
BLIP的细节。（面试中提的问题是BLIP为什么将训练分成两个阶段） 
Visual Encoder有哪些常见的类型？ 
深度学习中常用的优化器有哪些？ 
SimCSE的了解 
prenorm和postnorm 
LLaMA 2的创新/ChatGLM的创新点/Qwen的创新点/Baichuan的创新点 
LLM的评估方式有哪些？特点是什么？（中文的呢？） 
文本生成模型中生成参数的作用（temperature，top p, top k，num beams） 
LoRA的作用和原理 
CoT的作用 
神经网络经典的激活函数以及它们的优缺点 
softmax函数求导的推导 
BERT的参数量如何计算？ 
AUC和ROC 
batch norm和layer norm 
大模型训练的超参数设置 
经典的词向量模型有哪些？ 
InstructGPT三个阶段的训练过程，用语言描述出来（过程，损失函数） 
大模型推理加速的方法 
Transformer中注意力的作用是什么 
RNN、CNN和Transformer的比较（复杂度，特点，适用范围etc） 
AC自动机 
产生梯度消失问题的原因有哪些？ 
大模型的幻觉问题 
大模型训练数据处理 
RLHF的计算细节 
构建CoT样本的时候，怎么保证覆盖不同的场景？ 
召回的三个指标：Recall、NDCG、RMSE 
RoPE和ALiBi 
交叉熵、NCE和InfoNCE的区别和联系 
贝叶斯学派和概率学派的区别 
一个文件的大小超过了主存容量，如何对这个文件进行排序？应该使用什么算法？ 
Python中的线程、进程和协程 
python中的生成器和迭代器
1.简历中论文课题
2.adam优化器原理
3.LLama框架和transformer框架的主要区别，以及其中的归一化操作是什么，怎样实现的
4.lora的实现原理，其他微调方法.大语言模型量化、蒸馏、剪枝操作如何实现
5.旋转位置编码的原理
6.手撕多头注意力机制
7.反问

为什么位置编码里就要drop out、dropout 的神经元是哪些
layer_norm起到一个什么效果？为什么FFN里也有dropout但是没有实际使用
内积其实是一个矩形的面积，只是几何意义表征了投影
cross的地方选词维度问题
transformer应该是多个句子一个batch（多个token向量），一个句子一个矩阵输入
文中qk的规定实际上是q中向量对于k中向量的注意
mask是在qk相乘完毕之后做的，因此mask矩阵维度和qk相乘的结果一致
代码实现有点问题？通常，d_k 和 d_v 会设置为 d_model // n_heads。  相当于写死
layernorm就是把向量的均值为0方差为1，其实就变成了模为1的向量
在使用预训练的词嵌入（如 Word2Vec 或 GloVe）时，embedding 层的权重也可以被设置为不可训练，或仅在特定情况下更新，具体取决于你的任务需求。
source embedding和target embedding还不太一样 因为词表不一样
全文两处用到注意力，一处是self attention，另一处是co attention，前者不必说，后者的k和v都是encoder的输出，所以k和v的形状（token数而非dk_dv）总是相同的

上来先自我介绍，博主简单介绍了background，然后说了下实习原因，就开始提问了。讲一下teaching forcing的原理；如果生成时全部使用真实token作为输入会有什么后果（项目相关）讲一下beam search算法原理（项目相关）训练模型时有没有遇到过loss为nan的现象，怎么解决的训练时loss一直降不下来可能有哪些原因有没有了解过并行训练，说一下原理dropout的作用训练模型时遇到out of memory怎么解决，没有额外显卡的情况下self attention的计算过程transformer中计算复杂度最高的是哪个模块，时间复杂度与什么有关，复杂度是多少打开IDE现场写算法题（leetcode easy难度）IDE现场手写self attention代码有没有调研过已有的大模型一周能实习几天

解释PPL指标这段实习持续多久，为什么不继续实习除了llama以外，还有在其他中文大模型如chatglm上做推理吗介绍一下对比了哪些baseline方法，简单介绍baseline方法你们使用的方法在不考虑推理长度超过KV Cache时，推理结果的准确性相比正常推理有变化吗有没有做过大模型微调相关工作，不考虑BERT这类预训练模型

问实习经历，介绍大模型长文本推理，主要做长文本哪方面的优化介绍下什么是长度外推你们一共测评了大模型推理过程中哪些指标你们用了什么样的测评数据集，用户输入是怎么样的你提到的baseline方法有自己的测试数据集吗你对long context这方面还有其他了解的吗场景问题：现在有个大模型正常只能处理8k的上下文，现在我们有个200w长度的prompt输入要处理，你觉得有哪些点是可以考虑优化的；我：可以用RAG解决这个问题。面试官：RAG主要处理问答，那要是做文本摘要呢；我：尝试使用prompt压缩

说一下prompt tuning和ICL（In-Context Learning）的区别、共性和联系

BN和LN的区别，NLP领域主要用哪个，为什么不用BN
Llama模型结构相对于传统的transformer架构有什么改进，这个师姐在论文讨论班还专门讲了，我又忘了，只记得RoPE，和MQA、GQA，忘了RMSNorm归一化以及SwiGLU激活函数

llama2中使用的注意力机制是什么?手写实现下分组注意力。了解langchain吗?讲讲其结构。对位置编码熟悉吗?讲讲几种位置编码的异同RLHF的具体工程是什么?包含了哪几个模型?分别讲讲 encoder-only、decoder-only、encoder-decoder 几种大模型的代表作。具体讲讲 p-tuning、lora 等微调方法，并指出它们与传统fine-tuning微调有何不同。显存不够一般怎么解决的?几种主流大模型的 loss 了解过吗? 有哪些异同?了解半精度训练吗?展开讲讲。deepspeed 用过吗? 展开讲讲。


