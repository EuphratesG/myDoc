### 常见机器学习任务分类
* 有监督学习
  * 回归
  * 分类
    * 分类和回归的区别：输出目标、损失函数（有激活函数且多个vs无激活函数且单个）
  * 多标签，例如多类别目标检测
  * 搜索，例如结合机器学习的pagerank
  * 推荐系统
  * 序列分析
* 无监督学习 聚类、PCA、因果、GAN
* 强化学习

### 计算和pytorch语法相关
矩阵求导 其实结果都是A，只是因为列vec/列vec的布局和列vec/行vec不一样
深度学习中常用的其实是标量函数对向量自变量求导（因为损失函数肯定or一般是个标量），就算y不是标量一般最后也会sum成标量
0轴求和保留一行，1轴求和保留一列   
所谓的似然就是给定X后f的值（因为这时候描述的是θ的可能性，让其最大的θ就是最可能出现的参数值）当前样本来自什么参数的分布的可能性最大。  
本质就是function中的每个f分别对变元中的每个元素逐个求偏导，求导的布局是另外考虑的事情。**先算出结果再去考虑布局。** 分子布局就是分子是没转置的列向量，分母是转置之后的行向量或者矩阵。矩阵变元和矩阵函数都用vec拉成向量形式去解决。
摆放：列优先去求导，行优先去放在结果矩阵里面。即所谓先计算再摆放。       
![](https://cdn.jsdelivr.net/gh/EuphratesG/myPic@main/202311101033658.png)  
这里如果按先计算的原则的话xtA和Atx是等价的。
![](https://cdn.jsdelivr.net/gh/EuphratesG/myPic@main/202311101050211.png)
如果这个了然了，矩阵求导也就了然。
**深度学习中我们很少用向量函数求导，绝大多数对标量求导**。即便y有些时候是向量通常也会把它转化成标量（例如sum()一下,是向量每个分量的函数，但是求偏导全没了）再去对向量求导。  





平方损失函数就是对theta的极大似然估计
平方损失函数是定义出来的，所谓假设误差服从正态分布仅仅说明了为什么可以这样定义。  

为什么计算二阶导数比一阶导数的开销要更大？

在运行反向传播函数之后，立即再次运行它，看看会发生什么。：计算图不会保存，可以通过参数设置

在控制流的例子中，我们计算d关于a的导数，如果将变量a更改为随机向量或矩阵，会发生什么？  
backwwards只能损失函数是标量，因此就算是向量y也要sum  

寻找最好的模型参数（包括偏置）还需要一种模型质量的度量方式（loss）和可以提高模型预测质量的方法（梯度下降法）。四要素：训练数据、loss、优化算法、模型结构

验证集的运行
验证集用于评估模型的性能和调整超参数，但它不会影响模型的权重更新。

运行阶段：通常在每个训练周期（epoch）或若干个周期之后，模型在验证集上运行一次。
作用：
检测模型的泛化能力，观察模型是否过拟合。
根据验证集的指标调整模型的超参数（如学习率、正则化强度）。
选出在验证集上表现最好的模型作为最终模型。
可见性：验证集在训练阶段仅用于评估，不参与模型的参数更新。


记录性能：

如果验证集上的指标开始恶化（如损失上升、准确率下降），通常说明模型可能开始过拟合训练数据。
早停（early stopping）等机制可能会终止训练，以保存验证集表现最佳的模型。

## 为什么要用向量表示各种计算？
因为python的for非常慢，而torch的向量会用cpu（simd，多线程）或者gpu加速（warp、共享内存）  


## 损失函数从何而来
回归问题：和label误差最小，直接用均方误差

分类问题：和label分布拟合程度最高，且求导容易，交叉熵误差
具体参考知乎收藏损失函数部分，熵、KL散度、交叉熵  

## 过拟合  
欠拟合单纯就是训练集和验证集损失都高，因此我们主要研究过拟合  
我们总是可以通过去收集更多的训练数据来缓解过拟合。 但这可能成本很高，耗时颇多，或者完全超出我们的控制，因而在短期内不可能做到。 假设我们已经拥有尽可能多的高质量数据，我们便可以将重点放在正则化技术上。
![alt text](image.png)



梯度叠加原来是这个意思，就是因为共享参数把反向传播过程中的同名层的梯度累计一下，更新的时候一起更新还维持一样的权重  


## 序列模型
在时间t观察到xt，得到t个随机变量，px = p(x1..xt)  
机器学习中所有建模基本都是对Px建模，但之前的各个x是独立的，序列模型是非独立的  
### 什么叫自回归？
输入数据和我要预测的数据是同类数据的回归问题（图片和标签不是同类数据）p(xt|x1,..xt-1)p(xt|f(x1,...xt-1))对之前t-1个数据建模训练个模型  
* 方案A 马尔科夫假设 当前数据之和之前τ个数据相关  
* 方案B 隐变量模型 就是RNN的结构，本质上是建两个模型ht(xt,ht-1)，fxt+1(xt,ht) 而且一般构建数据时也会用马尔科夫假设，相当于两个方案一起用 

总而言之把一个时序序列通过马尔科夫假设转化成一个可以建模的回归问题（数据是怎么构造的是关键）  
### 语言模型  
属于序列模型，只不过xt是token(embedding)。说白了也是个词表大小的多分类问题。衡量语言模型的好坏也是使用平均交叉熵，NLP常用perplexity困惑度，exp(平均交叉熵)    
应用包括：预训练模型、生成式模型、判断多个序列哪个出现可能大  
![alt text](image-1.png)
