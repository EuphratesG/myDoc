# 深度学习相关的延伸


## 频率学派和贝叶斯学派、概率模型和非概率模型、生成式模型和判别式模型
机器学习分为监督学习（包括类似语言模型的半监督模型）、无监督模型、强化学习。**我们主要考虑监督学习。**  

---
### 贝叶斯学派和频率学派（MLE、MAP、贝叶斯估计）
**概率是样本空间的公理，频率派和贝叶斯派的区别只是对概率的理解和对模型参数的理解。**   频率派认为概率就是频率的极限，贝叶斯派认为概率是信心的度量。实验数量不是无限的情况下频率派偶发事件影响严重，贝叶斯派可以引入先验。     
**两大学派的求解问题思路主要为：训练（MLE、MAP、贝叶斯估计求解模型参数）、预测（判别或者生成）**。

期望损失和经验风险的区别：一个是损失函数的数学期望一个是损失函数的样本平均数




正则就是规则的意思，是我们人类主观加到数据上的额外的规则。频率派主张不要添加这个额外的规则，这时，机器学习的大部分内容就退化到求参数的极大似然值，有时候甚至都不用求，比如投硬币7头3尾，那么头的概率就是0.7。本身这也没什么大问题，只需要你的数据客观的反印现实就行了，或者用最简单的语言来说就是这组数据要充分多。但实际运用时，没人能保证数据是否充分，比如投了2次都是正面，那能说明正面是100%吗？这就引出了频率派的老冤家——贝叶斯派，他们认为数据是不完美的，因此我们首先需要一个基本的假设，不仅需要假设这些参数的初始值，还要假设这些参数符合某种概率分布。这个基本假设来自于我们的经验，取名先验概率。在先验概率的基础上，用收集到的数据对先验概率进行修正（修正的方式是贝叶斯定理），得到后验概率，这个后验概率才是贝叶斯派认为正确的结果。这里额外增加的基本假设，学名就叫正则。（BTW，频率派和贝叶斯派都是认可贝叶斯定理的）通常，我们只增加一些最常见的假设，比如投硬币时先认为是五五开的，搭设模型时认为越简单越好的（也就是参数越少越好），因此我们给模型增加了这些约束。在机器学习中，这些常规的约束现在已经简化到了我们所说的正则化项的参数，你只需要调节参数即可，不再需要重头开始推导先验、后验概率。我们添加的假设一定有对有错，在我们添加的假设正确时，就会出现模型更加准确，也就是测试集上效果提升，如果假设错误，那么可能会没有影响，也可能出现副作用。
### 监督学习中的生成式模型和判别式模型
直接看b站情书的朴素贝叶斯就是生成模型的典型。朴素贝叶斯训练过程就是在数据集中统计计算概率，朴素在于各个特征互相独立。对p（xy）建模（因为都求出来了）然后自然地由贝叶斯公式得到要求的条件概率。最后取分类类别的时候单纯的概率高的是。   
![](https://cdn.jsdelivr.net/gh/EuphratesG/myPic@main/202501260929882.png)  
**生成式模型到底是怎么生成的找到病因了**，输出不是单个概率值，而是一个概率分布！！！通过这个概率分布去做采样。开始时生成第一个单词，输出第一个单词的概率分布（如 "The": 0.5, "A": 0.3, "It": 0.2）。采样第一个单词：从分布中采样得到 "The"。而判别式模型只能输出一个概率值。 MLM也只是bert训练时做的一个带训练目标的句子分类而已



### **概念辨析：MLE、MAP、生成式模型、判别式模型**

这几个概念分属不同维度，但它们在实际模型训练和预测中相互关联。以下是分层次的解析：

---

### **1. 参数估计方法：MLE vs. MAP**
这两个概念是**参数估计方法**，用于从数据中学习模型参数。

| **方法**   | **核心思想**                                                                 | **公式**                                                                 | **特点**                                                                 |
|------------|-----------------------------------------------------------------------------|-------------------------------------------------------------------------|-------------------------------------------------------------------------|
| **MLE**    | 最大化**似然函数**，找到最可能生成数据的参数（仅依赖数据本身）。                          | \( \theta_{\text{MLE}} = \arg\max_{\theta} P(D \mid \theta) \)           | 完全依赖数据，可能过拟合（尤其是小数据场景）。                                     |
| **MAP**    | 在MLE基础上引入**先验分布**，找到后验概率最大的参数（结合数据与先验知识）。                   | \( \theta_{\text{MAP}} = \arg\max_{\theta} P(D \mid \theta)P(\theta) \) | 通过先验修正参数估计，更鲁棒（如拉普拉斯平滑）。                                    |

**关系与区别**：  
- MLE是MAP的特例（当先验分布是均匀分布时，MAP退化为MLE）。  
- **实际应用**：  
  - 朴素贝叶斯中的条件概率估计使用MAP（拉普拉斯平滑）。  
  - 逻辑回归的参数估计通常使用MLE（交叉熵损失），但加入L2正则化等价于MAP（高斯先验）。

---

### **2. 模型类型：生成式模型 vs. 判别式模型**
这两个概念是**模型的设计范式**，决定了模型如何建模输入 \( X \) 和输出 \( Y \) 的关系。

| **模型类型**     | **建模目标**                               | **核心能力**                     | **典型例子**                              |
|------------------|------------------------------------------|----------------------------------|------------------------------------------|
| **生成式模型**   | 联合分布 \( P(X, Y) \)                     | 生成新样本、分类、数据补全          | 朴素贝叶斯、隐马尔可夫模型（HMM）、GAN           |
| **判别式模型**   | 条件分布 \( P(Y \mid X) \)                 | 分类、回归、预测                   | 逻辑回归、支持向量机（SVM）、神经网络、线性回归    |

**关系与区别**：  
- **生成式模型**需要显式建模数据的生成过程（如 \( P(X \mid Y) \) 和 \( P(Y) \)），因此计算复杂度更高。  
- **判别式模型**直接关注输入到输出的映射，通常更高效且在小样本任务中表现更好。  

---

### **3. 参数估计方法与模型类型的交叉关系**
生成式模型和判别式模型在训练时，均可使用MLE或MAP进行参数估计。以下是典型组合：

#### **(1) 生成式模型 + MAP**  
- **例子**：朴素贝叶斯（带拉普拉斯平滑）。  
- **过程**：  
  1. 假设先验分布（如Dirichlet先验）。  
  2. 通过最大化后验概率 \( P(\theta \mid D) \) 估计参数（如条件概率 \( P(X_i \mid Y) \)）。  

#### **(2) 生成式模型 + MLE**  
- **例子**：高斯混合模型（GMM）。  
- **过程**：  
  直接最大化似然函数 \( P(D \mid \theta) \)，估计高斯分布的均值和方差。  

#### **(3) 判别式模型 + MLE**  
- **例子**：逻辑回归（无正则化）。  
- **过程**：  
  最大化条件似然 \( P(Y \mid X, \theta) \)，使用交叉熵损失函数。  

#### **(4) 判别式模型 + MAP**  
- **例子**：带L2正则化的逻辑回归。  
- **过程**：  
  最大化后验概率 \( P(\theta \mid D) \)，等价于在损失函数中加入L2正则项（假设参数服从高斯先验）。  

---

### **4. 总结：一张表理清所有关系**
| **维度**          | **生成式模型**                          | **判别式模型**                          |
|-------------------|----------------------------------------|----------------------------------------|
| **核心建模目标**   | \( P(X, Y) \)                          | \( P(Y \mid X) \)                      |
| **能否生成数据**   | 能（如生成图片、文本）                    | 不能                                   |
| **参数估计方法**   | MLE（如GMM）、MAP（如朴素贝叶斯）         | MLE（如逻辑回归）、MAP（如带L2正则化的SVM） |
| **典型任务**       | 生成、分类、聚类                        | 分类、回归、预测                        |

---

### **5. 常见问题解答**
#### **Q1：生成式模型一定比判别式模型强吗？**  
- **不一定**！生成式模型需要更强的假设（如朴素贝叶斯的特征独立性假设），如果假设不成立，性能可能较差。判别式模型通常在小样本或复杂任务中表现更好。

#### **Q2：MAP和贝叶斯估计有什么区别？**  
- **MAP**是点估计（取后验概率最大的参数值），**贝叶斯估计**是积分所有参数可能性（计算后验分布的期望）。贝叶斯估计更全面但计算复杂，MAP是实用折中。

#### **Q3：逻辑回归是生成式还是判别式模型？**  
- **判别式模型**，因为它直接建模 \( P(Y \mid X) \)，不涉及 \( P(X) \) 或 \( P(X \mid Y) \)。

---

### **最终结论**
- **MLE/MAP**是参数估计方法，**生成式/判别式**是模型设计范式，两者是正交的维度。  
- 实际中选择哪种组合，取决于任务需求（是否需要生成数据）、数据量（小数据适合MAP）和模型假设的合理性。





房价预测的线性回归模型属于**判别式模型**。以下是关键分析步骤：

1. **模型目标**：  
   线性回归旨在直接学习输入变量（如房屋面积、位置等）与输出变量（房价）之间的映射关系，即建模条件概率 \( P(Y|X) \)。它关注的是在给定特征 \( X \) 时如何预测目标值 \( Y \)，而非联合分布 \( P(X,Y) \)。

2. **生成式 vs 判别式的核心区别**：  
   - **生成式模型**（如朴素贝叶斯）学习联合分布 \( P(X,Y) \)，可生成新的数据样本（如模拟房屋特征和对应价格）。  
   - **判别式模型**（如逻辑回归、支持向量机）直接建模 \( P(Y|X) \) 或决策边界，仅关注预测输出，不涉及输入数据的生成。

3. **线性回归的数学本质**：  
   假设目标变量 \( Y \) 服从以 \( X \) 的线性组合为均值的高斯分布，通过最大似然估计参数。这种条件概率建模方式（\( Y \) 依赖于 \( X \)）明确属于判别式框架。

4. **误区澄清**：  
   即使模型能预测 \( Y \)，若未对 \( X \) 的分布进行建模（如生成新房屋特征），仍为判别式模型。生成式模型需同时描述 \( X \) 和 \( Y \) 的关系。

**结论**：线性回归通过直接建模条件概率 \( P(Y|X) \) 完成预测任务，属于典型的判别式模型。

### 概率模型和非概率模型
我们讨论和另两组概念的关系。所谓频率派和贝叶斯派讨论的都是求模型参数，固定一个模型，然后用这个模型做预测。如何求？**极大似然估计、最大后验估计、贝叶斯估计，这些都是基于概率似然的，所以一定是概率模型**。那么非概率模型呢？感知机，直接定义一个fx函数，然后通过  



### 那么损失函数是怎么来的呢？
机器学习领域，注意是机器学习领域，我们要找到输入X和输出Y（标签Y）的关系。而大前提就是XY满足一个联合分布（代表X和Y存在某种机器学习要拟合的目标关系，所以最终结果的XY关系当然不会相互独立）   
频率学派根据最大似然估计，做多次实验，把在该参数下每次实验结果的概率乘起来，因为这是确定的已知结果所以让该概率值最大的参数值就是模型的参数值。  
贝叶斯学派则根据最大后验估计引入先验。随着样本量的增加，后验分布的先验信息影响会减弱，最终后验分布会接近似然函数，即MLE的结果。  

**那么损失函数和这些有什么关系**？**损失函数是标签Y和预测值FX的函数**。损失函数可以是根据朴素逻辑得到的（例如感知机模型，针对误分类点到超平面的距离总和进行建模，该值最小自然模型拟合完毕），也可以是根据最大似然估计的式子截取的和模型参数有关的部分（例如平方误差损失就是回归问题，噪声满足高斯分布的前提下最大似然估计的式子里取和模型参数有关的部分得到的）总而言之损失函数都必须是模型参数的函数，同时优化它可以达到拟合理想模型参数的目的。  


## 统计学习方法（机器学习）三要素

模型（模型假设空间）、策略（经验风险最小化、结构风险最小化，如何在假设空间中选取最优模型）、算法（实际选取模型，如梯度下降）

损失函数的期望叫做期望损失（expected loss），简单的理解就是对每个样本的损失进行平均（损失*输入样本分布的概率），学习的目标就是优化这个值。但是因为我们不知道联合概率分布的参数所以不能直接计算。所以引入经验损失（empirical loss，所有样本带入损失函数然后按样本数平均），最大似然估计就是经验损失最小化的典型例子。但是经验损失最小化有个问题训练集数据少的时候会**过拟合**。

过拟合（Overfitting）指的是模型在训练数据上表现得非常好，但在未见过的测试数据或新数据上表现较差的现象。

### 如何避免过拟合？主要是前两个原因

**1.** **增加训练数据量**：更多的数据可以帮助模型捕捉到数据的普遍规律，从而提高其泛化能力。

**2.** **简化模型**：使用更简单的模型或减少模型参数的数量，避免模型过度复杂化。比如，减少神经网络的层数、节点数，或使用更简单的回归模型。

3. **使用正则化**：引入正则化方法（如L1/L2正则化、Dropout）来防止模型过度拟合。

4. **交叉验证**：使用交叉验证方法来检查模型在不同数据集上的表现，确保它具有良好的泛化能力。

5. **数据预处理**：去除噪声和冗余特征，确保训练数据尽可能清晰和可靠。

6. **提前停止（Early Stopping）**：在训练过程中，监控验证集的性能，当验证集的性能不再提升时，提前停止训练，避免过拟合。

### 因此我们引入结构风险和结构风险最小化
在经验风险后面加入λJ（f），非负数λ是权衡经验风险和模型复杂度的系数，相当于在模型复杂度角度切入来改善过拟合问题（损失函数要多优化一些值）。贝叶斯估计的最大后验估计就是结构经验最小化的例子，模型复杂度由模型参数的先验分布表示。  






## 感知机、逻辑回归、softmax
最初的感知机模型其实就是线性函数外面套一个sign（值>0+1，<0-1）  
频率学派的观点的话那感知机的损失函数直接根据预测错的点到超平面的距离也没啥不行，因为最优w是固定的，因此和极大似然估计的区别只是找到w方法的区别，而01损失函数（对了+1错了+0）不好求导数
判别式模型（如BERT、逻辑回归）主要用于判别输入数据的类别或概率，而不是直接生成新样本逐个生成。






概率模型和非概率模型可以相互转化其实 只需要最大似然估计和函数归一化就行




分类问题（包括推广的标注问题）概率模型和fx模型都行 回归问题只能fx模型 这里也能看出概率模型其实就是用来最后分类的  


## attention和MLP有何不同
所以，注意力机制在尽可能减少算力压力的情况下，达到近似三阶张量全连接层参数量的效果，并且序列内部互相关联的机制使得它更能把握序列的特征。与二阶张量（矩阵）形式的 MLP 是不同的。
qkt和w相比肯定sai
## 样本比例问题
一看就没经受过毒打在样本比例不均衡的场景下例如正负样本1:9你不用正负样本1:1来训模型马上就收敛到全预测为负样本这样loss很低，准确率90%然后你就会发现训练出来的模型就是个粑粑然后你想救 发现救不回来 因为负样本的梯度强度是正样本的九倍模型不可避免地被负样本拐跑了你说那就给正样本加九倍的权重相当于一个正样本复制了九次那不还是正负样本1:1么而且我的实践经验是9倍正样本权重带9个负样本来训练的效果还不如正负样本1:1那不最后成了脱裤子放屁，多此一举？炼丹对前辈还是要有敬畏之心否则回旋镖打到的是你自己这个问题，原本只是回答入门初学者的疑问，但是已经被讨论到了，它本不该有深度。要把样本比例，在各种场景，各种任务下，都讨论清楚，那够写好几篇万字长文了。

我们处理不平衡数据的出发点是，训练集的数据相对于真实数据是有偏的，所以我们试图用一些技巧尽量去学到一个偏差尽量小的模型。但是也也有很多情况，训练集和真实样本空间中是一致的，尽管正负样本的比例并不是1:1，但是他仍然与真实样本空间相符合，这时候似乎就没有使用这些技巧的必要性了


## 什么是vllm？
vLLM 是一种旨在优化大语言模型（LLM）推理性能的高效推理引擎，专注于解决传统 LLM 推理过程中内存管理低效的问题，从而实现更快的推理速度和更低的硬件资源消耗。
PagedAttention 是 vLLM 中的核心技术，专门设计用于优化大语言模型（LLM）推理时的内存管理。它通过引入“分页”的动态内存管理机制，有效解决了传统方法中 Key-Value (KV) 缓存分配的低效问题。

注意gpt单decoder的结构是把cross-attention的模块去掉了的，只有masked-attention和FFN。为什么要kv cache？训练的时候没有，因为训练的时候基于teacher-forcing，每次都是给所有的上下文。推理的时候自回归迭代，才需要kvcache。注意大模型推理分两个阶段，第一个是解析prompt并保存kv cache和产生第一个token，第二个阶段每个循环只会输入一个新生成的token提供q，而把新token的kv计算出来之后再和kv cache并在一起做attention。因此为什么没有q cache就知道了，因为没必要啊只有当前一个q。可以缓存Q，但没用。

因为当前token的Q要跟之前所有token的K和V算注意力，所以缓存之前的K和V是有用的，避免重复计算K和V。
Q的使命就是参与注意力值的计算，而之前所有token的注意力值已经算过了，无需再算，所以就不用缓存之前的Q了。  

PagedAttention 是一种高效管理 Key-Value Cache 的机制，它将缓存分块存储在内存和磁盘之间，适配长序列生成场景。以下通过一个具体实例，说明 PagedAttention 如何在推理中高效利用缓存生成 Token。

---

### **场景描述**
#### **任务：生成故事文本**  
Prompt:  
`"Once upon a time in a faraway land, a young prince named Arin"`

目标：基于这个 Prompt，生成后续 Token。假设序列长度非常长，超出 GPU 内存的直接缓存能力。

#### **挑战**  
- 当前模型序列长度 \(L = 50,000\)（每个 Token 的 Key 和 Value 是 \(d=16,384\) 维）。  
- 如果全缓存，KV Cache 大小为 \(L \times d \times 2 = 50,000 \times 16,384 \times 2 \approx 1.6\ \text{GB}\)。
- 单 GPU 显存不够，PagedAttention 引入分块管理缓存。


### **PagedAttention 的优势**
1. **动态内存管理**：通过分页机制，将长序列的 Key 和 Value 缓存分为活跃页（存储于 GPU）和非活跃页（存储于 CPU 或磁盘），显著降低显存需求。
2. **上下文扩展能力强**：即使序列长度达到百万级，也可以通过分块管理确保模型可以高效访问全部上下文。
3. **按需调度**：仅在需要时加载非活跃页，减少不必要的数据传输和计算。

PagedAttention 的这种优化对于大模型长序列推理尤为重要，既保证了内存效率，又提供了生成时的上下文完整性。



## gbdt是什么
GBDT，全称为Gradient Boosting Decision Tree，即梯度提升决策树，是一种集成学习算法。  

**Ensemble Learning（集成学习）** 是一种将多个学习模型结合起来，以提高整体预测性能的机器学习方法。它的基本思想是通过组合多个弱学习器（通常是简单模型）来产生一个强学习器，从而克服单一模型的缺点。集成学习通常能够显著提高模型的准确性和鲁棒性。

### 集成学习与其他机器学习算法的关系

1. **集成学习与基本学习器**：
   - 集成学习依赖于多个基本学习器（或弱学习器）。这些基本学习器可以是任何机器学习算法，如决策树、支持向量机（SVM）、线性回归、k近邻（KNN）等。集成学习的核心思想是通过组合多个弱学习器的结果，形成一个强学习器。
   
   - 例如，**随机森林（Random Forest）** 是一种基于决策树的集成学习方法。它通过多棵决策树的组合来提高预测的准确性。每棵决策树本身可能是一个“弱学习器”，但是多个决策树结合后可以显著提高性能。

2. **集成学习与单一模型的区别**：
   - **单一模型**：通常是一个单独的机器学习模型（如一棵决策树或一个支持向量机）训练出来的模型，适用于某个特定问题。
   - **集成学习**：则是将多个模型（通常是同类或不同类的多个学习器）组合起来，以提高总体的泛化能力，降低模型的偏差（bias）和方差（variance）。
     - **偏差（Bias）**：指模型对数据的简化程度，过高的偏差通常意味着模型太简单，可能无法捕捉到数据的复杂性。
     - **方差（Variance）**：指模型对训练数据的敏感程度，过高的方差可能意味着模型过拟合，不能有效地推广到新的数据。

   - 例如，单一决策树模型可能过拟合（方差高），而通过集成多棵决策树（如在随机森林中）可以减少过拟合，并提高模型的稳定性和准确性。

3. **集成学习与分类/回归算法的关系**：
   集成学习可以应用于任何分类或回归算法中。例如：
   - **分类任务**：如果你使用的基本学习器是分类器（如决策树、逻辑回归、K近邻等），那么集成学习的最终目标就是提高分类准确率。
   - **回归任务**：同理，集成学习可以应用于回归问题，其中常见的集成学习方法包括集成回归树（如梯度提升回归树和随机森林回归）。

4. **集成学习与 Bagging、Boosting、Stacking 的关系**：
   集成学习方法有多种，其中最常见的包括：
   - **Bagging（Bootstrap Aggregating）**：
     - Bagging 的目标是通过减少模型的方差来提高性能。它通过训练多个独立的模型，并对这些模型的结果进行平均（回归）或投票（分类）。
     - **随机森林** 就是 Bagging 的一个典型例子，它通过多次随机采样数据和特征的子集来训练多棵决策树，然后将这些树的预测结果进行集成。
   
   - **Boosting**：
     - Boosting 的目标是通过减少模型的偏差来提高性能。它通过逐步训练一系列模型，每个模型都试图修正前一个模型的错误预测。
     - 常见的 Boosting 算法包括 **AdaBoost** 和 **Gradient Boosting**。这些算法会给训练数据中的错分样本加大权重，迫使后续的模型更好地处理这些难以预测的样本。
   
   - **Stacking**：
     - Stacking（堆叠）是一种更加灵活的集成方法，它通过将不同类型的模型组合在一起，训练一个新的模型来整合这些模型的预测。堆叠通常会使用一个“元学习器”来根据基本学习器的输出进行最终预测。
     - 比如，可以用逻辑回归、决策树和SVM作为基本学习器，然后用一个线性回归模型作为元学习器来综合这些模型的预测结果。

5. **集成学习与深度学习的关系**：
   - 集成学习方法可以与深度学习相结合。例如，通过集成多个深度神经网络（DNN）的结果，可能会比单一的神经网络获得更好的结果。这类集成方法在某些应用中（如图像识别、NLP任务）可能具有较大的优势。
   - 例如，在 **Kaggle** 等数据科学竞赛中，往往通过集成不同类型的深度学习模型来提高准确率。


### Gradient Boosting Decision Tree，梯度提升决策树  
属于提升算法（Boosting）的一种。它通过逐步构建多个决策树，并结合每个树的预测结果来提高模型的准确性。GBDT的核心思想是通过优化目标函数（通常是损失函数）来减少误差，并在每一步迭代中使用梯度下降来修正前一轮预测的残差。

## moe
研究低算力slm 产生的灵感，我或许不需要那么多领域的覆盖，所以研究混合专家，做有限领域垂直应用。就好比人类社会，各有分工，简洁明了。  
同时直观想法横向拓展参数量，理论上就应该有提升。  
如何提升MOE并行的效率，专家之间的网络通信会成为计算的瓶颈。而且GPU擅长做矩阵运算，不擅长做分支；每个专家小模型分配的样本数较少，无法得到充分的训练；（是否可以先训练出若干个完整的大模型，例如数学大模型、代码大模型、问答大模型等，然后作为专家的初始化？）需要确保专家模型上的负载均衡

其实就是把attention的ffn换成多个网络（可以是ffn，也可以是别的所有）  
训练和推理的时候在前面加一个gate（router，也是ffn），同时设定超参数topk决定有几个专家加入推理。  
LLM用“MoE”这个词很误导人，和之前推荐系统中的MoE根本不是一回事。LLM用的这个技术准确点应该叫做SMoE。

.推荐系统中的专家是真的专家，每个专家都有特定的功能（一般是每个专家负责输入一个子业务，或者预测一个业务目标）。而LLM各“专家”都是自动学出来的，并没有什么实质的区别。实验发现，从模型中删掉一两个“专家”，也不会使模型发生质的变化，只是效果稍差一点；
推荐系统中的MoE，一般是各个专家都会计算，最后加权求和，并不能节省计算量。而LLM中一般只激活部分“专家”。  

感觉计算中的中间向量结果其实都可以理解成概率，只是需要softmax去归一化罢了（例如上面所说的router的输出就是token数、选专家的概率）  


## 基于梯度下降法的模型训练的源头
深度学习框架帮我们解决了哪些问题？其实用numpy也能实现前向反向传播，但是速度和效率都不行，且不能使用gpu加速。
```python
np.random.seed(0)

N, D = 3, 4

x = np.random.randn(N, D) 
```  
深度学习框架，帮助我们解决的核心问题就是反向传播时的梯度计算和更新，当然，它们的功能远不止这些，像各种方便的loss函数:交叉熵，MSE均方损失...；各种优化器：sgd,adam...;GPU并行计算加速等；模型的保存恢复可视化等等。(没说前向传播的原因是前向传播逻辑实际上写在了网络类里面了)  



## CoT_chain_of_thought

我先放个在这里，预训练可能会用到。
* 人工Few-shot CoT 人工直接给样例回答
* auto zero-shot CoT 只说step by step
  * auto auto-CoT 把上面的结果当做样例





## 面试题
用语言介绍一下Transformer的整体流程 
深度学习的三种并行方式：数据并行，模型并行，流水线并行 
Deepspeed分布式训练的了解，zero 0-3的了解。 
对于CLIP的了解 
说几种对比学习的损失函数，以及它们的特点和优缺点 
说说大模型生成采样的几种方式，它们的特点和优缺点比较 
损失函数中温度的作用 
BLIP的细节。（面试中提的问题是BLIP为什么将训练分成两个阶段） 
Visual Encoder有哪些常见的类型？ 
深度学习中常用的优化器有哪些？ 
SimCSE的了解 
prenorm和postnorm 
LLaMA 2的创新/ChatGLM的创新点/Qwen的创新点/Baichuan的创新点 
LLM的评估方式有哪些？特点是什么？（中文的呢？） 
文本生成模型中生成参数的作用（temperature，top p, top k，num beams） 
LoRA的作用和原理 
CoT的作用 
神经网络经典的激活函数以及它们的优缺点 
softmax函数求导的推导 
BERT的参数量如何计算？ 
AUC和ROC 
batch norm和layer norm 
大模型训练的超参数设置 
经典的词向量模型有哪些？ 
InstructGPT三个阶段的训练过程，用语言描述出来（过程，损失函数） 
大模型推理加速的方法 
Transformer中注意力的作用是什么 
RNN、CNN和Transformer的比较（复杂度，特点，适用范围etc） 
AC自动机 
产生梯度消失问题的原因有哪些？ 
大模型的幻觉问题 
大模型训练数据处理 
RLHF的计算细节 
构建CoT样本的时候，怎么保证覆盖不同的场景？ 
召回的三个指标：Recall、NDCG、RMSE 
RoPE和ALiBi 
交叉熵、NCE和InfoNCE的区别和联系 
贝叶斯学派和概率学派的区别 
一个文件的大小超过了主存容量，如何对这个文件进行排序？应该使用什么算法？ 
Python中的线程、进程和协程 
python中的生成器和迭代器
1.简历中论文课题
2.adam优化器原理
3.LLama框架和transformer框架的主要区别，以及其中的归一化操作是什么，怎样实现的
4.lora的实现原理，其他微调方法.大语言模型量化、蒸馏、剪枝操作如何实现
5.旋转位置编码的原理
6.手撕多头注意力机制
7.反问

为什么位置编码里就要drop out、dropout 的神经元是哪些
layer_norm起到一个什么效果？为什么FFN里也有dropout但是没有实际使用
内积其实是一个矩形的面积，只是几何意义表征了投影
cross的地方选词维度问题
transformer应该是多个句子一个batch（多个token向量），一个句子一个矩阵输入
文中qk的规定实际上是q中向量对于k中向量的注意
mask是在qk相乘完毕之后做的，因此mask矩阵维度和qk相乘的结果一致
代码实现有点问题？通常，d_k 和 d_v 会设置为 d_model // n_heads。  相当于写死
layernorm就是把向量的均值为0方差为1，其实就变成了模为1的向量
在使用预训练的词嵌入（如 Word2Vec 或 GloVe）时，embedding 层的权重也可以被设置为不可训练，或仅在特定情况下更新，具体取决于你的任务需求。
source embedding和target embedding还不太一样 因为词表不一样
全文两处用到注意力，一处是self attention，另一处是co attention，前者不必说，后者的k和v都是encoder的输出，所以k和v的形状（token数而非dk_dv）总是相同的

上来先自我介绍，博主简单介绍了background，然后说了下实习原因，就开始提问了。讲一下teaching forcing的原理；如果生成时全部使用真实token作为输入会有什么后果（项目相关）讲一下beam search算法原理（项目相关）训练模型时有没有遇到过loss为nan的现象，怎么解决的训练时loss一直降不下来可能有哪些原因有没有了解过并行训练，说一下原理dropout的作用训练模型时遇到out of memory怎么解决，没有额外显卡的情况下self attention的计算过程transformer中计算复杂度最高的是哪个模块，时间复杂度与什么有关，复杂度是多少打开IDE现场写算法题（leetcode easy难度）IDE现场手写self attention代码有没有调研过已有的大模型一周能实习几天

解释PPL指标这段实习持续多久，为什么不继续实习除了llama以外，还有在其他中文大模型如chatglm上做推理吗介绍一下对比了哪些baseline方法，简单介绍baseline方法你们使用的方法在不考虑推理长度超过KV Cache时，推理结果的准确性相比正常推理有变化吗有没有做过大模型微调相关工作，不考虑BERT这类预训练模型

问实习经历，介绍大模型长文本推理，主要做长文本哪方面的优化介绍下什么是长度外推你们一共测评了大模型推理过程中哪些指标你们用了什么样的测评数据集，用户输入是怎么样的你提到的baseline方法有自己的测试数据集吗你对long context这方面还有其他了解的吗场景问题：现在有个大模型正常只能处理8k的上下文，现在我们有个200w长度的prompt输入要处理，你觉得有哪些点是可以考虑优化的；我：可以用RAG解决这个问题。面试官：RAG主要处理问答，那要是做文本摘要呢；我：尝试使用prompt压缩

说一下prompt tuning和ICL（In-Context Learning）的区别、共性和联系

BN和LN的区别，NLP领域主要用哪个，为什么不用BN
Llama模型结构相对于传统的transformer架构有什么改进，这个师姐在论文讨论班还专门讲了，我又忘了，只记得RoPE，和MQA、GQA，忘了RMSNorm归一化以及SwiGLU激活函数

llama2中使用的注意力机制是什么?手写实现下分组注意力。了解langchain吗?讲讲其结构。对位置编码熟悉吗?讲讲几种位置编码的异同RLHF的具体工程是什么?包含了哪几个模型?分别讲讲 encoder-only、decoder-only、encoder-decoder 几种大模型的代表作。具体讲讲 p-tuning、lora 等微调方法，并指出它们与传统fine-tuning微调有何不同。显存不够一般怎么解决的?几种主流大模型的 loss 了解过吗? 有哪些异同?了解半精度训练吗?展开讲讲。deepspeed 用过吗? 展开讲讲。


