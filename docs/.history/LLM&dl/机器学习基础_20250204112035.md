# 深度学习相关的延伸


机器学习分为监督学习（包括类似语言模型的半监督模型）、无监督模型、强化学习。**我们主要考虑监督学习，且分类问题占很大比重。**




1. **非概率模型**：  
   假设空间 \( H \) ⇆ 参数分布 \( P(y|x) \)

2. **经验风险最小化**：  
   极大似然估计

3. **结构风险最小化**：  
   极大后验估计

4. **正则化项**：  
   分布参数的先验概率


1、统计学习方法（机器学习）三要素
-----

模型（模型假设空间）、策略（经验风险最小化、结构风险最小化，如何在假设空间中选取最优模型）、算法（实际选取模型，如梯度下降）

损失函数的期望叫做期望损失（expected loss），简单的理解就是对每个样本的损失进行平均（损失*输入样本分布的概率），学习的目标就是优化这个值。但是因为我们不知道联合概率分布的参数所以不能直接计算。所以引入经验损失（empirical loss，所有样本带入损失函数然后按样本数平均），最大似然估计就是经验损失最小化的典型例子。但是经验损失最小化有个问题训练集数据少的时候会**过拟合**。

过拟合（Overfitting）指的是模型在训练数据上表现得非常好，但在未见过的测试数据或新数据上表现较差的现象。

### 如何避免过拟合？主要是前两个原因

**1.** **增加训练数据量**：更多的数据可以帮助模型捕捉到数据的普遍规律，从而提高其泛化能力。

**2.** **简化模型**：使用更简单的模型或减少模型参数的数量，避免模型过度复杂化。比如，减少神经网络的层数、节点数，或使用更简单的回归模型。

3. **使用正则化**：引入正则化方法（如L1/L2正则化、Dropout）来防止模型过度拟合。

4. **交叉验证**：使用交叉验证方法来检查模型在不同数据集上的表现，确保它具有良好的泛化能力。

5. **数据预处理**：去除噪声和冗余特征，确保训练数据尽可能清晰和可靠。

6. **提前停止（Early Stopping）**：在训练过程中，监控验证集的性能，当验证集的性能不再提升时，提前停止训练，避免过拟合。

### 因此我们引入结构风险和结构风险最小化
在经验风险后面加入λJ（f），非负数λ是权衡经验风险和模型复杂度的系数，相当于在模型复杂度角度切入来改善过拟合问题（损失函数要多优化一些值）。贝叶斯估计的最大后验估计就是结构经验最小化的例子，模型复杂度由模型参数的先验分布表示。  






2、感知机、逻辑回归、softmax
----

最初的感知机模型其实就是线性函数外面套一个sign（值>0+1，<0-1）  
频率学派的观点的话那感知机的损失函数直接根据预测错的点到超平面的距离也没啥不行，因为最优w是固定的，因此和极大似然估计的区别只是找到w方法的区别，而01损失函数（对了+1错了+0）不好求导数
判别式模型（如BERT、逻辑回归）主要用于判别输入数据的类别或概率，而不是直接生成新样本逐个生成。






概率模型和非概率模型可以相互转化其实 只需要最大似然估计和函数归一化就行




分类问题（包括推广的标注问题）概率模型和fx模型都行 回归问题只能fx模型 这里也能看出概率模型其实就是用来最后分类的  



3、梯度爆炸和梯度消失
----

### **一、梯度消失与爆炸的是什么为什么怎么做**
梯度消失和爆炸的根源在于**反向传播中链式法则的连乘效应**，具体表现为：
- **梯度消失**：多层导数（尤其是激活函数导数和权重）的乘积趋近于零，导致浅层参数无法更新。
- **梯度爆炸**：多层导数的乘积呈指数级增长，导致参数更新步长过大，模型无法收敛。

* **根本原因**：链式求导中的连乘效应导致梯度指数级缩放。
* **核心矛盾**：激活函数导数的范围与权重矩阵的幅值共同作用。
* **解决思路**：控制权重初始化、改进激活函数、引入归一化与特殊结构。

---



### **二、反向传播梯度推导**

#### **1. 单层网络梯度推导**
##### **网络结构**：
- **输入**：\( x = [x_1, x_2] \)
- **权重**：\( w = [w_1, w_2] \)
- **线性层输出**：\( a = x_1 w_1 + x_2 w_2 \)
- **激活函数**：\( h = \text{sigmoid}(a) = \frac{1}{1 + e^{-a}} \)
- **损失函数**：\( \mathcal{L} = \frac{1}{2}(h - y)^2 \)

##### **梯度计算**：
1. **损失对输出的梯度**：
   \[
   \frac{\partial \mathcal{L}}{\partial h} = h - y \tag{1}
   \]
2. **激活函数导数**：
   \[
   \frac{\partial h}{\partial a} = h(1 - h) \tag{2}
   \]
3. **线性层梯度**：
   \[
   \frac{\partial a}{\partial x_i} = w_i, \quad \frac{\partial a}{\partial w_i} = x_i \tag{3}
   \]
4. **最终梯度（链式法则）**：
   \[
   \frac{\partial \mathcal{L}}{\partial x_i} = (h - y) \cdot h(1 - h) \cdot w_i \tag{4}
   \]
   \[
   \frac{\partial \mathcal{L}}{\partial w_i} = (h - y) \cdot h(1 - h) \cdot x_i \tag{5}
   \]

##### **关键观察**：
- 梯度包含 \( h(1 - h) \) 项（值域 \( (0, 0.25) \)），可能导致梯度缩小。
- 权重 \( w_i \) 和输入 \( x_i \) 的幅值进一步影响梯度稳定性。

---

#### **2. 双层网络梯度推导**
##### **网络结构**：
- **输入层**：\( x = [x_1, x_2] \)
- **隐藏层（第1层）**：
  - 权重：\( W^{(1)} = [w_1, w_2] \)
  - 线性输出：\( a^{(1)} = x_1 w_1 + x_2 w_2 \)
  - 激活函数：\( h^{(1)} = \text{sigmoid}(a^{(1)}) \)
- **输出层（第2层）**：
  - 权重：\( w_3 \)
  - 线性输出：\( a^{(2)} = h^{(1)} w_3 \)
  - 激活函数：\( o = \text{sigmoid}(a^{(2)}) \)
- **损失函数**：\( \mathcal{L} = \frac{1}{2}(o - y)^2 \)

##### **梯度计算**：
1. **输出层权重 \( w_3 \) 的梯度**：
   \[
   \frac{\partial \mathcal{L}}{\partial w_3} = \frac{\partial \mathcal{L}}{\partial o} \cdot \frac{\partial o}{\partial a^{(2)}} \cdot \frac{\partial a^{(2)}}{\partial w_3}
   \]
   - 逐项计算：
     \[
     \frac{\partial \mathcal{L}}{\partial o} = o - y, \quad \frac{\partial o}{\partial a^{(2)}} = o(1 - o), \quad \frac{\partial a^{(2)}}{\partial w_3} = h^{(1)}
     \]
   - 合并结果：
     \[
     \frac{\partial \mathcal{L}}{\partial w_3} = (o - y) \cdot o(1 - o) \cdot h^{(1)} \tag{6}
     \]

2. **隐藏层权重 \( w_1 \) 的梯度**：
   \[
   \frac{\partial \mathcal{L}}{\partial w_1} = \frac{\partial \mathcal{L}}{\partial o} \cdot \frac{\partial o}{\partial a^{(2)}} \cdot \frac{\partial a^{(2)}}{\partial h^{(1)}} \cdot \frac{\partial h^{(1)}}{\partial a^{(1)}} \cdot \frac{\partial a^{(1)}}{\partial w_1}
   \]
   - 逐项计算：
     \[
     \frac{\partial a^{(2)}}{\partial h^{(1)}} = w_3, \quad \frac{\partial h^{(1)}}{\partial a^{(1)}} = h^{(1)}(1 - h^{(1)}), \quad \frac{\partial a^{(1)}}{\partial w_1} = x_1
     \]
   - 合并结果：
     \[
     \frac{\partial \mathcal{L}}{\partial w_1} = (o - y) \cdot o(1 - o) \cdot w_3 \cdot h^{(1)}(1 - h^{(1)}) \cdot x_1 \tag{7}
     \]

##### **梯度表达式总结**：
\[
\begin{align*}
\text{输出层梯度} &: \frac{\partial \mathcal{L}}{\partial w_3} = \underbrace{(o - y)}_{\text{损失梯度}} \cdot \underbrace{o(1 - o)}_{\text{激活导数}} \cdot \underbrace{h^{(1)}}_{\text{隐藏层输出}} \tag{6} \\
\text{隐藏层梯度} &: \frac{\partial \mathcal{L}}{\partial w_1} = \underbrace{(o - y) \cdot o(1 - o) \cdot w_3}_{\text{上游梯度}} \cdot \underbrace{h^{(1)}(1 - h^{(1)})}_{\text{激活导数}} \cdot \underbrace{x_1}_{\text{输入}} \tag{7}
\end{align*}
\]

---

#### **3. 梯度消失与爆炸的机制对比**
| **网络类型** | **梯度表达式**                                                                 | **梯度缩放因子**                                                                 |
|--------------|------------------------------------------------------------------------------|----------------------------------------------------------------------------------|
| **单层网络** | \( \frac{\partial \mathcal{L}}{\partial w_i} = (h - y) \cdot h(1 - h) \cdot x_i \) | 单次激活导数 \( h(1 - h) \in (0, 0.25) \)                                        |
| **双层网络** | \( \frac{\partial \mathcal{L}}{\partial w_1} = \text{上游梯度} \cdot h^{(1)}(1 - h^{(1)}) \cdot x_1 \) | 两次激活导数 \( o(1 - o) \cdot h^{(1)}(1 - h^{(1)}) \in (0, 0.0625) \) + 权重 \( w_3 \) 影响 |

- **单层网络**：梯度受单次 Sigmoid 导数限制，可能缩小但幅度可控。
- **双层网络**：梯度需经历两次 Sigmoid 导数连乘（最大缩至 0.0625），且权重 \( w_3 \) 的幅值进一步放大或缩小梯度。
  - **若 \( |w_3| < 1 \)**：梯度继续缩小 → **梯度消失**。
  - **若 \( |w_3| > 1 \)**：梯度可能放大 → **梯度爆炸**。

---

#### **4. 关键结论**
1. **梯度消失根源**：
   - 激活函数导数连乘（如 Sigmoid 每层最大缩至 0.25）。
   - 权重值过小进一步加剧缩小效应。
2. **梯度爆炸根源**：
   - 权重值过大导致连乘放大效应。
3. **深层网络挑战**：
   - 层数 \( L \) 增加使缩放因子呈指数变化（\( \text{因子}^L \)），问题急剧恶化。



### **三、深层网络中的梯度传递分析**
#### **1. 梯度缩放的数学机制**
假设网络有 \( L \) 层，每层梯度传递包含两部分：
- **激活函数导数**：如 Sigmoid 的 \( h(1 - h) \in (0, 0.25) \)
- **权重矩阵**：\( W^{(l)} \) 的谱范数（最大奇异值）

梯度传递到第 \( l \) 层时的总缩放因子为：
\[
\text{缩放因子} = \prod_{k=l}^{L} \left( \text{激活导数}_k \cdot \|W^{(k)}\| \right)
\]
- **若每层缩放因子 < 1**：梯度指数衰减 → **梯度消失**
- **若每层缩放因子 > 1**：梯度指数增长 → **梯度爆炸**

#### **2. 关键因素对比**
| **因素**          | **对梯度的影响**                                                                 | **示例与解决方案**                                                                 |
|--------------------|----------------------------------------------------------------------------------|----------------------------------------------------------------------------------|
| **激活函数导数**   | Sigmoid/Tanh 导数范围受限，易导致梯度消失；ReLU 缓解此问题                       | 使用 ReLU：\( \frac{\partial \text{ReLU}}{\partial a} = 1 \text{ if } a > 0 \) 负值问题通过Leaky ReLU（负值赋予一个小的超参数斜率）或监控神经元的激活率（如超过 50% 的神经元处于“死亡”状态时调整初始化策略）解决  |
| **权重矩阵初始化** | 权重过大 → 梯度爆炸；权重过小 → 梯度消失                                         | He 初始化：\( W \sim \mathcal{N}(0, \sqrt{2/n_{\text{in}}}) \)  略                   |
| **网络深度**       | 层数增加放大连乘效应，加剧梯度问题                                               | 残差连接（ResNet）：跳过部分层，缓解梯度衰减                                      |

---

### **四、实例验证与可视化**
#### **1. Sigmoid 激活的梯度衰减**
- **假设条件**：权重 \( W^{(k)} = 1 \)，激活导数 \( h(1 - h) = 0.25 \)
- **L 层梯度缩放因子**：\( (0.25 \cdot 1)^L = 0.25^L \)
  - \( L=10 \) → \( 0.25^{10} \approx 9.5 \times 10^{-7} \)（梯度消失）

#### **2. 权重过大的梯度爆炸**
- **假设条件**：权重 \( W^{(k)} = 2 \)，激活导数 \( h(1 - h) = 0.25 \)
- **L 层梯度缩放因子**：\( (0.25 \cdot 2)^L = 0.5^L \)
  - \( L=10 \) → \( 0.5^{10} \approx 0.001 \)（相对缓和）
- **若权重 \( W^{(k)} = 3 \)**：\( (0.25 \cdot 3)^L = 0.75^L \)
  - \( L=10 \) → \( 0.75^{10} \approx 0.056 \)（仍可能爆炸）

---

### **五、解决方案与工程实践**
1. **激活函数选择**：使用 ReLU、Leaky ReLU 等缓解梯度消失。
2. **权重初始化**：
   - Xavier 初始化：适配 Sigmoid/Tanh。
   - He 初始化：适配 ReLU。
3. **归一化技术**：Batch Normalization / Layer Normalization 稳定梯度分布。
4. **架构设计**：残差连接（ResNet）、门控机制（LSTM/GRU）。






## attention和MLP有何不同
所以，注意力机制在尽可能减少算力压力的情况下，达到近似三阶张量全连接层参数量的效果，并且序列内部互相关联的机制使得它更能把握序列的特征。与二阶张量（矩阵）形式的 MLP 是不同的。
qkt和w相比肯定sai
## 样本比例问题
一看就没经受过毒打在样本比例不均衡的场景下例如正负样本1:9你不用正负样本1:1来训模型马上就收敛到全预测为负样本这样loss很低，准确率90%然后你就会发现训练出来的模型就是个粑粑然后你想救 发现救不回来 因为负样本的梯度强度是正样本的九倍模型不可避免地被负样本拐跑了你说那就给正样本加九倍的权重相当于一个正样本复制了九次那不还是正负样本1:1么而且我的实践经验是9倍正样本权重带9个负样本来训练的效果还不如正负样本1:1那不最后成了脱裤子放屁，多此一举？炼丹对前辈还是要有敬畏之心否则回旋镖打到的是你自己这个问题，原本只是回答入门初学者的疑问，但是已经被讨论到了，它本不该有深度。要把样本比例，在各种场景，各种任务下，都讨论清楚，那够写好几篇万字长文了。

我们处理不平衡数据的出发点是，训练集的数据相对于真实数据是有偏的，所以我们试图用一些技巧尽量去学到一个偏差尽量小的模型。但是也也有很多情况，训练集和真实样本空间中是一致的，尽管正负样本的比例并不是1:1，但是他仍然与真实样本空间相符合，这时候似乎就没有使用这些技巧的必要性了


## 什么是vllm？
vLLM 是一种旨在优化大语言模型（LLM）推理性能的高效推理引擎，专注于解决传统 LLM 推理过程中内存管理低效的问题，从而实现更快的推理速度和更低的硬件资源消耗。
PagedAttention 是 vLLM 中的核心技术，专门设计用于优化大语言模型（LLM）推理时的内存管理。它通过引入“分页”的动态内存管理机制，有效解决了传统方法中 Key-Value (KV) 缓存分配的低效问题。

注意gpt单decoder的结构是把cross-attention的模块去掉了的，只有masked-attention和FFN。为什么要kv cache？训练的时候没有，因为训练的时候基于teacher-forcing，每次都是给所有的上下文。推理的时候自回归迭代，才需要kvcache。注意大模型推理分两个阶段，第一个是解析prompt并保存kv cache和产生第一个token，第二个阶段每个循环只会输入一个新生成的token提供q，而把新token的kv计算出来之后再和kv cache并在一起做attention。因此为什么没有q cache就知道了，因为没必要啊只有当前一个q。可以缓存Q，但没用。

因为当前token的Q要跟之前所有token的K和V算注意力，所以缓存之前的K和V是有用的，避免重复计算K和V。
Q的使命就是参与注意力值的计算，而之前所有token的注意力值已经算过了，无需再算，所以就不用缓存之前的Q了。  

PagedAttention 是一种高效管理 Key-Value Cache 的机制，它将缓存分块存储在内存和磁盘之间，适配长序列生成场景。以下通过一个具体实例，说明 PagedAttention 如何在推理中高效利用缓存生成 Token。

---

### **场景描述**
#### **任务：生成故事文本**  
Prompt:  
`"Once upon a time in a faraway land, a young prince named Arin"`

目标：基于这个 Prompt，生成后续 Token。假设序列长度非常长，超出 GPU 内存的直接缓存能力。

#### **挑战**  
- 当前模型序列长度 \(L = 50,000\)（每个 Token 的 Key 和 Value 是 \(d=16,384\) 维）。  
- 如果全缓存，KV Cache 大小为 \(L \times d \times 2 = 50,000 \times 16,384 \times 2 \approx 1.6\ \text{GB}\)。
- 单 GPU 显存不够，PagedAttention 引入分块管理缓存。


### **PagedAttention 的优势**
1. **动态内存管理**：通过分页机制，将长序列的 Key 和 Value 缓存分为活跃页（存储于 GPU）和非活跃页（存储于 CPU 或磁盘），显著降低显存需求。
2. **上下文扩展能力强**：即使序列长度达到百万级，也可以通过分块管理确保模型可以高效访问全部上下文。
3. **按需调度**：仅在需要时加载非活跃页，减少不必要的数据传输和计算。

PagedAttention 的这种优化对于大模型长序列推理尤为重要，既保证了内存效率，又提供了生成时的上下文完整性。



## gbdt是什么
GBDT，全称为Gradient Boosting Decision Tree，即梯度提升决策树，是一种集成学习算法。  

**Ensemble Learning（集成学习）** 是一种将多个学习模型结合起来，以提高整体预测性能的机器学习方法。它的基本思想是通过组合多个弱学习器（通常是简单模型）来产生一个强学习器，从而克服单一模型的缺点。集成学习通常能够显著提高模型的准确性和鲁棒性。

### 集成学习与其他机器学习算法的关系

1. **集成学习与基本学习器**：
   - 集成学习依赖于多个基本学习器（或弱学习器）。这些基本学习器可以是任何机器学习算法，如决策树、支持向量机（SVM）、线性回归、k近邻（KNN）等。集成学习的核心思想是通过组合多个弱学习器的结果，形成一个强学习器。
   
   - 例如，**随机森林（Random Forest）** 是一种基于决策树的集成学习方法。它通过多棵决策树的组合来提高预测的准确性。每棵决策树本身可能是一个“弱学习器”，但是多个决策树结合后可以显著提高性能。

2. **集成学习与单一模型的区别**：
   - **单一模型**：通常是一个单独的机器学习模型（如一棵决策树或一个支持向量机）训练出来的模型，适用于某个特定问题。
   - **集成学习**：则是将多个模型（通常是同类或不同类的多个学习器）组合起来，以提高总体的泛化能力，降低模型的偏差（bias）和方差（variance）。
     - **偏差（Bias）**：指模型对数据的简化程度，过高的偏差通常意味着模型太简单，可能无法捕捉到数据的复杂性。
     - **方差（Variance）**：指模型对训练数据的敏感程度，过高的方差可能意味着模型过拟合，不能有效地推广到新的数据。

   - 例如，单一决策树模型可能过拟合（方差高），而通过集成多棵决策树（如在随机森林中）可以减少过拟合，并提高模型的稳定性和准确性。

3. **集成学习与分类/回归算法的关系**：
   集成学习可以应用于任何分类或回归算法中。例如：
   - **分类任务**：如果你使用的基本学习器是分类器（如决策树、逻辑回归、K近邻等），那么集成学习的最终目标就是提高分类准确率。
   - **回归任务**：同理，集成学习可以应用于回归问题，其中常见的集成学习方法包括集成回归树（如梯度提升回归树和随机森林回归）。

4. **集成学习与 Bagging、Boosting、Stacking 的关系**：
   集成学习方法有多种，其中最常见的包括：
   - **Bagging（Bootstrap Aggregating）**：
     - Bagging 的目标是通过减少模型的方差来提高性能。它通过训练多个独立的模型，并对这些模型的结果进行平均（回归）或投票（分类）。
     - **随机森林** 就是 Bagging 的一个典型例子，它通过多次随机采样数据和特征的子集来训练多棵决策树，然后将这些树的预测结果进行集成。
   
   - **Boosting**：
     - Boosting 的目标是通过减少模型的偏差来提高性能。它通过逐步训练一系列模型，每个模型都试图修正前一个模型的错误预测。
     - 常见的 Boosting 算法包括 **AdaBoost** 和 **Gradient Boosting**。这些算法会给训练数据中的错分样本加大权重，迫使后续的模型更好地处理这些难以预测的样本。
   
   - **Stacking**：
     - Stacking（堆叠）是一种更加灵活的集成方法，它通过将不同类型的模型组合在一起，训练一个新的模型来整合这些模型的预测。堆叠通常会使用一个“元学习器”来根据基本学习器的输出进行最终预测。
     - 比如，可以用逻辑回归、决策树和SVM作为基本学习器，然后用一个线性回归模型作为元学习器来综合这些模型的预测结果。

5. **集成学习与深度学习的关系**：
   - 集成学习方法可以与深度学习相结合。例如，通过集成多个深度神经网络（DNN）的结果，可能会比单一的神经网络获得更好的结果。这类集成方法在某些应用中（如图像识别、NLP任务）可能具有较大的优势。
   - 例如，在 **Kaggle** 等数据科学竞赛中，往往通过集成不同类型的深度学习模型来提高准确率。


### Gradient Boosting Decision Tree，梯度提升决策树  
属于提升算法（Boosting）的一种。它通过逐步构建多个决策树，并结合每个树的预测结果来提高模型的准确性。GBDT的核心思想是通过优化目标函数（通常是损失函数）来减少误差，并在每一步迭代中使用梯度下降来修正前一轮预测的残差。

## moe
研究低算力slm 产生的灵感，我或许不需要那么多领域的覆盖，所以研究混合专家，做有限领域垂直应用。就好比人类社会，各有分工，简洁明了。  
同时直观想法横向拓展参数量，理论上就应该有提升。  
如何提升MOE并行的效率，专家之间的网络通信会成为计算的瓶颈。而且GPU擅长做矩阵运算，不擅长做分支；每个专家小模型分配的样本数较少，无法得到充分的训练；（是否可以先训练出若干个完整的大模型，例如数学大模型、代码大模型、问答大模型等，然后作为专家的初始化？）需要确保专家模型上的负载均衡

其实就是把attention的ffn换成多个网络（可以是ffn，也可以是别的所有）  
训练和推理的时候在前面加一个gate（router，也是ffn），同时设定超参数topk决定有几个专家加入推理。  
LLM用“MoE”这个词很误导人，和之前推荐系统中的MoE根本不是一回事。LLM用的这个技术准确点应该叫做SMoE。

.推荐系统中的专家是真的专家，每个专家都有特定的功能（一般是每个专家负责输入一个子业务，或者预测一个业务目标）。而LLM各“专家”都是自动学出来的，并没有什么实质的区别。实验发现，从模型中删掉一两个“专家”，也不会使模型发生质的变化，只是效果稍差一点；
推荐系统中的MoE，一般是各个专家都会计算，最后加权求和，并不能节省计算量。而LLM中一般只激活部分“专家”。  

感觉计算中的中间向量结果其实都可以理解成概率，只是需要softmax去归一化罢了（例如上面所说的router的输出就是token数、选专家的概率）  


## 基于梯度下降法的模型训练的源头
深度学习框架帮我们解决了哪些问题？其实用numpy也能实现前向反向传播，但是速度和效率都不行，且不能使用gpu加速。
```python
np.random.seed(0)

N, D = 3, 4

x = np.random.randn(N, D) 
```  
深度学习框架，帮助我们解决的核心问题就是反向传播时的梯度计算和更新，当然，它们的功能远不止这些，像各种方便的loss函数:交叉熵，MSE均方损失...；各种优化器：sgd,adam...;GPU并行计算加速等；模型的保存恢复可视化等等。(没说前向传播的原因是前向传播逻辑实际上写在了网络类里面了)  



## CoT_chain_of_thought

我先放个在这里，预训练可能会用到。
* 人工Few-shot CoT 人工直接给样例回答
* auto zero-shot CoT 只说step by step
  * auto auto-CoT 把上面的结果当做样例





## 面试题
用语言介绍一下Transformer的整体流程 
深度学习的三种并行方式：数据并行，模型并行，流水线并行 
Deepspeed分布式训练的了解，zero 0-3的了解。 
对于CLIP的了解 
说几种对比学习的损失函数，以及它们的特点和优缺点 
说说大模型生成采样的几种方式，它们的特点和优缺点比较 
损失函数中温度的作用 
BLIP的细节。（面试中提的问题是BLIP为什么将训练分成两个阶段） 
Visual Encoder有哪些常见的类型？ 
深度学习中常用的优化器有哪些？ 
SimCSE的了解 
prenorm和postnorm 
LLaMA 2的创新/ChatGLM的创新点/Qwen的创新点/Baichuan的创新点 
LLM的评估方式有哪些？特点是什么？（中文的呢？） 
文本生成模型中生成参数的作用（temperature，top p, top k，num beams） 
LoRA的作用和原理 
CoT的作用 
神经网络经典的激活函数以及它们的优缺点 
softmax函数求导的推导 
BERT的参数量如何计算？ 
AUC和ROC 
batch norm和layer norm 
大模型训练的超参数设置 
经典的词向量模型有哪些？ 
InstructGPT三个阶段的训练过程，用语言描述出来（过程，损失函数） 
大模型推理加速的方法 
Transformer中注意力的作用是什么 
RNN、CNN和Transformer的比较（复杂度，特点，适用范围etc） 
AC自动机 
产生梯度消失问题的原因有哪些？ 
大模型的幻觉问题 
大模型训练数据处理 
RLHF的计算细节 
构建CoT样本的时候，怎么保证覆盖不同的场景？ 
召回的三个指标：Recall、NDCG、RMSE 
RoPE和ALiBi 
交叉熵、NCE和InfoNCE的区别和联系 
贝叶斯学派和概率学派的区别 
一个文件的大小超过了主存容量，如何对这个文件进行排序？应该使用什么算法？ 
Python中的线程、进程和协程 
python中的生成器和迭代器
1.简历中论文课题
2.adam优化器原理
3.LLama框架和transformer框架的主要区别，以及其中的归一化操作是什么，怎样实现的
4.lora的实现原理，其他微调方法.大语言模型量化、蒸馏、剪枝操作如何实现
5.旋转位置编码的原理
6.手撕多头注意力机制
7.反问

为什么位置编码里就要drop out、dropout 的神经元是哪些
layer_norm起到一个什么效果？为什么FFN里也有dropout但是没有实际使用
内积其实是一个矩形的面积，只是几何意义表征了投影
cross的地方选词维度问题
transformer应该是多个句子一个batch（多个token向量），一个句子一个矩阵输入
文中qk的规定实际上是q中向量对于k中向量的注意
mask是在qk相乘完毕之后做的，因此mask矩阵维度和qk相乘的结果一致
代码实现有点问题？通常，d_k 和 d_v 会设置为 d_model // n_heads。  相当于写死
layernorm就是把向量的均值为0方差为1，其实就变成了模为1的向量
在使用预训练的词嵌入（如 Word2Vec 或 GloVe）时，embedding 层的权重也可以被设置为不可训练，或仅在特定情况下更新，具体取决于你的任务需求。
source embedding和target embedding还不太一样 因为词表不一样
全文两处用到注意力，一处是self attention，另一处是co attention，前者不必说，后者的k和v都是encoder的输出，所以k和v的形状（token数而非dk_dv）总是相同的

上来先自我介绍，博主简单介绍了background，然后说了下实习原因，就开始提问了。讲一下teaching forcing的原理；如果生成时全部使用真实token作为输入会有什么后果（项目相关）讲一下beam search算法原理（项目相关）训练模型时有没有遇到过loss为nan的现象，怎么解决的训练时loss一直降不下来可能有哪些原因有没有了解过并行训练，说一下原理dropout的作用训练模型时遇到out of memory怎么解决，没有额外显卡的情况下self attention的计算过程transformer中计算复杂度最高的是哪个模块，时间复杂度与什么有关，复杂度是多少打开IDE现场写算法题（leetcode easy难度）IDE现场手写self attention代码有没有调研过已有的大模型一周能实习几天

解释PPL指标这段实习持续多久，为什么不继续实习除了llama以外，还有在其他中文大模型如chatglm上做推理吗介绍一下对比了哪些baseline方法，简单介绍baseline方法你们使用的方法在不考虑推理长度超过KV Cache时，推理结果的准确性相比正常推理有变化吗有没有做过大模型微调相关工作，不考虑BERT这类预训练模型

问实习经历，介绍大模型长文本推理，主要做长文本哪方面的优化介绍下什么是长度外推你们一共测评了大模型推理过程中哪些指标你们用了什么样的测评数据集，用户输入是怎么样的你提到的baseline方法有自己的测试数据集吗你对long context这方面还有其他了解的吗场景问题：现在有个大模型正常只能处理8k的上下文，现在我们有个200w长度的prompt输入要处理，你觉得有哪些点是可以考虑优化的；我：可以用RAG解决这个问题。面试官：RAG主要处理问答，那要是做文本摘要呢；我：尝试使用prompt压缩

说一下prompt tuning和ICL（In-Context Learning）的区别、共性和联系

BN和LN的区别，NLP领域主要用哪个，为什么不用BN
Llama模型结构相对于传统的transformer架构有什么改进，这个师姐在论文讨论班还专门讲了，我又忘了，只记得RoPE，和MQA、GQA，忘了RMSNorm归一化以及SwiGLU激活函数

llama2中使用的注意力机制是什么?手写实现下分组注意力。了解langchain吗?讲讲其结构。对位置编码熟悉吗?讲讲几种位置编码的异同RLHF的具体工程是什么?包含了哪几个模型?分别讲讲 encoder-only、decoder-only、encoder-decoder 几种大模型的代表作。具体讲讲 p-tuning、lora 等微调方法，并指出它们与传统fine-tuning微调有何不同。显存不够一般怎么解决的?几种主流大模型的 loss 了解过吗? 有哪些异同?了解半精度训练吗?展开讲讲。deepspeed 用过吗? 展开讲讲。


