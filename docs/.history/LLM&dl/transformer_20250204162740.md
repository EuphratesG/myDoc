# transformer相关扣细节

1、[为什么 Transformer 需要进行 Multi-head Attention？](https://www.zhihu.com/question/341222779/answer/814111138)
-------------------------------------------------------------------------------------------------------------

可以类比 CNN 中同时使用**多个卷积核**的作用，直观上讲，不同随机初始化的多头的注意力**有助于网络捕捉到更丰富的特征 / 信息**。Multi-Head 其实不是必须的，去掉一些头效果依然有不错的效果（而且效果下降可能是因为参数量下降），这是因为在头足够的情况下，这些头已经能够有关注位置信息、关注语法信息、关注罕见词的能力了，再多一些头，无非是一种 enhance 或 noise 而已。  
**至于为什么每个head相比原dmodel要降维，参数少的同时特征空间多一点**。


2、transformer 是如何处理可变长度数据的？
-----------------------------------------------------------------------------------------

对不等长的数据，按照最长或者固定长度进行补齐，利用 padding mask 机制，补齐的数据并不参与训练，transformer 在**计算 attention 矩阵时进行padding mask处理**。这里是因为 padding 都是 0，e0=1, 但是 softmax 的函数，也会导致为 padding 的值占全局一定概率，mask 就是让这部分值取无穷小，让他再 softmax 之后基本也为 0，不去影响非 attention socore 的分布 mask为True的位置会被设为-1e9

3、Transformer 为什么 Q 和 K 使用不同的权重矩阵生成，为何不能使用同一个值进行自身的点乘？
------------------------------------------------------------------------------------------------------------------------

Transformer 模型中，**Q（Query）** 和 **K（Key）** 使用不同的权重矩阵生成，而非共享同一矩阵，这一设计是出于对注意力机制灵活性、表达能力以及任务适配性的深度考量。以下是具体原因和逻辑分析：

---

### **1. 打破注意力权重的对称性**
假设 Q 和 K 共享同一矩阵（即 Q=K），则注意力分数矩阵 **A** 会天然对称：  
\[
A_{i,j} = \text{Softmax}(Q_i \cdot K_j) = \text{Softmax}(Q_j \cdot K_i) = A_{j,i}
\]  
这意味着 **Token A 对 Token B 的关注度** 必须等于 **Token B 对 Token A 的关注度**。然而，这在语言建模中并不合理。  
- **例子**：句子 *“猫追老鼠”* 中，“追”对“猫”的关注度（动作发起者）应高于“猫”对“追”的关注度，但对称性会强制二者相等，限制模型表达能力。

使用独立的 Q 和 K 矩阵，可让模型学习**非对称的注意力关系**，更贴合实际语义需求。

---

### **2. 避免自注意力过度偏向自身位置**
若 Q=K，每个 Token 的 Query 和 Key 来自同一投影，自注意力分数（即对角线元素）会显著偏高：  
\[
Q_i \cdot K_i = Q_i \cdot Q_i = ||Q_i||^2 \quad (\text{点积为向量自身的模长平方})
\]  
这会导致模型倾向于**过度关注自身位置**，而非根据上下文动态调整注意力。  
- **问题**：例如在句子 *“他打开了窗户因为房间太热”* 中，“房间”需要关注“热”而非自身，但对称投影可能削弱这种跨位置的关联。

独立投影后，Q 和 K 的向量空间解耦，模型可通过训练自主决定是否强调自关注或跨位置关注。

---

### **3. 增强投影空间的多样性**
Q、K、V 使用不同权重矩阵，本质上是将输入映射到**三个不同的子空间**：  
- **Query 空间**：关注“当前 Token 需要什么信息”。  
- **Key 空间**：关注“其他 Token 能提供什么信息”。  
- **Value 空间**：编码“其他 Token 的实际内容信息”。  

若 Q 和 K 共享矩阵，相当于强制 Key 和 Query 的语义角色一致，限制了模型对不同语义角色的建模能力。

---

### **4. 初始化与训练的稳定性**
- **初始化阶段**：若 Q=K，随机初始化可能导致某些非对角线注意力分数异常偏高（例如某些 \(Q_i \cdot Q_j\) 偶然较大），迫使模型在训练初期需额外努力修正这些偏差。  
- **训练动态**：独立投影为模型提供了更灵活的优化路径，允许 Q 和 K 独立适应不同任务需求（如问答任务中 Query 需更主动“提问”，Key 需被动“回答”）。


---

### **总结**
| **设计选择**           | **Q/K 独立投影**                          | **Q/K 共享投影**                          |
|-------------------------|------------------------------------------|------------------------------------------|
| **注意力对称性**         | 非对称，灵活建模单向关系                  | 强制对称，限制表达能力                   |
| **自注意力偏向性**       | 可动态学习自关注或跨位置关注              | 易过度关注自身位置                       |
| **投影空间多样性**       | 支持多角色语义建模（Query/Key 分工明确）  | 语义角色混叠，灵活性下降                 |
| **训练稳定性**           | 优化路径更灵活，收敛稳定                  | 需额外修正初始化偏差                     |

因此，Q 和 K 使用独立权重矩阵是 Transformer 设计中的关键优化，平衡了表达能力、灵活性与训练效率。


4、Transformer 计算 attention 的时候为何选择点乘而不是加法？两者计算复杂度和效果上有什么区别？
-----------------------------------------------------------

好的！让我们更详细地梳理**加法注意力（Additive Attention）**的机制、计算过程及其与点乘注意力的区别。

---

### **1. 加法注意力的起源**
加法注意力最早由 **Bahdanau 等人**在 2015 年提出，用于解决 RNN 在机器翻译中的长距离依赖问题（论文 [_Neural Machine Translation by Jointly Learning to Align and Translate_](https://arxiv.org/abs/1409.0473)）。它是注意力机制的早期形式，后来被 Transformer 中的点乘注意力取代。

---

### **2. 加法注意力的计算步骤**
假设有一个 **Query 向量 \(Q_i\)**（当前目标位置的表示）和一组 **Key 向量 \(\{K_j\}\)**（源序列的表示），加法注意力通过以下步骤计算注意力分数：

#### **(1) 拼接 Query 和 Key**
将 \(Q_i\) 和 \(K_j\) **拼接**（或相加）为一个联合向量：
\[
\text{Concat}(Q_i, K_j) \quad \text{或} \quad Q_i + K_j
\]

#### **(2) 非线性变换**
通过一个全连接层（参数矩阵 \(W\)）和非线性激活函数（如 \(\tanh\)）生成中间表示：
\[
h_{ij} = \tanh(W \cdot [Q_i; K_j] + b)
\]
其中：
- \(W\) 是权重矩阵，维度为 \(d \times 2d\)（假设 \(Q_i, K_j\) 的维度均为 \(d\)）。
- \(b\) 是偏置项。

#### **(3) 计算注意力分数**
将中间表示 \(h_{ij}\) 与一个可学习的**权重向量 \(v\)** 做点乘，得到标量分数：
\[
\text{Score}(Q_i, K_j) = v^T \cdot h_{ij}
\]
- 这里的 \(v\) 是维度为 \(d\) 的向量。

#### **(4) Softmax 归一化**
对所有 Key 位置的分数进行 Softmax 归一化，得到注意力权重：
\[
\alpha_{ij} = \text{Softmax}(\text{Score}(Q_i, K_j))
\]

#### **(5) 加权求和**
用注意力权重对 **Value 向量 \(V_j\)** 加权求和，得到最终输出：
\[
\text{Output}_i = \sum_j \alpha_{ij} V_j
\]

---

### **3. 加法注意力的特点**
#### **(1) 核心设计**
- **非线性交互**：通过 \(\tanh\) 激活函数，捕捉 Query 和 Key 之间的复杂非线性关系。
- **可学习参数**：引入了额外的参数 \(W\) 和 \(v\)，增强了模型的表达能力。

#### **(2) 计算复杂度**
- **时间复杂度**：计算每个 Query-Key 对的分数需要 \(O(d^2)\) 操作（由全连接层 \(W \cdot [Q_i; K_j]\) 导致），总复杂度为 \(O(n^2 d^2)\)（\(n\) 为序列长度）。
- **空间复杂度**：需存储中间矩阵 \(W\) 和向量 \(v\)，参数量随维度 \(d\) 平方增长。

#### **(3) 与点乘注意力的对比**
| **维度**       | **加法注意力**                              | **点乘注意力**                              |
|----------------|--------------------------------------------|--------------------------------------------|
| **交互方式**    | 非线性（\(\tanh\) + 全连接）                | 线性（点积）                                |
| **参数量**      | 更多（引入 \(W\) 和 \(v\)）                  | 更少（仅需 Q/K/V 投影矩阵）                  |
| **计算效率**    | 低（逐位置非线性计算）                      | 高（矩阵乘法并行化）                        |
| **硬件适配性**  | 差（非矩阵密集型操作）                      | 优（GPU 友好）                              |

---

### **4. 为什么 Transformer 未采用加法注意力？**
#### **(1) 计算效率瓶颈**
- **长序列处理**：当序列长度 \(n=1000\)、维度 \(d=1024\) 时，加法注意力的计算量是点乘的 \(d\) 倍（\(O(n^2 d^2)\) vs \(O(n^2 d)\)），导致训练和推理速度大幅下降。
- **并行化受限**：加法注意力中的逐位置非线性计算难以充分利用 GPU 的矩阵加速能力。

#### **(2) 实验效果未显著提升**
- Transformer 论文发现，在相同模型规模下，点乘注意力通过缩放因子 \(\sqrt{d_k}\) 调整后，效果与加法注意力相当甚至更优。

#### **(3) 参数冗余**
- 加法注意力引入的额外参数 \(W\) 和 \(v\) 增加了模型复杂度，可能引发过拟合，尤其在小规模数据集上。

---

### **5. 加法注意力的应用场景**
尽管在 Transformer 中被取代，加法注意力仍有一定价值：
- **小规模模型**：参数量较少时，非线性交互可能捕捉到更复杂的模式。
- **特定任务结构**：某些任务（如需要强位置交互的序列对齐）可能受益于非线性设计。
- **理论研究**：作为注意力机制的早期形式，帮助理解注意力机制的本质。

---

### **6. 图解加法注意力 vs 点乘注意力**
#### **(1) 加法注意力结构**
```
Query (Q_i) --> [拼接] --> [全连接 + tanh] --> [向量v] --> 分数
  Key (K_j) ----↑
```

#### **(2) 点乘注意力结构**
```
Query (Q_i) --> [点乘] --> [缩放 + Softmax] --> 分数
  Key (K_j) ----↑
```

---

### **7. 代码示例（简化版加法注意力）**
```python
import torch
import torch.nn as nn

class AdditiveAttention(nn.Module):
    def __init__(self, d_model):
        super().__init__()
        self.W = nn.Linear(2 * d_model, d_model)  # 拼接后维度是 2d
        self.v = nn.Linear(d_model, 1)            # 生成标量分数

    def forward(self, Q, K, V):
        # Q: [batch_size, n_q, d]
        # K: [batch_size, n_k, d]
        scores = []
        for i in range(Q.size(1)):
            q_i = Q[:, i, :]  # 当前 Query
            expanded_q = q_i.unsqueeze(1).expand(-1, K.size(1), -1)  # 复制为 [batch, n_k, d]
            concat = torch.cat([expanded_q, K], dim=-1)  # [batch, n_k, 2d]
            h = torch.tanh(self.W(concat))               # [batch, n_k, d]
            score = self.v(h).squeeze(-1)                # [batch, n_k]
            scores.append(score)
        scores = torch.stack(scores, dim=1)              # [batch, n_q, n_k]
        attn_weights = torch.softmax(scores, dim=-1)
        output = torch.bmm(attn_weights, V)              # [batch, n_q, d]
        return output
```

---

### **总结**
- **加法注意力**：通过非线性交互增强表达能力，但计算复杂度高，难以扩展到大模型。
- **点乘注意力**：牺牲部分非线性能力，换取计算效率和硬件友好性，成为 Transformer 的核心设计。


5、[为什么在进行 softmax 之前需要对 attention 进行 scaled（为什么除以 dk 的平方根），并使用公式推导进行讲解](https://www.zhihu.com/question/339723385/answer/782509914)
----------------------------------------------------------------------------------------------------------------------------------


### **Transformer 中 Q·K 内积方差与维度 \(d_k\) 的关系及对 Softmax 的影响**

在 Transformer 的自注意力机制中，**Query (\(Q\))** 和 **Key (\(K\))** 的点积方差与向量维度 \(d_k\) 呈正相关，这一现象源自随机变量的累积效应。其数学推导和实际影响如下：

---

#### **1. 内积方差与 \(d_k\) 的数学关系**
假设 \(Q\) 和 \(K\) 的每个元素是独立初始化的随机变量，且满足：
- 均值为 0：\(\mathbb{E}[q_i] = \mathbb{E}[k_i] = 0\)
- 方差为 1：\(\text{Var}(q_i) = \text{Var}(k_i) = 1\)

**点积方差推导**：
1. **单个元素方差**：  
   \[
   \text{Var}(q_i k_i) = \mathbb{E}[(q_i k_i)^2] - (\mathbb{E}[q_i k_i])^2 = \mathbb{E}[q_i^2] \mathbb{E}[k_i^2] - 0 = 1 \cdot 1 = 1
   \]
   （因 \(q_i\) 和 \(k_i\) 独立，\(\mathbb{E}[q_i k_i] = \mathbb{E}[q_i] \mathbb{E}[k_i] = 0\)）

2. **整体点积方差**：  
   \[
   \text{Var}(Q \cdot K) = \text{Var}\left( \sum_{i=1}^{d_k} q_i k_i \right) = \sum_{i=1}^{d_k} \text{Var}(q_i k_i) = d_k \cdot 1 = d_k
   \]
   - **结论**：点积方差与 \(d_k\) **线性正相关**。

---

#### **2. 方差增大对 Softmax 的影响**
假设 \(Q \cdot K\) 的方差为 \(d_k\)，随着 \(d_k\) 增大，点积值的分布呈现以下特性：
1. **极端值出现概率上升**：  
   - 根据中心极限定理，点积值 \(Q \cdot K\) 近似服从均值为 0、方差为 \(d_k\) 的正态分布 \(N(0, d_k)\)。
   - \(d_k\) 越大，点积值的绝对值可能越大（如 \(d_k=100\) 时，点积值可能达到 ±10 标准差）。

2. **Softmax 的极化效应**：  
   - Softmax 函数对输入值的指数放大效应：\( \text{Softmax}(x_i) = \frac{e^{x_i}}{\sum_j e^{x_j}} \)。
   - 若某位置的点积值 \(x_i\) 显著大于其他位置，\(e^{x_i}\) 将主导分母，导致该位置的注意力权重接近 1（one-hot 分布）。

**示例**：  
- 当 \(d_k=64\)，点积值可能分布在 ±8 左右（标准差 \(\sqrt{64}=8\)）。
- 若某位置的 \(x_i = 8\)，而其他位置 \(x_j ≈ 0\)，则：
  \[
  \text{Softmax}(x_i) ≈ \frac{e^{8}}{e^{8} + (d_k-1)e^{0}} ≈ \frac{2980}{2980 + 63} ≈ 0.98
  \]
  注意力权重高度集中于最大值位置。

---

#### **3. 训练中的梯度问题**
当 Softmax 输出趋向于 one-hot 分布时，会导致梯度消失：
1. **Softmax 梯度公式**：  
   \[
   \frac{\partial \text{Softmax}(x_i)}{\partial x_j} = \text{Softmax}(x_i)(\delta_{ij} - \text{Softmax}(x_j))
   \]
   - 若 \(\text{Softmax}(x_i) ≈ 1\)，其他 \(\text{Softmax}(x_j) ≈ 0\)，则：
     \[
     \frac{\partial \text{Softmax}(x_i)}{\partial x_i} ≈ 1 \cdot (1 - 1) = 0
     \]
     \[
     \frac{\partial \text{Softmax}(x_i)}{\partial x_j} ≈ 1 \cdot (0 - 0) = 0 \quad (j \neq i)
     \]
   - **结论**：梯度趋近于 0，参数更新停滞。

2. **模型收敛困难**：  
   - 梯度消失导致模型无法通过反向传播有效调整注意力权重，尤其对长序列任务（需动态关注不同位置）影响显著。

---

#### **4. Transformer 的解决方案：缩放点积注意力**
为缓解上述问题，Transformer 引入**缩放因子 \(\frac{1}{\sqrt{d_k}}\)**：  
\[
\text{Attention}(Q, K, V) = \text{Softmax}\left( \frac{QK^T}{\sqrt{d_k}} \right)V
\]

**作用分析**：
- **方差修正**：缩放后点积方差变为 \(\frac{d_k}{d_k} = 1\)，与 \(d_k\) 无关。
- **梯度稳定性**：Softmax 输入值的尺度受控，避免极端值导致的梯度消失。
- **效果验证**：实验表明，缩放后模型训练更稳定，收敛速度更快。

---

### **总结**
| **关键点**                 | **解释**                                                                 |
|----------------------------|-------------------------------------------------------------------------|
| **内积方差与 \(d_k\) 正相关** | 随机变量累积效应导致方差随维度线性增长。                                 |
| **Softmax 极化效应**         | 高方差使点积值差异被指数函数放大，注意力权重趋于 one-hot 分布。           |
| **梯度消失问题**             | one-hot 分布导致 Softmax 梯度趋近于 0，阻碍参数更新。                     |
| **缩放因子的作用**           | 控制点积值尺度，稳定训练动态，是 Transformer 成功的关键设计之一。          |




6、transformer 为什么使用 layer normalization，而不是其他的归一化方法？
-----------------------------------------------------------------------------------------------------------------------------------------------------------------



代码：https://www.zywvvd.com/notes/study/deep-learning/layer/norm-layer/norm-layer/#
bn和ln：https://blog.csdn.net/Little_White_9/article/details/123345062    
公式：https://zhuanlan.zhihu.com/p/524829507





### Transformer 为何需要归一化？

### **一、提升模型训练稳定性（梯度消失梯度爆炸）**
#### **1. 缓解梯度问题（相当于约束激活值）**
- **梯度爆炸/消失**：深层网络中，激活值的分布可能逐层偏移，导致反向传播时梯度异常（过大或过小）。
- **LN 的作用**：通过对每层输入进行归一化（均值为0，方差为1），约束激活值的尺度，使梯度保持在合理范围内。
  
**示例**：  
假设某层权重初始化过大，激活值 \(x\) 的幅度显著增加。经过 LN 后：
\[
\hat{x} = \frac{x - \mu}{\sigma}, \quad y = \gamma \hat{x} + \beta
\]
即使 \(x\) 的原始值很大，归一化后的 \(\hat{x}\) 被限制在合理范围，后续层的输入不会因前层权重问题失控。

#### **2. 减少内部协变量偏移（Internal Covariate Shift, ICS）**
- **ICS 问题**：简单来说 就是在每一个layer输出的feature map的distribution都不一样，发生了shift.比如 输入的是一个normalized 好的（mean为0 var为1），但是过了几个layer mean不断变大 导致无法训练. bn所做的事情就是在每个layer后都把输出重新变为 mean为0 var为1的分布（虽然有个scaling term）

- **LN 的解决**：标准化每层输入，使其分布相对固定，降低参数更新的连锁影响。

---

### **二、加速模型收敛**
#### **1. 优化地形平滑化**
- **输入分布一致性**：LN 确保每层的输入分布稳定，优化器（如 Adam）在更新参数时，无需频繁调整方向以应对分布变化。
- **更平坦的损失曲面**：归一化后的数据分布更集中，损失函数的梯度方向更一致，参数更新路径更直接。

**类比**：  
想象在崎岖的山地（未归一化）与平整的公路（归一化）上跑步，后者更容易快速到达终点。

---

### **三、降低对权重初始化的敏感性（也可以归类到一中）**
#### **1. 权重初始化的挑战**
- **极端初始化的影响**：若权重初始化过大或过小，激活值可能进入饱和区（如 Sigmoid 的两端或 ReLU 的死亡区），导致梯度消失。
- **传统解决方案**：依赖精细的初始化策略（如 He 初始化），但对超参数敏感。


---





### **二、Batch Normalization（BN）与 Layer Normalization（LN）对比**


### **批量归一化（Batch Normalization, BN）**
#### **输入数据形状**：
- 图像数据：`[N, C, H, W]`（批次大小 N，通道数 C，高度 H，宽度 W）
- 序列数据：`[N, T, D]`（批次大小 N，序列长度 T，特征维度 D）

#### **计算步骤**：
1. **计算通道/特征维度的均值和方差**：
   - **均值**：
     \[
     \mu_B = \frac{1}{N \cdot H \cdot W} \sum_{n=1}^N \sum_{h=1}^H \sum_{w=1}^W x_{n,c,h,w} \quad (\text{图像}) 
     \]
   - **方差**：
     \[
     \sigma_B^2 = \frac{1}{N \cdot H \cdot W} \sum_{n=1}^N \sum_{h=1}^H \sum_{w=1}^W (x_{n,c,h,w} - \mu_B)^2 \quad (\text{图像})
     \]

2. **归一化**：
   \[
   \hat{x} = \frac{x - \mu_B}{\sqrt{\sigma_B^2 + \epsilon}}
   \]
   - 其中 \(\epsilon\) 为数值稳定性常数（如 \(10^{-5}\)）。

3. **仿射变换（可训练参数）**：
   \[
   y = \gamma \cdot \hat{x} + \beta
   \]
   - **参数维度**：\(\gamma, \beta \in \mathbb{R}^C\)（图像）或 \(\mathbb{R}^D\)（序列），与通道/特征维度对齐。

#### **关键特性**：
- **训练与测试差异**：训练时使用当前批次统计量 \(\mu_B, \sigma_B^2\)，测试时使用移动平均统计量 \(\mu_{\text{EMA}}, \sigma_{\text{EMA}}^2\)。
- **适用场景**：CNN（固定长度输入）、稳定批次统计量的任务。

---

### **层归一化（Layer Normalization, LN）**
#### **输入数据形状**：
- 通用形状：`[N, T, D]`（序列数据）或 `[N, D]`（全连接层输出）

#### **计算步骤**：
1. **计算单个样本所有特征的均值和方差**：
   - **均值**：
     \[
     \mu_L = \frac{1}{D} \sum_{d=1}^D x_{n,t,d} \quad (\text{序列}) \quad \text{或} \quad \mu_L = \frac{1}{D} \sum_{d=1}^D x_{n,d} \quad (\text{全连接})
     \]
   - **方差**：
     \[
     \sigma_L^2 = \frac{1}{D} \sum_{d=1}^D (x_{n,t,d} - \mu_L)^2 \quad (\text{序列}) \quad \text{或类似扩展}
     \]

2. **归一化**：
   \[
   \hat{x} = \frac{x - \mu_L}{\sqrt{\sigma_L^2 + \epsilon}}
   \]

3. **仿射变换（可训练参数）**：
   \[
   y = \gamma \cdot \hat{x} + \beta
   \]
   - **参数维度**：\(\gamma, \beta \in \mathbb{R}^D\)，与特征维度对齐。

#### **关键特性**：
- **训练与测试一致**：始终使用单个样本的统计量，无移动平均。
- **适用场景**：RNN、Transformer（变长序列）、动态输入分布的任务。

---

### **公式对比与核心差异**
| **特性**               | **BN（批量归一化）**                                      | **LN（层归一化）**                                      |
|-------------------------|---------------------------------------------------------|--------------------------------------------------------|
| **归一化维度**          | 沿批次维度（`N`）和空间维度（`H, W` 或 `T`）              | 沿特征维度（`D`）                                      |
| **参数维度**            | \(\gamma, \beta \in \mathbb{R}^C\)（通道）或 \(\mathbb{R}^D\)（特征） | \(\gamma, \beta \in \mathbb{R}^D\)（特征）             |
| **统计量依赖**          | 当前批次数据（训练）→ 移动平均（测试）                     | 单个样本数据（始终一致）                                |
| **对填充的鲁棒性**      | 差（填充值污染批次统计量）                                 | 好（仅依赖单样本统计）                                  |

---

### **示例说明**
#### **1. BN 在 CNN 中的应用**
- **输入**：图像数据 `[N=32, C=64, H=28, W=28]`
- **计算**：
  - 对每个通道 \(c \in [1, 64]\)，计算所有样本、所有位置的均值 \(\mu_B^{(c)}\) 和方差 \(\sigma_B^{2(c)}\)。
  - 归一化后通过 \(\gamma^{(c)}, \beta^{(c)}\) 调整。

#### **2. LN 在 Transformer 中的应用**
- **输入**：序列数据 `[N=16, T=100, D=512]`
- **计算**：
  - 对每个样本 \(n \in [1, 16]\) 和每个时间步 \(t \in [1, 100]\)，计算特征维度 \(D=512\) 的均值和方差。
  - 归一化后通过 \(\gamma, \beta \in \mathbb{R}^{512}\) 调整。

---

### **总结**
- **BN公式**：通过批次和空间维度统计量归一化，依赖批次数据。
- **LN公式**：通过特征维度统计量归一化，依赖单样本数据。
- **设计动机**：BN 适用于固定结构的静态数据（如图像），LN 适用于动态序列和变长输入。
---

### **三、Transformer 为何选择 Layer Normalization？**
#### **1. 序列建模的特性**
Transformer 的核心是处理**变长序列数据**（如文本、语音），其特点包括：
- 序列长度动态变化，存在大量填充（Padding）。
- 每个位置（Token）需独立建模上下文关系（如自注意力机制）。

**BN 的局限性**：
- 填充位置会破坏统计量计算（如填充 0 会被计入均值）。
- 小批量时（尤其在训练初期），BN 的统计量估计不稳定。BN存在训练和测试的不一致的问题，下图表明，在小batch size情况下，这种问题很严重，也就是说在evaluate时，使用验证集的mini-batch statisitics和训练集学到的population statistics的差异会大大影响性能。


**LN 的优势**：
- 对每个位置的**独立归一化**，天然适配变长序列。
- 无需跨样本统计量，避免填充干扰。

---

#### **2. 自注意力机制的需求**
自注意力机制（Self-Attention）的输入是**位置独立的**（Position-wise）：
- 每个 Token 的 Query/Key/Value 由其自身特征生成。
- 注意力权重通过 Token 间的交互动态计算。

**LN 的作用**：
- 对每个 Token 的特征进行归一化，确保不同位置的输入分布一致。
- 增强模型对输入特征的**尺度不变性**，使注意力权重更关注语义关系而非特征量纲。

---

#### **3. 训练稳定性与收敛速度**
- **梯度平滑性**：LN 对每个样本独立归一化，缓解了梯度爆炸或消失问题。
- **参数初始化鲁棒性**：LN 使得模型对初始权重的敏感性降低，尤其适合深层网络（如 Transformer 的堆叠结构）。

---

#### **4. 实际效果验证**
- **实验对比**：在语言模型、机器翻译等任务中，LN 显著优于 BN。
- **理论支持**：LN 的统计量计算与序列长度无关，更适合自回归生成任务。

---


### **四、总结：Transformer 选择 LN 的逻辑链**
1. **任务特性**：处理变长序列 → 需避免填充干扰 → 排除 BN。
2. **结构适配**：自注意力机制位置独立 → 需对每个 Token 归一化 → LN 天然适配。
3. **训练稳定性**：LN 提升梯度平滑性 → 加速深层网络收敛。
4. **实验验证**：LN 在语言任务中表现更优 → 成为 Transformer 标配。

因此，Layer Normalization 是 Transformer 在序列建模任务中的最优选择。



7、[Transformer 不同 batch 的长度可以不一样吗？还有同一 batch 内为什么需要长度一样？](https://zhuanlan.zhihu.com/p/360144789/ht%3C/i%3Etps://www.zhihu.com/questio%3Ci%3En/4%3C/i%3E39438113/answer/1714391336)
------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
- 不同 batch 的句子 token 数量可以不同，pytorch在并行时把seq_len填充到一致再padding
- 同一 batch 内，所有句子的 token 数量 必须一致。 
- 向量维度（token 嵌入维度） 在所有句子中是相同的，不受句子长度影响。

8、[Transformer 的 Positional embedding 为什么有用？](https://www.zhihu.com/question/385895601/answer/1146997944)
----------------------------------------------------------------------------------------------------------
可固定位置编码也可用可训练的位置编码

9、[Transformer 在哪里做了权重共享，为什么可以做权重共享？好处是什么？](https://www.zhihu.com/question/333419099/answer/743341017)
-------------------------------------------------------------------------------------------------------

Transformer 在两个地方进行了权重共享：

**（1）**Encoder 和 Decoder 间的 Embedding 层权重共享；

**（2）**Decoder 中 Embedding 层和 FC 层权重共享。

**对于（1）**，《[Attention is all you need](https://zhida.zhihu.com/search?content_id=168161579&content_type=Article&match_order=1&q=Attention+is+all+you+need&zhida_source=entity)》中 Transformer 被应用在机器翻译任务中，源语言和目标语言是不一样的，但它们可以共用一张大词表，对于两种语言中共同出现的词（比如：数字，标点等等）可以得到更好的表示，而且对于 Encoder 和 Decoder，**嵌入时都只有对应语言的 embedding 会被激活**，因此是可以共用一张词表做权重共享的。

论文中，Transformer 词表用了 bpe 来处理，所以最小的单元是 subword。英语和德语同属[日耳曼语族](https://zhida.zhihu.com/search?content_id=168161579&content_type=Article&match_order=1&q=%E6%97%A5%E8%80%B3%E6%9B%BC%E8%AF%AD%E6%97%8F&zhida_source=entity)，有很多相同的 subword，可以共享类似的语义。而像中英这样相差较大的语系，语义共享作用可能不会很大。

但是，共用词表会使得词表数量增大，增加 softmax 的计算时间，因此实际使用中是否共享可能要根据情况权衡。


**对于（2）**，Embedding 层可以说是通过 onehot 去取到对应的 embedding 向量，FC 层可以说是相反的，通过向量（定义为 x）去得到它可能是某个词的 softmax 概率，取概率最大（贪婪情况下）的作为预测值。

那哪一个会是概率最大的呢？在 FC 层的每一行量级相同的前提下，理论上和 x 相同的那一行对应的点积和 softmax 概率会是最大的（可类比本文问题 1）。

因此，Embedding 层和 FC 层权重共享，[Embedding 层](https://zhida.zhihu.com/search?content_id=168161579&content_type=Article&match_order=5&q=Embedding%E5%B1%82&zhida_source=entity)中和向量 x 最接近的那一行对应的词，会获得更大的预测概率。实际上，Decoder 中的 **Embedding 层和 FC 层有点像互为逆过程,不一定哪个词，这样参数共享相当于加速训练了**。

通过这样的权重共享可以减少参数的数量，加快收敛。

但开始我有一个困惑是：Embedding 层参数维度是：(v,d)，FC 层参数维度是：(d,v)，可以直接共享嘛，还是要转置？其中 v 是词表大小，d 是 [embedding 维度](https://zhida.zhihu.com/search?content_id=168161579&content_type=Article&match_order=1&q=embedding%E7%BB%B4%E5%BA%A6&zhida_source=entity)。

查看 pytorch 源码发现真的可以直接共享：

```
fc = nn.Linear(d, v, bias=False)    # Decoder FC层定义
​
weight = Parameter(torch.Tensor(out_features, in_features))   # Linear层权重定义

```

Linear 层的权重定义中，是按照 (out_features, in_features) 顺序来的，实际计算会先将 weight 转置在乘以输入矩阵。所以 FC 层 对应的 Linear 权重维度也是 (v,d)，可以直接共享。

10、transformer 一个 block 中最耗时的部分？
--------------------------------
在 **Transformer** 中，一个 **block** 通常由两部分组成：**多头自注意力机制（Multi-Head Self-Attention）** 和 **前馈神经网络（Feedforward Neural Network, FFN）**。其中，最耗时的部分通常是 **多头自注意力机制**，尤其是 **自注意力计算** 的步骤。

### 1. **多头自注意力机制（Multi-Head Self-Attention）**

自注意力机制（Self-Attention）是 Transformer 中的核心计算部分，尤其是在 **计算注意力权重（Attention Weights）** 时，消耗了大量计算资源。

#### 为什么多头自注意力最耗时？

- **点积计算：** 自注意力机制的核心计算是 **Query（Q）**、**Key（K）** 和 **Value（V）** 的点积操作。对于每个位置的 token，模型需要计算其与其他所有 token 的关系（即点积），这需要计算 **N x N** 的矩阵（其中 N 是序列长度）。计算这些点积操作的复杂度是 \( O(N^2 \cdot d) \)，其中 \( d \) 是每个 token 的维度。
  
- **矩阵操作：** 自注意力需要计算一个 **注意力矩阵**，然后根据注意力权重加权输入的 **Value** 向量。这个操作本质上是一个矩阵乘法，计算量较大。即使对于一个相对较短的序列，随着序列长度的增加，这个计算量是 **二次增长** 的。

- **多头机制：** Transformer 中的多头自注意力将注意力机制拆成多个头，每个头处理不同的子空间，这会增加计算量。假设有 \( h \) 个头，每个头的维度是 \( d_k \)，那么每个头的计算复杂度是 \( O(N^2 \cdot d_k) \)，总的计算量是 \( O(N^2 \cdot d_k \cdot h) \)。总的计算复杂度通常是 \( O(N^2 \cdot d) \)，其中 \( d \) 是每个头的维度和头数的总和。

- **并行计算的挑战：** 虽然 Transformer 可以并行计算，但每个 token 仍然需要和其他 token 进行 **全局交互**，这使得自注意力计算的 **并行化效率**受到一定限制，尤其是在长序列的情况下，计算开销较大。

### 2. **前馈神经网络（Feedforward Neural Network, FFN）**

- 在 Transformer 中，FFN 由两个线性变换和一个激活函数（通常是 ReLU）组成，计算量比多头自注意力小。
- 假设 FFN 的隐藏层大小是 \( d_{\text{ff}} \)，那么 FFN 的复杂度是 \( O(N \cdot d_{\text{model}} \cdot d_{\text{ff}}) \)，比自注意力的复杂度 \( O(N^2 \cdot d_{\text{model}}) \) 要小很多，尤其是当 \( N \) 较大时。

### 3. **总结**

- **最耗时的部分：** 在一个 Transformer block 中，最耗时的部分通常是 **多头自注意力机制**，尤其是 **自注意力计算**。计算 **注意力权重** 的复杂度是 **二次增长的（\( O(N^2) \)）**，这对于长序列来说是一个计算瓶颈。
- **前馈神经网络：** 相对而言，前馈神经网络的计算量要小很多，主要是由两个线性变换组成，复杂度为 **\( O(N \cdot d_{\text{model}} \cdot d_{\text{ff}}) \)**。

因此，在 Transformer 的每个 block 中，最耗时的部分通常是 **自注意力计算**，尤其是在序列长度较长时。  

在 Transformer 中，比较 **前馈神经网络（Feedforward Neural Network, FFN）** 和 **多头自注意力机制（Multi-Head Self-Attention）** 的 **参数量**，通常情况下， **FFN** 的参数量会 **比多头自注意力机制** 更大。以下是两者的详细比较：

### 1. **多头自注意力（Multi-Head Self-Attention）**

对于一个 Transformer block 中的多头自注意力机制，假设：
- 输入的嵌入维度为 \( d_{\text{model}} \)。
- 使用 \( h \) 个注意力头。
- 每个注意力头的维度为 \( d_k \)（通常，\( d_k = d_{\text{model}} / h \)）。

#### 参数量
多头自注意力的核心计算涉及 **Query（Q）**、**Key（K）** 和 **Value（V）** 的线性变换，它们是通过权重矩阵与输入的嵌入向量进行矩阵乘法得到的。
- 对于每个注意力头，生成 Q、K、V 所需的参数量是：
  \[
  \text{Parameters for each head} = 3 \times (d_{\text{model}} \times d_k)
  \]
  其中，Q、K、V 权重矩阵的大小都是 \( d_{\text{model}} \times d_k \)。
- 总的多头自注意力参数量：
  \[
  \text{Total Parameters for multi-head attention} = 3 \times d_{\text{model}} \times d_k \times h
  \]
  其中 \( h \) 是注意力头的数量。

最后，注意力计算后的结果还需要进行一个 **线性变换**（用于合并多头注意力的输出），因此还需要一个额外的权重矩阵：
\[
\text{Final Linear Transformation Parameters} = d_{\text{model}} \times d_{\text{model}}
\]

因此，多头自注意力机制的总参数量是：
\[
\text{Total Parameters for multi-head attention} = 3 \times d_{\text{model}} \times d_k \times h + d_{\text{model}}^2
\]
其中 \( d_k = d_{\text{model}} / h \)，所以可以简化为：
\[
\text{Total Parameters for multi-head attention} = 3 \times d_{\text{model}}^2 + d_{\text{model}}^2 = 4 \times d_{\text{model}}^2
\]
（假设 \( d_{\text{model}} \) 是一个能被 \( h \) 整除的数）

### 2. **前馈神经网络（Feedforward Neural Network, FFN）**

每个 Transformer block 中的 **前馈神经网络** 通常包括两个线性变换（一个升维和一个降维），并且通常会加入 **激活函数（如 ReLU）** 在中间。假设 FFN 的隐藏层大小为 \( d_{\text{ff}} \)，通常 \( d_{\text{ff}} \) 要比 \( d_{\text{model}} \) 大很多（例如，2048 与 512 或 1024）。

#### 参数量
- 输入到 FFN 的嵌入大小是 \( d_{\text{model}} \)。
- 第一个线性变换（升维）将输入从 \( d_{\text{model}} \) 转换到 \( d_{\text{ff}} \)，所需参数量是：
  \[
  \text{Parameters for first linear layer} = d_{\text{model}} \times d_{\text{ff}}
  \]
- 第二个线性变换（降维）将输出从 \( d_{\text{ff}} \) 转换回 \( d_{\text{model}} \)，所需参数量是：
  \[
  \text{Parameters for second linear layer} = d_{\text{ff}} \times d_{\text{model}}
  \]
- 因此，前馈神经网络的总参数量是：
  \[
  \text{Total Parameters for FFN} = d_{\text{model}} \times d_{\text{ff}} + d_{\text{ff}} \times d_{\text{model}}
  \]
  由于两个部分是对称的，因此可以简化为：
  \[
  \text{Total Parameters for FFN} = 2 \times d_{\text{model}} \times d_{\text{ff}}
  \]

### 3. **比较**

- **多头自注意力（Multi-Head Attention）** 的参数量是 \( 4 \times d_{\text{model}}^2 \)。
- **前馈神经网络（Feedforward Network, FFN）** 的参数量是 \( 2 \times d_{\text{model}} \times d_{\text{ff}} \)。

如果假设 \( d_{\text{ff}} \) 比 \( d_{\text{model}} \) 大很多（例如，2048 vs 512），那么 **FFN 的参数量会更大**。通常， \( d_{\text{ff}} \) 可能是 \( d_{\text{model}} \) 的 4 到 8 倍，甚至更大，这样 **FFN 的参数量** 将比 **多头自注意力的参数量** 更大。

### 举个例子：

假设 \( d_{\text{model}} = 512 \) 和 \( d_{\text{ff}} = 2048 \)：
- 多头自注意力的参数量：\( 4 \times 512^2 = 1,048,576 \)
- 前馈神经网络的参数量：\( 2 \times 512 \times 2048 = 2,097,152 \)

在这种情况下，**前馈神经网络的参数量会是多头自注意力的两倍**。

### 总结

- 如果 **\( d_{\text{ff}} \)（前馈网络的隐藏层维度）远大于 \( d_{\text{model}} \)**，那么 **FFN 的参数量通常会比多头自注意力的参数量多**。
- 在大多数标准配置中，前馈神经网络的参数量往往较大，尤其是在 \( d_{\text{ff}} \) 被设计为远大于 \( d_{\text{model}} \) 时。


11、[Transformer 使用 positionencoding 会影响输入 embedding 的原特征吗？](https://zhuanlan.zhihu.com/p/360144789/http%3Ci%3Es://www.zhihu.co%3C/i%3Em/question/350116316/answer/864616018)
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------

不会，只是一个加和过程


12、**Self-Attention 的时间复杂度是怎么计算的？**
-----------------------------------

Self-Attention 时间复杂度： $O(n^2 \cdot d)$O(n^2 \cdot d) ，这里，n 是序列的长度，d 是 embedding 的维度。

Self-Attention 包括**三个步骤：相似度计算，softmax 和加权平均**，它们分别的时间复杂度是：

**相似度计算可以看作大小为 (n,d) 和(d,n)的两个矩阵相乘**： $(n,d)*(d,n)=O(n^2 \cdot d)$(n,d)*(d,n)=O(n^2 \cdot d) ，得到一个 (n,n) 的矩阵

softmax 就是直接计算了，时间复杂度为 $O(n^2)$O(n^2)

加权平均可以看作大小为 (n,n) 和(n,d)的两个矩阵相乘： $(n,n)*(n,d)=O(n^2 \cdot d)$(n,n)*(n,d)=O(n^2 \cdot d) ，得到一个 (n,d) 的矩阵

因此，Self-Attention 的时间复杂度是 $O(n^2 \cdot d)$O(n^2 \cdot d) 。

这里再分析一下 Multi-Head Attention，它的作用类似于 CNN 中的多核。

多头的实现不是循环的计算每个头，而是通过 transposes and reshapes，用[矩阵乘法](https://zhida.zhihu.com/search?content_id=168161579&content_type=Article&match_order=1&q=%E7%9F%A9%E9%98%B5%E4%B9%98%E6%B3%95&zhida_source=entity)来完成的。

> In practice, the multi-headed attention are done with transposes and reshapes rather than actual separate tensors. —— 来自 google BERT 源码

Transformer/BERT 中把 **d ，**也就是 hidden_size/embedding_size 这个维度做了 reshape 拆分，可以去看 Google 的 TF [源码](https://link.zhihu.com/?target=https%3A//github.com/google-research/bert)或者上面的 pytorch 源码：

> hidden_size (d) = num_attention_heads (m) * attention_head_size (a)，也即 d=m*a

并将 num_attention_heads 维度 transpose 到前面，使得 Q 和 K 的维度都是 (m,n,a)，这里不考虑 batch 维度。

这样点积可以看作大小为 (m,n,a) 和(m,a,n)的两个张量相乘，得到一个 (m,n,n) 的矩阵，其实就相当于 (n,a) 和(a,n)的两个矩阵相乘，做了 m 次，时间复杂度（感谢评论区指出）是 $O(n^2 \cdot m \cdot a)=O(n^2 \cdot d)$O(n^2 \cdot m \cdot a)=O(n^2 \cdot d) 。

张量乘法时间复杂度分析参见：[矩阵、张量乘法的时间复杂度分析](https://link.zhihu.com/?target=https%3A//liwt31.github.io/2018/10/12/mul-complexity/)

因此 Multi-Head Attention 时间复杂度也是 $O(n^2 \cdot d)$O(n^2 \cdot d) ，复杂度相较单头并没有变化，主要还是 transposes and reshapes 的操作，相当于把一个大矩阵相乘变成了多个小矩阵的相乘。




13、简单介绍一下 Transformer 的位置编码？有什么意义和优缺点？
--------------------------------------

参见：ref="[https://z](https://link.zhihu.com/?target=https%3A//z)_huan_[http://lan.zhihu.com/p/106644634](http://lan.zhihu.com/p/106644634)"> 一文读懂 Transformer 模型的位置编码

14、你还了解哪些关于位置编码的技术，各自的优缺点是什么？
-----------------------------

参见：[如何优雅地编码文本中的位置信息？三种 positionalencoding 方法简述](https://link.zhihu.com/?target=https%3A//zhuanlan.zh%253Ci%253Eihu.com/%253C/i%253Ep/121126531)

[让研究人员绞尽脑汁的 Transformer 位置编码](https://zhuanlan.zhihu.com/p/360144789/h%3Ci%3Ettps://zhuanlan%3C/i%3E.zhihu.com/p/352898810)

15、简单讲一下 Transformer 中的残差结构以及意义。
--------------------------------

防止梯度消失，帮助深层网络训练


16、简答讲一下 BatchNorm 技术，以及它的优缺点。
------------------------------

参见：[https://zhuanlan.zhihu.com/p/153183322](https://zhuanlan.zhihu.com/p/153183322)  
BN 的理解重点在于它是针对整个 Batch 中的样本在同一维度特征在做处理。  
在 MLP 中，比如我们有 10 行 5 列数据。5 列代表特征，10 行代表 10 个样本。是对第一个特征这一列（对应 10 个样本）做一次处理，第二个特征（同样是一列）做一次处理，依次类推。  
在 CNN 中扩展，我们的数据是 N·C·H·W。其中 N 为样本数量也就是 batch_size，C 为通道数，H 为高，W 为宽，BN 保留 C 通道数，在 N,H,W 上做操作。比如说把第一个样本的第一个通道的数据，第二个样本第一个通道的数据..... 第 N 个样本第一个通道的数据作为原始数据，处理得到相应的均值和方差。  
**BN 有两个优点。**  
第一个就是可以解决内部[协变量](https://zhida.zhihu.com/search?content_id=168161579&content_type=Article&match_order=1&q=%E5%8D%8F%E5%8F%98%E9%87%8F&zhida_source=entity)偏移，简单来说训练过程中，各层分布不同，增大了学习难度，BN 缓解了这个问题。当然后来也有论文证明 BN 有作用和这个没关系，而是可以使损失平面更加的平滑，从而加快的[收敛速度](https://zhida.zhihu.com/search?content_id=168161579&content_type=Article&match_order=1&q=%E6%94%B6%E6%95%9B%E9%80%9F%E5%BA%A6&zhida_source=entity)。  
第二个优点就是缓解了梯度饱和问题（如果使用 sigmoid [激活函数](https://zhida.zhihu.com/search?content_id=168161579&content_type=Article&match_order=1&q=%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0&zhida_source=entity)的话），加快收敛。  
**BN 的缺点：**  
第一个，batch_size 较小的时候，效果差。这一点很容易理解。BN 的过程，使用 整个 batch 中样本的均值和方差来模拟全部数据的均值和方差，在 batch_size 较小的时候，效果肯定不好。  
第二个缺点就是 BN 在 RNN 中效果比较差。这一点和第一点原因很类似，不过我单挑出来说。  
首先我们要意识到一点，就是 RNN 的输入是长度是动态的，就是说每个样本的长度是不一样的。  
举个最简单的例子，比如 batch_size 为 10，也就是我有 10 个样本，其中 9 个样本长度为 5，第 10 个样本长度为 20。  
那么问题来了，前五个单词的均值和方差都可以在这个 batch 中求出来从而模型真实均值和方差。但是第 6 个单词到底 20 个单词怎么办？  
只用这一个样本进行模型的话，不就是回到了第一点，batch 太小，导致效果很差。  
第三个缺点就是在测试阶段的问题，分三部分说。  
首先测试的时候，我们可以在队列里拉一个 batch 进去进行计算，但是也有情况是来一个必须尽快出来一个，也就是 batch 为 1，这个时候均值和方差怎么办？  
这个一般是在训练的时候就把均值和方差保存下来，测试的时候直接用就可以。那么选取效果好的均值和方差就是个问题。  
其次在测试的时候，遇到一个样本长度为 1000 的样本，在训练的时候最大长度为 600，那么后面 400 个单词的均值和方差在训练数据没碰到过，这个时候怎么办？  
这个问题我们一般是在数据处理的时候就会做截断。  
还有一个问题就是就是[训练集](https://zhida.zhihu.com/search?content_id=168161579&content_type=Article&match_order=1&q=%E8%AE%AD%E7%BB%83%E9%9B%86&zhida_source=entity)和测试集的均值和方差相差比较大，那么训练集的均值和方差就不能很好的反应你测试数据特性，效果就回差。这个时候就和你的数据处理有关系了。

17、简单描述一下 Transformer 中的[前馈神经网络](https://zhida.zhihu.com/search?content_id=168161579&content_type=Article&match_order=1&q=%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&zhida_source=entity)？使用了什么激活函数？相关优缺点？
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
我觉得这是transformer很优雅的一个设计：注意力机制专注于在token这个层级来优化权重，让token之间建立起来丰富的联系，解决了序列中的长短程依赖问题；而FFN专注于在特征这个层次来优化权重，让不同的特征之间相互融合，丰富局部的表现力，两者相辅相成，各自独立又互相配合。


前馈神经网络采用了两个[线性变换](https://zhida.zhihu.com/search?content_id=168161579&content_type=Article&match_order=1&q=%E7%BA%BF%E6%80%A7%E5%8F%98%E6%8D%A2&zhida_source=entity)，激活函数为 Relu，公式如下：

$FFN(x) = max(0, xW_1 + b_1) W_2 + b_2$ FFN(x) = max(0, xW_1 + b_1) W_2 + b_2

优点：

SGD 算法的收敛速度比 sigmoid 和 tanh 快；（梯度不会饱和，解决了[梯度消失问题](https://zhida.zhihu.com/search?content_id=168161579&content_type=Article&match_order=1&q=%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E9%97%AE%E9%A2%98&zhida_source=entity)）

计算复杂度低，不需要进行指数运算；

适合用于后向传播。

缺点：

ReLU 的输出不是 zero-centered；

ReLU 在训练的时候很” 脆弱”，一不小心有可能导致神经元” 坏死”。举个例子：由于 ReLU 在 x<0 时梯度为 0，这样就导致负的梯度在这个 ReLU 被置零，而且这个神经元有可能再也不会被任何数据激活。如果这个情况发生了，那么这个神经元之后的梯度就永远是 0 了，也就是 ReLU 神经元坏死了，不再对任何数据有所响应。实际操作中，如果你的 learning rate 很大，那么很有可能你网络中的 40% 的神经元都坏死了。 当然，如果你设置了一个合适的较小的 learning rate，这个问题发生的情况其实也不会太频繁。，Dead ReLU Problem（神经元坏死现象）：某些神经元可能永远不会被激活，导致相应参数永远不会被更新（在负数部分，梯度为 0）。产生这种现象的两个原因：参数初始化问题；learning rate 太高导致在训练过程中参数更新太大。 解决方法：采用 Xavier 初始化方法，以及避免将 learning rate 设置太大或使用 adagrad 等自动调节 learning rate 的算法。

ReLU 不会对数据做幅度压缩，所以数据的幅度会随着模型层数的增加不断扩张。





18、Transformer的并行体现在哪个地方？Decoder端可以做并行化吗？
------------------------------------------------------

RNN的每一个样本（一个句子）内部必须一步一步地计算完成，而CNN和Self-Attention则可以完全并行计算。这里说的不能并行指的是每一个样本内部（模型并行），不同样本彼此之间是可以进行并行计算的（数据并行）。  

---

### **1. 训练阶段的并行性**
- **Transformer的并行性**：
  - **自注意力机制**：可同时计算序列中所有位置之间的关联，通过矩阵运算（如Q、K、V的并行计算）一次性处理整个输入序列，无需像RNN那样逐步处理。
  - **前馈网络（FFN）**：可并行处理所有位置的输出。
  - **批处理（数据并行）**：同时处理多个样本（批量输入），利用GPU的并行计算能力加速训练。
  
- **GPT的并行性**：
  - **掩码自注意力**：训练时通过掩码隐藏未来位置信息，但所有位置的注意力计算仍可并行完成（通过矩阵掩码实现，teacher forcing）。
  - **大规模批处理**：与Transformer类似，通过同时处理多个文本序列提升训练效率。

---

### **2. 推理阶段的并行性**
- **Transformer（仅编码器）**：
  - 输入序列已知，可一次性并行计算所有位置的输出（如机器翻译中的编码器）。

- **GPT（自回归模型）**：
  - **单序列推理**：生成文本时需逐个预测下一个Token（自回归），序列内部无法并行。
  - **批处理优化**：同时处理多个独立序列（如同时响应多个用户请求），利用GPU并行计算加速。
  - **KV缓存**：缓存历史Token的Key-Value向量，减少重复计算，间接提升效率。



19、简单描述一下 wordpiece model 和 byte pair encoding，有实际应用过吗？
-------------------------------------------------------

参见：[深入理解 NLP Subword 算法：BPE、WordPiece、ULM](https://link.zhihu.com/?target=https%3A//zhuan%253Ci%253Elan.zhihu%253C/i%253E.com/p/86965595)

20、Transformer 训练的时候学习率是如何设定的？Dropout 是如何设定的，位置在哪里？Dropout 在测试的需要有什么需要注意的吗？
---------------------------------------------------------------------------

21、[有哪些令你印象深刻的魔改 transformer？](https://zhuanlan.zhihu.com/p/360144789/htt%3Ci%3Eps://www.zh%3C/i%3Eihu.com/question/349958732/answer/945349902)
-----------------------------------------------------------------------------------------------------------------------------------------------
22、transformer是否可以引入预训练的词向量？
--------------
可以但不一定有效果，不一定有直接随机初始化而训练的好  


23、kv cache。为什么没有 Q cache？
--------------
---
为什么加速LLM推断有KV Cache而没有Q Cache？
https://www.zhihu.com/question/653658936/answer/3545520807

无需区分推理还是训练，从原理上就可以看出，无论推理的时候，还是训练的时候，都可以使用KV-Cache；
也无需区分是否输入是一个词还是多个词，都一样；
### **KV Cache 的简介：从来源到原理与应用**

#### **1. 来源：解决自回归生成的计算瓶颈**
在 Transformer 模型的自回归生成任务（如文本生成、翻译）中，模型需要**逐个生成 token**，每个新 token 的预测依赖于之前所有 token 的上下文。  
- **原始问题**：每次生成新 token 时，需重新计算所有历史 token 的 Key（K）和 Value（V）向量，导致计算复杂度为 \(O(n^2)\)（\(n\) 为序列长度），效率极低。  
- **核心优化目标**：避免重复计算历史 token 的 K 和 V，从而降低计算量。  

KV Cache（Key-Value 缓存）应运而生，通过缓存历史 token 的 K 和 V，实现生成效率的显著提升。

---

#### **2. 原理：复用历史计算的 K 和 V**
##### **(1) 自注意力机制回顾**
在 Transformer 的自注意力中，每个 token 会生成三个向量：  
- **Query（Q）**：表示当前 token 的“需求”。  
- **Key（K）**：表示当前 token 的“身份标识”。  
- **Value（V）**：表示当前 token 的“内容信息”。  

注意力权重通过 Q 与所有 K 的点积计算，再对 V 加权求和得到输出。

##### **(2) KV Cache 的工作流程**
1. **首次生成**：  
   - 输入序列为 \([x_1]\)，计算 \(K_1, V_1\)，并缓存。  
   - 预测下一个 token \(x_2\)。  

2. **后续生成**：  
   - 输入序列变为 \([x_1, x_2]\)，但只需计算 \(x_2\) 的 \(Q_2\)，复用缓存的 \(K_1, V_1\)。  
   - 计算注意力权重：\(Q_2 \cdot [K_1, K_2]\)，再与 \([V_1, V_2]\) 加权求和。  
   - 缓存新增的 \(K_2, V_2\)，用于后续步骤。  

3. **重复迭代**：  
   - 每次生成新 token 时，只需计算当前 token 的 Q，并复用缓存的 K 和 V。  

**复杂度变化**：  
- **无 KV Cache**：计算复杂度为 \(O(n^2)\)。  
- **有 KV Cache**：复杂度降为 \(O(n)\)（仅需计算当前 Q 与历史 K 的点积）。  

---

#### **3. 何时发挥作用？**
KV Cache 主要在以下场景中发挥关键作用：

##### **(1) 推理阶段的自回归生成**
- **文本生成**：如 GPT 生成文章、对话。  
- **机器翻译**：逐词生成目标语言序列。  
- **代码补全**：逐个 token 预测后续代码。  

##### **(2) 长序列生成**
- **显著加速**：序列越长，KV Cache 的收益越大（避免重复计算历史 token）。  

##### **(3) 实时应用**
- **低延迟需求**：如聊天机器人、实时翻译，需快速响应每个生成步骤。  

---

#### **4. 优势与权衡**
| **优势**                     | **权衡**                     |
|------------------------------|------------------------------|
| **计算效率高**：复杂度从 \(O(n^2)\) 降至 \(O(n)\) | **内存占用增加**：需缓存所有历史 K 和 V（内存随序列长度线性增长） |
| **减少重复计算**：避免冗余的 K/V 计算   | **实现复杂度**：需管理缓存的读写与更新逻辑   |
| **加速长序列生成**：适合生成文章、长文本 | **动态长度限制**：超出缓存容量时需截断或重新计算 |

---

#### **5. 实际应用示例**
假设生成句子 **“人工智能改变世界”**：  
1. **生成“人”**：计算并缓存 \(K_{\text{人}}, V_{\text{人}}\)。  
2. **生成“工”**：复用 \(K_{\text{人}}, V_{\text{人}}\)，仅计算 \(Q_{\text{工}}\) 和新 token 的 \(K_{\text{工}}, V_{\text{工}}\)，更新缓存。  
3. **后续步骤**：依此类推，每次仅需计算当前 token 的 Q 和新 K/V，其余复用缓存。  

---

### **总结**
- **KV Cache 的本质**：通过缓存历史 token 的 Key 和 Value 向量，避免自回归生成中的重复计算。  
- **核心价值**：大幅降低计算复杂度，提升长文本生成效率。  
- **适用场景**：所有基于 Transformer 的自回归生成任务（如 GPT、T5、BART）。  

通过平衡内存与计算效率，KV Cache 成为现代大模型推理优化的关键技术之一。