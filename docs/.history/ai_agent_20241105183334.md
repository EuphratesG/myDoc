# RAG 和使用工具在智能体（AI agents）解决大模型幻觉（hallucination）方面的异同

## RAG（Retrieval-Augmented Generation）

### 基本概念：
RAG结合了检索和生成两种方法。首先从外部知识库中检索相关信息，然后用这些信息作为上下文生成回答。

### 优点：
- **增强信息准确性**：通过检索外部信息，模型可以获取最新和更准确的数据，从而减少生成内容中的错误或虚构信息。
- **上下文丰富**：提供更多的背景信息，有助于生成更符合实际的响应。

### 适用场景：
- 适合需要大量事实支持的任务，如问答系统和信息检索。

## 使用工具

### 基本概念：
使用工具是指智能体在执行任务时调用外部工具或API（如数据库查询、计算器、网络搜索等）来获取所需信息。

### 优点：
- **实时数据访问**：可以直接从工具中获取实时信息，确保信息的时效性和准确性。
- **灵活性**：能够执行复杂的操作，如数据处理、计算或图形生成，直接响应用户请求。

### 适用场景：
- 适合需要动态响应或处理具体任务的场景，如编程助手、数据分析等。

## 异同点

### 共同点：
- 两者都旨在提高生成内容的准确性，减少幻觉。
- 都依赖外部信息源来增强模型的能力。

### 不同点：
- **信息获取方式**：RAG依赖于静态知识库的检索，而使用工具则可以实时访问动态数据或功能。
- **复杂性**：RAG相对简单，主要依赖检索；使用工具可以涉及更复杂的操作和交互。
- **场景适应性**：RAG更适合知识密集型任务，而使用工具则更灵活，适应各种实际需求。

总的来说，两者各有优势，结合使用时可以进一步提升智能体的性能，减少生成幻觉的可能性。

---

# 大语言模型与LLM的未来

我很看好LLM，大部分公司是想让LLM做100%的事情，这是不可能的。LLM只能解决80%-90%的事情，剩下的10%-20%如果想要靠LLM去解决，需要花上几倍甚至几十倍的代价。起码未来几年，产品应该还是LLM为底，业务为主体，剩下的人工去解决。

---

# 机器学习常规操作套路

作为小白，想学习/了解/深入机器学习常规操作套路，那么很好，Kaggle公开讨论区有很多大佬公开的自己的脚本，可以在代码实践中熟悉这些处理步骤，常规线路一般是：
1. 数据清洗
2. 特征挖掘/特征工程
3. 搭建模型
4. 训练模型
5. 给出预测
6. 提交结果

---

# NLP的发展

首先是用神经网络对文本序列的生成建模，例如RNN，基础的RNN其实是输入一个词。假设你输入的种子词是“春天”：
1. 输入词向量（“春天”）到模型。
2. 模型输出一个词的概率分布，可能选择“来了”。
3. 将“来了”作为下一个输入，再次输入模型。
4. 模型继续输出下一个词，例如“，”，然后重复此过程，直到生成完整的句子。
在此之上可能有多个种子词。

## word2vec
- **word2vec** 是无监督学习，不需要标注。有监督学习还是很强的限制条件，必须要有标签标注。
- 预训练语言模型，都是为了获得embedding，提取特征用的。
- 神经语言模型专注于学习任务无关的语义表征，旨在减少人类特征工程的工作量，可以大范围扩展语言模型可应用的任务。进一步，预训练语言模型加强了语义表征的上下文感知能力，并且可以通过下游任务进行微调，能够有效提升下游任务（主要局限于自然语言处理任务）的性能。

---

## Transformer
- 基础目标仅仅是机器翻译，训练目标也是翻译得准，大量文本无监督拿来训练。

## BERT
**BERT（Bidirectional Encoder Representations from Transformers）** 从随机初始化开始的训练目标主要有两个：
- **掩蔽语言模型（Masked Language Model, MLM）**：在输入句子中随机掩蔽一些单词，模型需要预测被掩蔽的单词。
- **下一个句子预测（Next Sentence Prediction, NSP）**：给定一对句子，模型需要判断第二个句子是否是第一个句子的下一个句子。

### 训练集数据的例子
- **例子 1（MLM）**：
  - 输入：The cat sat on the [MASK].
  - 目标：mat
  - 整个上下文：The cat sat on the mat.
- **例子 2（MLM）**：
  - 输入：Artificial [MASK] is fascinating.
  - 目标：intelligence
  - 整个上下文：Artificial intelligence is fascinating.
- **例子 3（NSP）**：
  - 输入句子对：
    - 句子 1：The sky is blue.
    - 句子 2：It is a beautiful day.
  - 目标：否（这两个句子没有直接关系）
- **例子 4（NSP）**：
  - 输入句子对：
    - 句子 1：The cat is sleeping.
    - 句子 2：It is dreaming.
  - 目标：是（这两个句子有逻辑上的连续性）

## GPT-1
- 与 BERT 模型不同的是，GPT-1 采用了仅有解码器的 Transformer 架构，以及基于下一个词元预测的预训练任务进行模型的训练。

### 例子：
- **例子 1**：
  - 输入：The quick brown fox
  - 目标：jumps
  - 整个上下文：The quick brown fox jumps
- **例子 2**：
  - 输入：In the year 2020, the world faced
  - 目标：a pandemic
  - 整个上下文：In the year 2020, the world faced a pandemic

---

# 第二章：大语言模型的构建过程

大语言模型的构建过程、扩展法则（Scaling Law）、涌现能力（Emergent Abilities），然后将介绍 GPT 系列模型的研发历程。

- 神经网络是一种具有特定模型结构的函数形式，而大语言模型则是一种基于 Transformer 结构的神经网络模型。
- 大语言模型的优化目标更加泛化，不仅仅是为了解决某一种或者某一类特定任务，而是希望能够作为通用任务的求解器。

一般来说，这个训练过程可以分为 **大规模预训练** 和 **指令微调与人类对齐** 两个阶段。

---

## 指令微调（Supervised Fine-Tuning，SFT）

SFT 是一种基于监督学习的微调方法，通常用于提升预训练语言模型在特定任务上的性能。

### SFT 的主要特点和工作流程：
1. **预训练模型**：SFT 通常基于一个已经过大量无监督数据预训练的基础模型，比如 GPT 或 BERT。
2. **有监督标注数据**：使用带有输入输出对的标注数据。
3. **微调训练**：通过监督学习进行训练，调整模型参数以最小化预测输出与真实标签之间的差距。
4. **任务适应性**：经过 SFT 之后，模型能够对目标任务产生更准确和一致的输出，表现为在测试数据上的精度提升。

### 举例说明：
假设我们有一个预训练的语言模型，想微调它以适应情感分析任务：

- **数据**：有一组情感标注数据，例如：
  - 输入：`The movie was fantastic!`
  - 标签：`Positive`
  
- **训练**：模型会使用这些带标签的数据进行训练，调整参数以更好地预测情感标签。
  
- **结果**：微调后，模型在看到类似的文本时能够准确判断情感倾向。

### SFT 的优点：
- **适应性强**：使预训练模型能够高效应用于特定任务。
- **性能提升**：通过标注数据的指导，提高模型在特定任务上的准确率。

---

## 人工对齐
- RLHF、或者其他 SFT 代替强化学习。

---

# 第五章 模型结构

现有大模型多为因果大模型（还有前缀补全、编码解码）。GPT 模型主要采纳了 **解码器架构 + 预测下一个词** 的技术路径。

### 位置编码
绝对位置编码能够一定程度上建模位置信息，但它




## 第五章 模型结构 前面的基本可以pass，不然就用gpt递归补充
现有大模型多为因果大模型（还有前缀补全、编码解码）。且因为gpt的成功多只有decoder  
相比rnn、lstm的一大进步是方便gpu并行，另外才是长文本序列关系和多头自注意力  
### 位置编码
尽管绝对位置编码能够一定程度上建模位置信息，然
而它只能局限于建模训练样本中出现的位置，无法建模训练数据中未出现过的位
置，因此极大地限制了它们处理长文本的能力。我们将会在第 5.2.4 节深入讨论不
同的位置编码方式以及在第 5.4 节讨论长文本建模方法。

### 多头子注意力机制
多头自注意力是 Transformer 模型的核心创新技术。相比于循环神经网络（Recurrent Neural Network, RNN）和卷积神经网络（Convolutional Neural Network, CNN）
等传统神经网络，多头自注意力机制能够直接建模任意距离的词元之间的交互关
系。作为对比，循环神经网络迭代地利用前一个时刻的状态更新当前时刻的状态，
因此在处理较长序列的时候，常常会出现梯度爆炸或者梯度消失的问题。而在卷
积神经网络中，只有位于同一个卷积核的窗口中的词元可以直接进行交互，通过
堆叠层数来实现远距离词元间信息的交换

在卷积神经网络（CNN）中，堆叠层数可以用来增强模型捕捉长距离依赖关系的能力。以下是一个简单的例子来说明如何通过堆叠层数实现远距离词元间的信息交换。

### 示例

假设我们有一个句子：“The cat sat on the mat.”，我们希望通过卷积神经网络来分析这个句子。

1. **输入层**：
   - 将句子转换为词向量表示，比如每个词被表示为一个向量：
     - `The`: [0.1, 0.2, 0.3]
     - `cat`: [0.4, 0.5, 0.6]
     - `sat`: [0.7, 0.8, 0.9]
     - `on`: [0.1, 0.1, 0.1]
     - `the`: [0.2, 0.2, 0.2]
     - `mat`: [0.3, 0.3, 0.3]

2. **第一层卷积**：
   - 使用一个卷积核（比如大小为 2 的窗口）进行卷积操作。
   - 这个卷积核可以处理相邻的词元，比如 `(The, cat)`, `(cat, sat)`, `(sat, on)`, 等等。
   - 输出特征图将捕捉到相邻词元间的信息。

3. **第二层卷积**：
   - 将第一层的输出特征图输入到第二层卷积。
   - 此时，卷积核可以捕捉到更远的词元之间的关系，例如通过 `(cat, sat)` 和 `(sat, on)` 组合来连接 `cat` 和 `on`。

4. **堆叠更多层**：
   - 如果继续堆叠多层卷积，模型将能够通过更多的组合和层次结构，捕捉到更远距离词元之间的依赖关系。
   - 比如，最后一层的特征可能会融合 `cat` 和 `mat` 之间的信息。

### 总结

通过堆叠卷积层，卷积神经网络能够有效地聚合来自不同距离词元的信息，尽管在每一层中，卷积核只能直接交互在同一窗口中的词元。这种结构使得 CNN 能够逐层提取和组合特征，从而在全局上下文中理解输入数据。


transformer其实是先想出qkv的构想，然后随机初始化，一切交给训练。多头的每个头也是独立初始化的，虽然也是随机初始化，但是能够从不同角度看输入文本的特征（不同角度理解embedding）

需要再了解一下层归一化和残差连接  

为什么decoder的Q可以和