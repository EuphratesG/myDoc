# 动手学深度学习
![](https://cdn.jsdelivr.net/gh/EuphratesG/myPic@main/202311061034697.png)
![](https://cdn.jsdelivr.net/gh/EuphratesG/myPic@main/202311061100337.png)  
多层感知机分类的时候如何判断哪一个特征对分类结果影响更大。首先在MLP比较简单的时候, 你可以去检查对某个特征的权重大小。在神经网络比较复杂的时候(包括更复杂的模型), 人们把模型视为一个黑盒而去考虑具体特征对输出有什么贡献, 这一问题对应于深度学习的可解释性这一领域, 具体比较简单的方法有’leave one out’, ‘shap’ 等, 具体可以查看相关资料, 这里简单介绍leave one out的想法
把每个特征一个一个地mask掉, 看输出的改变, 对于mask后导致输出改变更大的特征, 认为它的贡献比较大  
通常来说，权重会初始化为较小的随机值，比如乘以0.01，有助于避免初始值过大或过小导致的梯度爆炸或梯度消失问题。偏置则通常初始化为零值。在神经网络中，权重和偏置通常被设置为requires_grad=True，以便在训练过程中能够对它们进行优化。

0轴求和保留一行，1轴求和保留一列   
交叉熵损失函数。 这可能是深度学习中最常见的损失函数，因为目前分类问题的数量远远超过回归问题的数量。  

概率密度函数就是参数和样本x的多元函数  f(θ,X)二元函数，给定θ时，f(θ,X)描述产生样本X的可能性/概率(在很小的区间里的积分)；给定X时，f(θ,X)描述在X下，分布参数是θ的可能性。 其实就是描述可能性，一个是θ参数的可能性，一个是产生样本X的可能性。  
所谓的似然就是给定X后f的值（因为这时候描述的是θ的可能性，让其最大的θ就是最可能出现的参数值）当前样本来自什么参数的分布的可能性最大。  


平方损失函数就是对theta的极大似然估计
平方损失函数是定义出来的，所谓假设误差服从正态分布仅仅说明了为什么可以这样定义。  
softmax输出是一个向量（对每个样本而言），是各个类的概率分布，训练目标是让该yhat接近分类目标的onehot向量（样本自带的标签）。  
而这样的话我们就只需要保证分类目标的概率最大就行了，再用平方损失函数的话要求就太高了，因此我们引入交叉熵损失函数。当然也要对样本数做平均。本质上就是只算分类目标那个类的-log而已。  


负采样这一技巧其实在别的算法中也有应用，其适用的场景是结果中正例比较少而负例比较多的情形，就比如word2vec，无论是CBOW模式还是skip-gram模型，整个词库词语数量为V，CBOW预测中间词，输出中只有中间词为1，其余词都为0;skip-gram预测周围词，只有周围有限的n个词为1，其余都为0，这两种模式下都是正例数很少，而负例数很多的情形。正是基于这样一种实际情况，负采样的思想就是只取所有正样本和部分负样本，比如按照1比10的比例选取负样本，然后整个预测输出的规模就变小了，这样再去计算softmax的时候计算量就很小了。（预测输出的概率分布中不会有那么多及其接近0的值了）  
3.4.6.1对数似然里面会讲，这**里要反复理解**  

GPU 0: Tesla V100-SXM2-32GB
GPU 1: Tesla V100-SXM2-32GB
GPU 2: Tesla V100-SXM2-32GB
GPU 3: Tesla V100-SXM2-32GB  
搞明白所谓的device号是怎么回事，fede里通过gpu参数来控制使用哪个  
再看看所谓collection是指什么  
改了一下max epoch  
进程stopped之后并不会死掉  

collection就是和isolation相对的不联邦在一起训练。  
transE是completion里面的，fede用它来测试embedding的效果  
