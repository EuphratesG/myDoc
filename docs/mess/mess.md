# mess



很有意思，修改注册表让右键出现装在conda\pkgs的git bash here  
https://blog.csdn.net/qq_41559271/article/details/115800774  

项目的license指的是将原项目商用需要遵守的，mit比较轻只要保留原作者版权信息就可以无限使用


mkdocs作用
在git平台创建mkdocs主题仓库，自动将markdown文件生成静态网页。 


也就是说，可以直接本地一条命令更新网页部署，只不过需要清楚浏览器缓存才能显示或者正常中文检索。  
可以写个脚本定期将项目源docs推到github上保证云版本

这样我们每次只需要在本地更新内容之后，通过该命令进行提交、部署，就可以实现文档内容的更新。
注：由于 Github Actions 运作需要一定时间（大约在几分钟到十分钟不等），因此部署完成后你需要等待一会儿才能看到 Github Pages 生效，否则会出现 404 错误。


环境变量其实就是让任何命令行都能直接找可执行程序的一个列表，不用cd到特定bin目录之下就可以执行内部或外部命令、可运行的程序或批处理文件.系统变量就是都可以用的，用户变量就是那个用户可以用的。






公私钥属于非对称加密，对称加密指加密解密都用一个钥匙，但是这样钥匙容易被截取，因此通常用非对称加密去加密钥匙。  
每个人都有自己的公钥和私钥，但私钥只有自己知道内容。  
公钥加密私钥解密别人只知道锁没有钥匙，对我自己的隐私数据加密。    
私钥加密公钥解密，把公钥公开给服务器，这样要远程连接的时候服务器给客户端发一个字符串，客户端用私钥加密后只有用之前保存的公钥解密才能还原出原来的内容。因此私钥加密相当于身份验证代替用户名密码。  


负采样的思想是通过仅考虑少量的负样本来近似全局损失，从而提高了训练效率，尤其是在大规模词汇表的情况下。  





把注意力集中到数据的理解、清洗、预处理、人肉特征、业务应用（而这些往往和屌丝、苦逼等形容词联系在一起）上来  
背景、目标、拟研究问题、假设、研究方法、预期结果、时间安排、预期调整、参考文献等部分的言之有物、逻辑清晰的 proposal，那么恭喜你，考虑继续读个博吧  
软件不是目的，而是手段。人的需求才是目的。  


路由器还能ssh登录？可以看一看  








main函数训练的时候构建的是trainer对象，trainer内部会构建KGEModel对象。 
打分函数就在KGEModel那几个函数里面啊，我直接换成LLM不就行了。  
LLaMa 的网络基于 Transformer 架构，在此基础上做出一些改进。  

1/8  
预训练模型的最终目标和输出取决于在微调阶段应用它的具体任务。预训练的语言模型，比如BERT、GPT等，在预训练阶段通常执行自监督任务，比如掩码语言建模（masked language modeling）。在微调阶段，这个预训练模型被用于特定的下游任务，比如文本分类、命名实体识别、机器翻译等。
文本分类：模型的输出可能是一组类别标签的概率分布，用于对文本进行分类。  
命名实体识别：输出可能是对文本中实体的边界和类型的预测。  
机器翻译：输出可能是对源语言句子的翻译或目标语言句子的生成。  
掩码语言建模输入带掩码的句子（词标号序列）和被masked的词作为自监督的标签，训练模型去预测这个词，模型参数就是各个词的embedding。  
自回归模型其实类似于n-gram例子，就是预测下一个出现词的概率，且下一个出现词的概率只和前面若干个词有关。  

TransE的损失函数理解损失函数是使用了负抽样的max-margin函数。L(y, y’) = max(0, margin - y + y’)y是正样本的得分，y'是负样本的得分。然后使损失函数值最小化，当这两个分数之间的差距大于margin的时候就可以了(我们会设置这个值，通常是1)。优化目标是往0去优化。这里的d是特征向量空间中h+l和t要很接近，负样本不接近，那么就会产生一个负值和γ相加，**属于自监督学习。**    
![](https://cdn.jsdelivr.net/gh/EuphratesG/myPic@main/202401081126322.png)

服务器没法翻墙怎么办，直接搞了个镜像网站export HF_ENDPOINT=https://hf-mirror.com  


目前找到了用NN的打分函数，torch环境没配好，明天做做ppt，看看llama，看看树的加速  



huggingface的pipeline可以选择各种任务  
llama是一个decoder-only的结构  
tokenizer方法是可以把tokenID转化成pytorch张量的，就是token序号的张量    


可以把多层感知机能够拟合异或的原因理解成低维空间线性不可分那么高维空间线性就可分了，是支持向量机核函数的思想。  
![](https://cdn.jsdelivr.net/gh/EuphratesG/myPic@main/202401181602663.png)


1/11   

fede实际上就是先进行t个round，每个round随机在所有client里面选一部分做联邦。全部收敛之后再对每个用户model-fusion权衡本地embedding（isolation）和联邦得来的embedding的打分。和collection的对比中FEDE只是部分占优势，但是相比其优点在于保证了信息的隐私（三元组不被传播至传输embedding）    

pylance速度很慢的原因原来是workspace文件太多卡住了，重新按小文件夹打开就好了正常显示补全了。  


看了llamamodel的pytorch结构发现原来是可以直接输入embedding的，我真佛了，还是要看模型结构啊  

最新成果，首先可以把embedding当输入  
贝叶斯决策就是在不完全情报下，对部分未知的状态用主观概率估计，然后用贝叶斯公式对发生概率进行修正，最后再利用期望值和修正概率做出最优决策。**发生证据的条件下对某个全概率事件的可信度。**  
至于条件概率是很自然的要乘条件时间的概率再乘条件下的概率的  


1/15最新成果，发现了输出的logits是torch.Size([1, 13, 32000])，第一个是批batch_size因为输入就一句话，第二个是根据句子截出来的时间步，第三个是线性层输出。得到了一个打分函数的值，下一步就是把输出改成embedding跑fede，这个要琢磨琢磨。llama原本embedding层是32000、4096，所以估计用自己的embedding输入的时候就用4096维的向量就好。（但是fede里面是256维的，这个思考一下）  
fede先是用了两个评估知识图谱的数据集，又针对联邦的evaluate搞了新的数据集。  
联邦的过程是先随机分给用户relation，再去匹配对应的实体。把前述两个数据集分别分给3510个client和3个client，训练集、验证集、测试集8：1:1.  

model_fusion是和fede单搞出来的，我们拿来做改进的话可以不用改。  

把dataloader直接赋值给三个变量的操作是因为这样做调用了dataloader所对应数据集的getitem方法的返回值  


1/19周五最新任务：  
问题一 4096，这个我直接全sum了打分函数恐怕不妥吧  
肯定不妥啊，现在是要把任务文本描述告诉llm，然后再在后面输入embedding（这个不知道要怎么具体实现，先看着吧）√  
问题二 现在所谓把打分函数换了其实就是在训练embedding，那用llm给embedding添加额外信息还要实体相关的描述语料。要不要弄，要的话语料在哪得到 还可以问
问题三 知识图谱本身embedding是默认256（虽然提供了参数改成4096）会不会受4096的影响维度夸大太多  不会有什么大问题 √  
问题四，把score function换成llm，会有head relation，tail之分。在llm输入端如何区分三者（输入端输入的直接是embedding矩阵）目前是直接cat在一起扔进去。 同问题一√  
任务是什么，我要给你

prompt和embedding都用llama的
文本直接写好 第一行实体，第二行关系，你给我



问题五 爆显存问题
加载模型就爆显存，是why 尝试分布式解决√  

问题六 llama模型输入的那个embedding具体的作用是？我既然给llama prompt了那么我还需要在那个接口输入embedding吗？那个接口有点像是跟token表强相关的了。可能有点问题，但转念一想输入的embedding也应该即使token提取的特征。。。思考一下吧    

chatgpt也可以让他返回打分函数

返回的还是一个text，可以自己操作成一个值

直接输入prompt描述好问题我接下来要给你一些embedding你要输出一个数值xxx
llama要decode一下，最后的输出如果有一个01概率其实是文本text，直接给他int以下就行
其实就类似chatgpt



**不是fine-tune**因为我们没有重新训练llama，只是处理一堆embedding而已。  

那我们就还是把三种实体拼接成一坨扔进去，只是用prompt做出解释，前面部分是实体啊，后面是关系啊xxx。  
我感觉应该不用告诉他什么是正例什么是负例，因为那是训练的过程中要处理的。这里还涉及到一个负例问题，看看dataloader里面负例的处理。  
拆出来一个先看看，然后再大规模的？也可以一起，输出一个list的打分值，int转一下送到损失函数里面去。  
总之prompt的设计似乎成了关键，prompt和输入embedding的关系又很怪。  





prompt经过tokenizer变成inputs，内有inputs_id和attention mask，那么inputs_id和inputs_embedding又不共存，难道要搞一个巨大无比的prompt吗？感觉那就要看一眼tokenizer了，看能不能在这里下手。  

懂了，就是全部放到prompt里面，然后先实现一个的，然后之后批量训练的时候写一个脚本循环把embedding放到源源不断的prompt里。  

122  
fuck,之所以爆显存是因为我把模型加载重复用了好多次  
放在LLM函数里肯定不行，放在kge_model里也不行，因为每个client实例化都要实例化这个类。  

放在args里必须是json，不能后来转成model  
放在fede里面了，然后给client和kge-model的接口里都加了这个参数  

model = Model(input_size, output_size)  # 实例化模型对象
if torch.cuda.device_count() > 1:  # 检查电脑是否有多块GPU
    print(f"Let's use {torch.cuda.device_count()} GPUs!")
    model = nn.DataParallel(model)  # 将模型对象转变为多GPU并行运算的模型

model.to(device)  # 把并行的模型移动到GPU上


kge_trainer只有isolation和collection会用到，fede用不到  
llama2 28g 7b  


边际效应的条件有保持其他变量不变   




BERT是论文 Pre-training of Deep Bidirectional Transformers for Language Understanding 所提出的模型，在11个NLP任务中取得好的结果。
现在研究，对于BERT的使用，一般是微调，微调时根据自己的数据，梯度下降loss(loss = 分类器的loss + Mask的loss,理论上分类器和bert都可以被更新，但实际看情况)，得到BERT模型当作词典使用或直接使用BERT的[CLS]进行预测（一段话是一个cls向量然后输入分类器里）。
BERT的输入为 token embedding（包含词的信息） + position embedding（包含句子中词的位置信息）+segment embedding （是否为两句话） 。（这些都是tokenizer分词之后的产物，实际上输入是文本）
BERT是特征提取器，输出是特征  


所谓跨平台是指语言编译出的程序没法跨平台，不同平台要不同编译器得到不同格式可执行文件。java的.class可以跑在JVM上  
runtime①程序运行时②系统级语言（c、c++）写裸机软件时用到的运行时库/标准库（函数的集合，通过链接和程序搞在一起）（所谓裸机程序就跟操作系统是怎么运行的一样，上电选择u盘启动就在cpu跑）③runtime system例如JVM，就是一个大库扩充为操作系统上的大软件了  
内核要加上对于系统调用的封装等一系列配套工具才叫做操作系统  
JDK>JRE>JVM 只需要跑java程序只需要下载JRE就够了  
java的引用更像c++的指针，只不过把.当->来用了  
java存在基本数据类型缓存池，根据业务决定用不用Interger.valueof(18)。包括字符串常量池，创建两个一样的字符串不同名指针会指向一处。    
由于 String 是不可变的(String类被final修饰不能有子类且char数组也被final修饰不能修改)，所以遇到字符串拼接的时候就可以考虑一下 String 的另外两个好兄弟，StringBuffer 和 StringBuilder，它俩是可变的。为什么不可变？①安全②作为常量哈希值text1.hashcode()③字符串常量值节省内存空间  
  ![](https://cdn.jsdelivr.net/gh/EuphratesG/myPic@main/202403122053628.png)
  另外java的类只能是public或者friendly  

  在JDK1.6及之前运行时常量池逻辑包含字符串常量池存放在方法区, 此时hotspot虚拟机对方法区的实现为永久代（位于堆内存中）在JDK1.7 字符串常量池被从方法区拿到了堆中, 这里没有提到运行时常量池,也就是说字符串常量池被单独拿到堆,运行时常量池剩下的东西还在方法区, 也就是hotspot中的永久代在JDK1.8 hotspot移除了永久代用元空间取而代之, 这时候字符串常量池还在堆, 运行时常量池还在方法区, 只不过方法区的实现从永久代变成了元空间(堆外内存)

作者：Himeros
链接：https://www.zhihu.com/question/377418017/answer/1062033254
来源：知乎
著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。


Seq-length指的是输入或输出序列中的词（或标记）的数量。这个长度可以根据不同的句子或文档而变化，通常是根据任务的需要来设置的。在一些任务中，比如机器翻译，Seq-length可能被设置为一个固定的最大长度，超过这个长度的部分将被截断。

Vocabulary-size指的是模型能够识别和生成的唯一词（或标记）的总数，就是个大字典。这个数字是由模型的词汇表预先确定的，词汇表中包含了所有可能的词以及一些特殊符号（如开始、结束标记等）。在实际应用中，Vocabulary-size可能非常大，通常包含成千上万甚至更多的词。例如多模态clip的词汇表大小是49408。

LLM模型的Vocab size对模型性能的影响 - 知乎 (zhihu.com)

按照你的分析在机器翻译任务的过程可以这样说

解码器输出: Transformer解码器的输出确实是一个三维向量，其维度通常表示为（Batch size, Sequence length, Feature size）。这里的Feature size通常等同于模型嵌入的维度。
线性层（Linear Layer）: 解码器的输出接着通过一个线性层（也称为全连接层），这个线性层的作用是将Feature size转换成与词汇表大小相同的维度。因此，线性层之后的输出维度会变为（Batch size, Sequence length, Vocabulary size）。这一步骤实际上是在为序列中的每个位置的每个可能的词分配一个得分。
选择下一个词: 在生成下一个词时，我们通常关注序列的最后一个位置（即当前正在生成的位置）。对于这个位置，模型会提供一个概率分布（通过对线性层输出应用softmax函数），这个分布覆盖了整个词汇表，表示了每个词作为下一个词的可能性。
在训练阶段，模型通常使用整个输出序列（与目标序列对齐）进行损失计算，从而学习如何生成每个位置的词。
在推理阶段，尤其是在逐步生成模式下，我们主要关注当前步骤生成的词。这时，我们会查看最后一个位置的概率分布，并根据这个分布选择下一个词。这个选择可以是概率最高的词（贪婪选择），也可以是通过某种采样策略（如顶部-k采样）或者通过Beam Search来进行。
Transformer解码器的输出为每个位置都提供了一个概率分布，但在逐步生成下一个词的过程中，还是主要关注当前步骤（即序列的最后一个位置）的输出，并基于这个位置的概率分布来选择下一个词。



