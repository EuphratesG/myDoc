
回归模型就是y（预测）=f（x，θ）（标签，模型上的点）+欧米伽 因此y也服从正态分布，均值为f方差为待定  
自监督学习是无监督学习中的一种类型，通过构造pretext辅助任务去挖掘监督信息，如**词的遮罩、时序数据的相似性、正负样本的区别**   
突然理解了masked的自监督的意义，比如图片你遮罩了一部分，然后拿遮罩的这一部分作为标签去参与构建损失函数训练模型  

参数化模型和非参数化模型，对于形如p（y|x）或者p（x）的概率模型，模型的解析表达式和参数已经确定（其实就是概率密度函数）例如高斯分布。优点是使用较快，缺点是对于数据分布的性质做了过多的假设  
对于形如p（y|x）或者p（x）的概率模型，表达式未定，knn（注意knn是有监督学习，因为要通过k个最近邻居的类别判断自己类别，核心是距离和k的超参数选取，kmeans是另一个无监督算法）  

维度灾难，数据维度太高统一维度下相关特征太少，甚至可能导致过拟合。解决方法降维、正则化、增加数据  

我们如何选择模型，一个评判标准是泛化误差，即模型对未来数据的错分率，但我们又得不到未来数据，因此用大规模测试集代替，但训练时又无法访问测试集，因此引入交叉验证。  
如果数据足够，就训练几个不同超参数的模型就先在验证集上选出最优，最后训练完毕再在测试集上跑。数据不足够的话就把全部数据分成k个子集训练k次，每次用k-1个子集训练1个子集验证，平均结果去评价本参数的模型的表现。  

## 第二章 离散数据的生成模型  
似然函数理解成在某个h（发生事件的先验参数）下发生已观测到事件的概率（令其最大）  
注意区分h的后验概率和对y做预测的后验概率（贝叶斯预测模型）
注意下雨、女孩去不去图书馆是朴素贝叶斯分类器一种模型，糖果是另一组贝叶斯预测模型（要用到那个公式）仔细体会一下  

生成模型（建模不同类的分布）建模的是联合分布p(x,y)，而判别模型建模（建模不同类的边界）的则是条件分布p(y|x)  
凭什么得到似然函数就要最大化似然函数？而我们会倾向于认为，现实发生的事件应该就对应于理论上概率最大的那一个事件。  
所谓的分类分布不过就是伯努利分布的骰子场景扩充    
共轭先验，和似然函数、后验具有相同的形式并可以合并。二项分布的共轭先验就是β分布，知道公式就行   

生成式模型：
p( y = c | x,θ) 正比于 p(x | y = c,θ) p( y = c |θ)    
c类别的概率正比于c在样本中的类条件密度（样本中是c类的概率）乘c的先验概率（c类占所有类的比例） 右边其实就是x和y=c的联合概率分布，对此建模  

数字游戏的似然函数和先验h的size和观测数据的个数都有关系，跟骰子还不一样  
先验如果是均匀分布的话那p（h）就是常数，上下的就能约掉，就是骰子例子的上下1/2.  
注意后验预测分布和后验分布是完全不同的两个概念。贝叶斯预测分布是后验预测分布。  
**所以重点是贝叶斯预测分布那个全概率公式推出来的预测公式，但是大多数情况下已知所有的h是不现实的，因此我们常用最大后验分布或者更简单最大似然分布**  
至于你说为什么人工智能女孩去图书馆那个没加权？也可以不加权吗
p17那个x∈c其实是x=1要具象化才对，中间那个是每个类每个先验假设的加权平均（后验的加权，加权权重是自己编的，ppt里是拿出酸橙味的概率，这样最后预测的就是各个模型平均预测酸橙的概率，但是由于如果一直出酸橙的话全是酸橙那个假设的后验会提高，因此该平均概率也会趋向于全是酸橙。cnmppt错了里面应该是x=1外面也是x=1），下面就是贝叶斯公式的上面  

扔硬币的似然和二项分布就差一个系数，而如果先验和似然的形式相同那么只需要系数相加就能得到后验正比的值了。因此引入二项分布的共轭先验是β分布  
p30其实就是用到前面的贝叶斯预测分布  
p35I指第j面朝上的次数  
狄利克雷那块不是重点，知道结论就行。  

词袋模型 狄利克雷的应用  
后面的暂时略  

## 第三章 高斯分布  
前面介绍了一下多元高斯分布，引出高斯判别分析    
生成式分类器是对x和θ条件下的y=c用贝叶斯公式  
高斯判别分析实际上就是生成式模型后面的类条件密度服从高斯分布。  
最近中心分类器实际上就是在高斯判别分析基础上再让类先验服从均匀分布。  
二次判别分析 其实就是把二分类中两个类的后验概率的差单独拿出来做一个判别函数，因为该函数是二次的所以叫二次判别分析  
若两个类的协方差矩阵相等则退化为线性判别分析  


运用高斯联合分布进行推理
例子 无噪声数据的函数拟合 没说完啊  
线性高斯系统就y是x的线性函数  
线性高斯系统推断未知标量和推断未知向量都是已知y去用贝叶斯得到x的后验  



## 第四章 核方法  
两个类线性不可分，那么就搞个映射升维让他线性可分。  
由于映射函数一般来说，非常难以寻找，  
❖因此，希望寻找另外一种函数，也能解决分类问题  
❖满足下列条件的函数，称为核函数  
▪ k(x, x) = φ(x)Tφ(x). 自己做是求模（映射函数自己点乘自己应该是模吧）  
▪ 具有对称性：k(x, x’) = k(x’, x).  对称 
▪ 函数值非负：k(x, x’) ≥ 0 正定 
核函数其实就是两个高维函数的点积  
全部跳过直接到p22构造核函数举例  

由最小化d得到一个最终损失函数用来优化  
核函数就是高维空间的内积等于原空间核函数的结果，soudesu。  

## 第六章 贝叶斯网络 
概率推理，因为有向图已经给出了联合概率分布的形式，推理就按图推理就行了。  


"Discriminative representations" （判别式表示，更注重特征提取和分类）和"生成式表示"（generative representations）更注重生成结果的过程。极大似然估计法贯穿了生成式模型的始终。所谓极大似然估计：假如有一坨已知分布但不知道参数（例如方差和均值）的样本数据，我们可以把每个样本带入f后乘起来作为似然函数再求极值得到参数估计值。  